<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第七章 支持向量机（1）——线性可分支持向量机  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
        <link href="https://xutree.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - Full Atom Feed" />
        <link href="https://xutree.github.io/feeds/读书笔记.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 读书笔记 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/基础知识.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 基础知识 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/教程.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 教程 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/其他.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 其他 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/趣闻.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 趣闻 Category Atom Feed" />
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/11/7_1/"> 统计学习方法 第七章 支持向量机（1）——线性可分支持向量机  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
            <div class="span8 offset2 article-content">

                <p>支持向量机（support vector machines，SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于<a href="https://xutree.github.io/pages/2018/11/04/2/">感知机</a>；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。</p>
<p>支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。</p>
<p>支持向量机学习方法包含构建由间至繁的模型：</p>
<ul>
<li>线性可分支持向量机（linear support vector machine in linearly separable case）：数据线性可分，通过硬间隔最大化（hard margin maximization），学习一个线性分类器</li>
<li>线性支持向量机（linear support vector machine）：数据近似线性可分，通过软间隔最大化（soft margin maximization），学习一个线性分类器，又称软间隔支持向量机</li>
<li>非线性支持向量机（non-linear support vector machine）：数据线性不可分，通过核技巧（kernel trick）及软间隔最大化，学习一个非线性分类器</li>
</ul>
<h2>7.1 线性可分支持向量机与硬间隔最大化</h2>
<h3>7.1.1 线性可分支持向量机</h3>
<p>一般的，当训练数据集线性可分时，存在无穷多个分离超平面可将两类数据正确分开。感知机利用分类误差最小的策略，求得分离超平面，不过这是的解有无穷多个。线性可分支持向量机利用间隔最大化求解最优分离超平面，这时，解是唯一的。</p>
<p><strong>
定义 7.1（线性可分支持向量机）给定线性可分训练数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为
<div class="math">$$w^\star\cdot x+b^\star=0$$</div>
以及相应的分类决策函数
<div class="math">$$f(x)=\text{sign}(w^\star\cdot x+b^\star)$$</div>
称为线性可分支持向量机。
</strong></p>
<h3>7.1.2 函数间隔和几何间隔</h3>
<p><strong>
定义 7.2（函数间隔）对于给定的训练数据集 <span class="math">\(T\)</span> 和超平面 <span class="math">\((w,b)\)</span>，定义超平面 <span class="math">\((w,b)\)</span> 关于样本点 <span class="math">\((x_i,y_i)\)</span> 的函数间隔为
<div class="math">$$\hat{\gamma}_i=y_i(w\cdot x_i+b)$$</div>
定义超平面 <span class="math">\((w,b)\)</span> 关于训练数据集 <span class="math">\(T\)</span> 的函数间隔为
<div class="math">$$\hat{\gamma}=\min_{i=1,2,\cdots,N}\hat{\gamma}_i$$</div>
</strong></p>
<p>函数间隔（functional margin）可以表示分类预测的正确性和确信度，但是具有不确定因子，我们需要对分离超平面的法向量 <span class="math">\(w\)</span> 加以约束，使得间隔是确定的。这时函数间隔就成了几何间隔（geometric margin）。</p>
<p><strong>
定义 7.2（几何间隔）对于给定的训练数据集 <span class="math">\(T\)</span> 和超平面 <span class="math">\((w,b)\)</span>，定义超平面 <span class="math">\((w,b)\)</span> 关于样本点 <span class="math">\((x_i,y_i)\)</span> 的几何间隔为
<div class="math">$$\gamma_i=y_i\left(\frac{w}{\|w\|}\cdot x_i+\frac{b}{\|w\|}\right)$$</div>
定义超平面 <span class="math">\((w,b)\)</span> 关于训练数据集 <span class="math">\(T\)</span> 的几何间隔为
<div class="math">$$\gamma=\min_{i=1,2,\cdots,N}\gamma_i$$</div>
</strong></p>
<p>易知：
</p>
<div class="math">$$\gamma_i=\frac{\hat{\gamma}_i}{\|w\|} \\
\gamma=\frac{\hat{\gamma}}{\|w\|}$$</div>
<p>
如果超平面参数 <span class="math">\(w\)</span> 和 <span class="math">\(b\)</span> 成比例变化（超平面没有改变），函数间隔也按此比例变化，几何间隔不变。</p>
<h3>7.1.3 间隔最大化</h3>
<p>间隔最大化的直观解释：对训练数据集找到几何间隔最大的超平面意味着以充分大的确定度对训练数据进行分类。这样的超平面对未知的新实例有很好的分类预测能力。</p>
<h4>最大间隔分离超平面</h4>
<p>求解几何间隔最大的分离超平面问题即求解下面的约束最优化问题：
</p>
<div class="math">$$\max_{w,b}\ \gamma \\
\text{s.t.}\ \ \ \ y_i\left(\frac{w}{\|w\|}\cdot x_i+\frac{b}{\|w\|}\right)\geq\gamma,\ i=1,2,\cdots,N$$</div>
<p>
可改写为：
</p>
<div class="math">$$\max_{w,b}\ \frac{\hat{\gamma}}{\|w\|} \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)\geq\hat{\gamma},\ i=1,2,\cdots,N$$</div>
<p>
对于上式来说，假设将分离超平面 <span class="math">\((w,b)\)</span> 按比例改变为 <span class="math">\((\lambda w,\lambda b)\)</span>，这时函数间隔变为 <span class="math">\(\lambda \hat{\gamma}\)</span>，所以对目标函数和约束条件都没有影响。那么，如果我们做如下变换：
</p>
<div class="math">$$(w,b)\to(\frac{1}{\hat{\gamma}}w,\frac{1}{\hat{\gamma}}b)$$</div>
<p>
则函数间隔变为 1，故上述问题可改写为：
</p>
<div class="math">$$\max_{w,b}\ \frac{1}{\|w\|} \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)\geq 1,\ i=1,2,\cdots,N$$</div>
<p>
注意到最大化 <span class="math">\(\frac{1}{\|w\|}\)</span> 和最小化 <span class="math">\(\frac{1}{2}\|w\|^2\)</span> 是等价的，于是问题变为：
</p>
<div class="math">$$\min_{w,b}\ \frac{1}{2}\|w\|^2 \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)-1\geq0,\ i=1,2,\cdots,N$$</div>
<p>
这是一个凸二次规划问题。</p>
<p>凸优化问题是指约束最优化问题
</p>
<div class="math">$$\min_w\ f(w) \\
\text{s.t.}\ \ \ \ \begin{eqnarray}
g_i(x) &amp;\leq0&amp;,\ i=1,2\cdots,k \\
h_i(w) &amp;=&amp; 0,\ i=1,2,\cdots,l
\end{eqnarray}
$$</div>
<p>
其中，目标函数 <span class="math">\(f(w)\)</span> 和约束函数 <span class="math">\(g_i(w)\)</span> 都是 <span class="math">\(\mathbb{R}^n\)</span> 上的连续可微凸函数，约束函数 <span class="math">\(h_i(w)\)</span> 是 <span class="math">\(\mathbb{R}^n\)</span> 上的仿射函数（即满足 <span class="math">\(h_i(w)=a\cdot w+b\)</span> 的形式）。</p>
<p><strong>
算法 7.1（线性可分支持向量机学习算法——最大间隔法）<br>
输入：线性可分训练数据集 <span class="math">\(T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}\)</span>，其中 <span class="math">\(x_i\in{\cal X}=\mathbb{R}^n\)</span>，<span class="math">\(y_i\in{\cal Y}=\{+1,-1\}\)</span>，<span class="math">\(i=1,2,\cdots,N\)</span><br>
输出：最大间隔分离超平面和分类决策函数<br>
(1) 构造并求解约束最优化问题
<div class="math">$$\min_{w,b}\ \frac{1}{2}\|w\|^2 \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)-1\geq0,\ i=1,2,\cdots,N$$</div>
求得最优解 <span class="math">\(w^\star\)</span>，<span class="math">\(b^\star\)</span><br>
(2) 由此得到分离超平面
<div class="math">$$w^\star\cdot x+b^\star=0$$</div>
分类决策函数
<div class="math">$$f(x)=\text{sign}(w^\star\cdot x+b^\star)$$</div>
</strong></p>
<p>上面的算法就是最大间隔法（maximum margin method）。</p>
<h4>最大间隔分离超平面的存在唯一性</h4>
<p><strong>
定理 7.1（最大间隔分离超平面的存在唯一性）若训练数据集线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。
</strong></p>
<h4>支持向量和间隔边界</h4>
<p>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。支持向量是约束条件
</p>
<div class="math">$$y_i\left(w\cdot x_i+b\right)-1\geq0,\ i=1,2,\cdots,N$$</div>
<p>
等号成立的点，即
</p>
<div class="math">$$y_i(w\cdot x_i+b)-1=0$$</div>
<p>
对 <span class="math">\(y_i=+1\)</span> 的正例点，支持向量在超平面
</p>
<div class="math">$$H_1:\ w\cdot x+b=1$$</div>
<p>
上。对 <span class="math">\(y_i=-1\)</span> 的负例点，支持向量在超平面
</p>
<div class="math">$$H_2:\ w\cdot x+b=-1$$</div>
<p>
上。</p>
<p><img alt="支持向量和间隔边界" src="https://xutree.github.io/images/statistical_learning_7.3.png"></p>
<p>注意到 <span class="math">\(H_1\)</span> 和 <span class="math">\(H_2\)</span> 平行，之间的距离称为间隔（margin）。间隔依赖于分离超平面的法向量，等于 <span class="math">\(\frac{2}{\|w\|}\)</span>，<span class="math">\(H_1\)</span> 和 <span class="math">\(H_2\)</span> 称为间隔边界。</p>
<p>所有的支持向量都精确的落在边缘上。不管空间的维度有多大，或者数据集合有多大，支持向量的个数一般很少，都可以像 2 个这么小。在决定分离超平面时，只有支持向量起着至关重要的作用，其他的点并不起作用。所以支持向量机是由这些很少的、很“重要的”训练数据决定的。</p>
<h3>7.1.4 学习的对偶算法</h3>
<p>为了求解线性可分支持向量机的最优化问题
</p>
<div class="math">$$\min_{w,b}\ \frac{1}{2}\|w\|^2 \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)-1\geq0,\ i=1,2,\cdots,N$$</div>
<p>
可以构造对偶问题。这样做的优点，一是对偶问题往往更容易求解；二是自然引入核函数，进而推广到非线性分类问题。</p>
<p>定义<a href="https://xutree.github.io/pages/2018/11/10/lagrange_duality/">拉格朗日函数</a>：
</p>
<div class="math">$$L(w,b,\alpha)=\frac{1}{2}\|w\|^2-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i$$</div>
<p>
其中，<span class="math">\(\alpha_i\geq0\)</span>，<span class="math">\(\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^\text{T}\)</span> 为拉格朗日乘子向量。</p>
<p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
</p>
<div class="math">$$\max_\alpha\min_{w,b}L(w,b,\alpha)$$</div>
<p><span class="math">\(\blacksquare\)</span> 求 <span class="math">\(\min_{w,b}L(w,b,\alpha)\)</span></p>
<p>求偏导并令偏导为 0
</p>
<div class="math">$$\nabla_wL(w,b,\alpha)=w-\sum_{i=1}^N\alpha_iy_ix_i=0 \\
\nabla_bL(w,b,\alpha)=-\sum_{i=1}^N\alpha_iy_i=0$$</div>
<p>
得
</p>
<div class="math">$$w=\sum_{i=1}^N\alpha_iy_ix_i \\
\sum_{i=1}^N\alpha_iy_i=0$$</div>
<p>
利用上面两式，得
</p>
<div class="math">$$L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i$$</div>
<p>
即
</p>
<div class="math">$$\min_{w,b}L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i$$</div>
<p><span class="math">\(\blacksquare\)</span> 求 <span class="math">\(\min_{w,b}L(w,b,\alpha)\)</span> 对 <span class="math">\(\alpha\)</span> 的极大</p>
<p>即是求对偶问题
</p>
<div class="math">$$\max_\alpha -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i \\
\text{s.t.}\ \ \ \ \begin{eqnarray}
\sum_{i=1}^N\alpha_iy_i &amp;=&amp; 0 \\
\alpha_i \geq 0,\ i &amp;=&amp; 1,2,\cdots,N
\end{eqnarray}$$</div>
<p>
也就是下面等价的最小化问题
</p>
<div class="math">$$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i \\
\text{s.t.}\ \ \ \ \begin{eqnarray}
\sum_{i=1}^N\alpha_iy_i &amp;=&amp; 0 \\
\alpha_i \geq 0,\ i &amp;=&amp; 1,2,\cdots,N
\end{eqnarray}$$</div>
<p>原始最优化问题和对偶最优化问题满足拉格朗日对偶性<a href="https://xutree.github.io/pages/2018/11/10/lagrange_duality/">定理 2</a> 的条件，所以存在 <span class="math">\(w^\star\)</span>，<span class="math">\(b^\star\)</span> 和 <span class="math">\(\alpha^\star\)</span> 使 <span class="math">\(w^\star\)</span>，<span class="math">\(b^\star\)</span>是原始问题的解，<span class="math">\(\alpha^\star\)</span>是对偶问题的解。</p>
<p>对线性可分训练数据集，假设对偶最优化问题的解是
</p>
<div class="math">$$\alpha^\star=\left(\alpha_1^\star,\alpha_2^\star,\cdots,\alpha_N^\star\right)^\text{T}$$</div>
<p>
可以由下面的定理求得 <span class="math">\(w^\star\)</span> 和 <span class="math">\(b^\star\)</span>。</p>
<p><strong>
定理 7.2 设 <span class="math">\(\alpha^\star=\left(\alpha_1^\star,\alpha_2^\star,\cdots,\alpha_N^\star\right)^\text{T}\)</span> 是对偶最优问题的解，则存在下标 <span class="math">\(j\)</span>，使得 <span class="math">\(\alpha_j^\star&gt;0\)</span>，并且可按下式求得原始最优化问题的解
<div class="math">$$w^\star=\sum_{i=1}^N\alpha_i^\star y_ix_i \\
b^\star=y_j-\sum_{i=1}^N\alpha_i^\star y_i(x_i\cdot x_j)$$</div>
</strong></p>
<p>由此定理可知，分离超平面可以写成
</p>
<div class="math">$$\sum_{i=1}^N\alpha_i^\star y_i(x\cdot x_i)+b^\star=0$$</div>
<p>
分类决策函数可以写成
</p>
<div class="math">$$f(x)=\text{sign}\left(\sum_{i=1}^N\alpha_i^\star y_i(x\cdot x_i)+b^\star\right)$$</div>
<p>
分类决策函数只依赖于输入 <span class="math">\(x\)</span> 和训练样本输入的内积。上式称为线性可分支持向量机的对偶形式。</p>
<p><strong>
算法 7.2（线性可分支持向量机学习算法）<br>
输入：线性可分训练数据集 <span class="math">\(T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}\)</span>，其中 <span class="math">\(x_i\in{\cal X}=\mathbb{R}^n\)</span>，<span class="math">\(y_i\in{\cal Y}=\{+1,-1\}\)</span>，<span class="math">\(i=1,2,\cdots,N\)</span><br>
输出：分离超平面和分类决策函数<br>
(1) 构造并求解约束最优化问题
<div class="math">$$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i \\
\text{s.t.}\ \ \ \ \begin{eqnarray}
\sum_{i=1}^N\alpha_iy_i &amp;=&amp; 0 \\
\alpha_i \geq 0,\ i &amp;=&amp; 1,2,\cdots,N
\end{eqnarray}$$</div>
求得最优解 <span class="math">\(\alpha^\star=\left(\alpha_1^\star,\alpha_2^\star,\cdots,\alpha_N^\star\right)^\text{T}\)</span><br>
(2) 计算
<div class="math">$$w^\star=\sum_{i=1}^N\alpha_i^\star y_ix_i$$</div>
并选择 <span class="math">\(\alpha^\star\)</span> 的一个正分量 <span class="math">\(\alpha_j^\star&gt;0\)</span>，计算
<div class="math">$$b^\star=y_j-\sum_{i=1}^N\alpha_i^\star y_i(x_i\cdot x_j)$$</div>
(3) 求得分离超平面
<div class="math">$$\sum_{i=1}^N\alpha_i^\star y_i(x\cdot x_i)+b^\star=0$$</div>
分类决策函数
<div class="math">$$f(x)=\text{sign}\left(\sum_{i=1}^N\alpha_i^\star y_i(x\cdot x_i)+b^\star\right)$$</div>
</strong></p>
<p>由公式知 <span class="math">\(w^\star\)</span>，<span class="math">\(b^\star\)</span> 只依赖于训练数据中对于与 <span class="math">\(\alpha_i^\star&gt;0\)</span> 的样本点 <span class="math">\((x_i,y_i)\)</span>，我们将这些实例点称为支持向量。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <aside>
                    <hr />
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/10/directional_derivative-gradient/">
                                    方向导数和梯度
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/10/lagrange_duality/">
                                    拉格朗日对偶性
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/09/6/">
                                    统计学习方法 第六章 逻辑回归与最大熵模型
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/07/5/">
                                    统计学习方法 第五章 决策树
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/07/4/">
                                    统计学习方法 第四章 朴素贝叶斯法
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="newer">
                        <h1>
                            <font color="#771515"><em>NEWER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/11/7_2/">
                                    统计学习方法 第七章 支持向量机（2）——线性支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/7_3/">
                                    统计学习方法 第七章 支持向量机（3）——非线性支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/7_4/">
                                    统计学习方法 第七章 支持向量机（4）——序列最小最优化算法
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <!-- Gitalk 评论 start  -->

                    <!-- Link Gitalk 的支持文件  -->
                    <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                    <div id="gitalk-container"></div>
                    <script type="text/javascript">
                        var dateTime = Date.now();
                        var timestamp = Math.floor(dateTime / 1000);
                        var gitalk = new Gitalk({

                            // gitalk的主要参数
                            clientID: '93f43349e9fd3154bfad',
                            clientSecret: 'd6d09d1d7261f6b62f46b39e5fcace85b81c3cd7',
                            repo: 'xutree.github.io',
                            owner: 'xutree',
                            admin: ['xutree'],
                            id: String(timestamp)

                        });
                        gitalk.render('gitalk-container');
                    </script> -->
                    <!-- Gitalk end -->
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-11T13:47:48+08:00">2018-11-11 13:47:48</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-11 17:55:33</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>10</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>10</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>