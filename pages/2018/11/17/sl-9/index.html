<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第九章 EM 算法及其推广  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
        <link href="https://xutree.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - Full Atom Feed" />
        <link href="https://xutree.github.io/feeds/读书笔记.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 读书笔记 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/基础知识.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 基础知识 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/教程.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 教程 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/其他.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 其他 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/趣闻.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 趣闻 Category Atom Feed" />
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/17/sl-9/"> 统计学习方法 第九章 EM 算法及其推广  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
            <div class="span8 offset2 article-content">

                <p>EM 算法是一种迭代算法，用于含有隐变量（hidden varibale）的概率模型参数的极大似然估计，或极大后验概率估计。EM 算法每次的迭代分两步：E 步，求期望（expectation）；M 步，求极大（maximization）。所以这一算法被称为期望极大算法（expectation maximization）。</p>
<h2>9.1 EM 算法的引入</h2>
<p>概率模型有时既含有观测变量（observable variable），又含有隐变量或潜在变量（latent variable）。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单的使用这些估计方法。EM 算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。</p>
<h3>9.1.1 EM 算法</h3>
<p>一般的，用 $Y$ 表示观测随机变量的数据，$Z$ 表示隐随机变量的数据。$Y$ 和 $Z$ 连在一起称为完全数据（complete-data），观测数据 $Y$ 又称为不完全数据（incomplete-data）。假设给定观测数据 $Y$，其概率分布是 $P(Y|\theta)$，其中 $\Theta$ 是需要估计的模型参数，那么不完全数据了 $Y$ 的似然函数是 $P(Y|\theta)$，对数似然函数是 $L(\theta)=\log P(Y|\theta)$；假设 $Y$ 和 $Z$ 的联合概率分布是 $P(Y,Z|\theta)$，那么完全数据的对数似然函数是 $\log P(Y,Z|\theta)$。</p>
<p>EM 算法通过迭代求 $L(\theta)=\log P(Y|\theta)$ 的极大似然估计。</p>
<p><strong>
算法 9.1（EM 算法）<br>
输入：观测变量数据 $Y$，隐变量数据 $Z$，联合分布 $P(Y,Z|\theta)$，条件分布 $P(Z|Y,\theta)$<br>
输出：模型参数 $\theta$<br>
(1) 选择参数的初值 $\theta^{(0)}$，开始迭代<br>
(2) E 步：记 $\theta^{(i)}$ 为第 $i$ 次迭代参数 $\theta$ 的估计值。在第 $i+1$ 次迭代的 E 步，计算
$$\begin{eqnarray}
Q\left(\theta,\theta^{(i)}\right) &amp;=&amp; \text{E}<em Z>Z\left[\log P(Y,Z|\theta)|Y,\theta^{(i)}\right] \
&amp;=&amp; \sum</em>\log P(Y,Z|\theta)P(Z|Y,\theta^{(i)})
\end{eqnarray}$$
这里，$P(Z|Y,\theta^{(i)})$ 是在给定观测数据 $Y$ 和当前的参数估计 $\theta^{(i)}$ 下隐变量数据 $Z$ 的条件概率分布<br>
(3) M 步：求极大，更新 $\theta$<br>
$$\theta^{(i+1)}=\arg\max_{\theta}Q\left(\theta,\theta^{(i)}\right)$$
(4) 重复 (2)，(3)，直到收敛
</strong></p>
<p>$Q$ 函数是 EM 算法的核心。</p>
<p><strong>
定义 9.1（$Q$ 函数）玩去数据的对数似然函数 $\log P(Y,Z|\theta)$ 关于在给定观测数据 $Y$ 的当前参数 $\theta^{(i)}$ 下对未观测数据 $Z$ 的条件概率分布 $P\left(Z|Y,\theta^{(i)}\right)$ 的期望称为 $Q$ 函数，即
$$Q\left(\theta,\theta^{(i)}\right)=\text{E}_Z\left[\log P(Y,Z|\theta)|Y,\theta^{(i)}\right]$$
</strong></p>
<p>关于 EM 算法需要注意：参数的初值可以任意选择，但需注意 EM 算法对初值是敏感的；给出停止迭代的条件，一般是对较小的正数 $\epsilon_1$，$\epsilon_2$，若满足
$$|\theta^{(i+1)}-\theta^{(i)}|&lt;\epsilon_1 $$
或
$$\left|Q\left(\theta^{(i+1)},\theta^{(i)}\right)-Q\left(\theta^{(i)},\theta^{(i)}\right)\right|&lt;\epsilon_2$$
则迭代停止。</p>
<h3>9.1.2 EM 算法的推导</h3>
<p>首先
$$\begin{eqnarray}
L(\theta) &amp;=&amp; \log P(Y|\theta)=\log\sum_ZP(Y,Z|\theta) \
&amp;=&amp; \log\left(\sum_ZP(Y|Z,\theta)P(Z|\theta)\right)
\end{eqnarray}$$
则
$$\begin{eqnarray}
L\left(\theta\right)-L(\theta^{(i)}) &amp;=&amp; \log\left(\sum_ZP(Y|Z,\theta)P(Z|\theta)\right)-\log P(Y|\theta^{(i)}) \
&amp;=&amp; \log\left(\sum_ZP(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}\right)-\log P(Y|\theta^{(i)}) \
&amp;\geq&amp; \sum_Z P(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-\log P(Y|\theta^{(i)}) \
&amp;=&amp; \sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}
\end{eqnarray}$$
令
$$B(\theta,\theta^{(i)})\equiv L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$$
则只需要最大化 $B(\theta,\theta^{(i)})$，故
$$\begin{eqnarray}
\theta^{(i+1)} &amp;=&amp; \arg\max_\theta\left(L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\right) \
&amp;=&amp; \arg\max_\theta\left(\sum_ZP(Z|Y,\theta^{(i)})\log\left(P(Y|Z,\theta)P(Z|\theta)\right)\right) \
&amp;=&amp; \arg\max_\theta\left(\sum_ZP(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)\right) \
&amp;=&amp; \arg\max_\theta Q(\theta,\theta^{(i)})
\end{eqnarray}$$</p>
<p>EM 算法不能保证全局最优。</p>
<h2>9.2 EM 算法的收敛性</h2>
<p><strong>
定理 9.1 设 $P(Y|\theta)$ 为观测数据的似然函数，$\theta^{(i)}$ 为 EM 算法得到的参数估计序列，$P(Y|\theta^{(i)})$ 为对应的似然函数序列，则 $P(Y|\theta^{(i)})$ 是单调递增的，即
$$P(Y|\theta^{(i+1)})\geq P(Y|\theta^{(i)})$$
</strong></p>
<p><strong>
定理 9.2 设 $L(\theta)=\log P(Y|\theta)$ 为观测数据的对数似然函数，$\theta^{(i)}$ 为 EM 算法得到的参数估计序列，$L(\theta^{(i)})$ 为对应的对数似然函数序列。<br>
(1) 如果 $P(Y|\theta)$ 有界，则 $L(\theta^{(i)})=\log P(Y|\theta^{(i)})$ 收敛到某一值 $L^\star$<br>
(2) 在函数 $Q(\theta,\theta')$ 与 $L(\theta)$ 满足一定条件下，由 EM 算法得到的参数估计序列 $\theta^{(i)}$ 的收敛值 $\theta^\star$ 是 $L(\theta)$ 的稳定点
</strong></p>
<p>在应用中，初始值的选择很重要，常用的办法是选取几个不同的初值进行迭代，然后对得到的各个估计值加以比较，从中选择最好的。</p>
<h2>9.3 EM 算法在高斯混合模型学习中的应用</h2>
<p>EM 算法的一个重要应用是高斯混合模型的参数估计。高斯混合模型应用广泛，在许多情况下，EM 算法是学习高斯混合模型（Gaussian misture model）的有效方法。</p>
<h3>9.3.1 高斯混合模型</h3>
<p><strong>
定义 9.2（高斯混合模型）高斯混合模型是指具有如下形式的概率分布模型：
$$P(y|\theta)=\sum_{k=1}^K\alpha_k\phi(y|\theta_k)$$
其中，$\alpha_k$ 是系数，$\alpha_k\geq0$，$\sum_{k=1}^K\alpha_k=1$；$\phi(y|\theta_k)$ 是高斯分布密度，$\theta_k=(\mu_k,\sigma_k^2)$
$$\phi(y|\theta_k)=\frac{1}{\sqrt{2\pi}\sigma_k}\exp\left(-\frac{(y-\mu_k)^2}{2\sigma_k^2}\right)$$
称为第 $k$ 个分模型。
</strong></p>
<p>一般混合模式可以由任意概率分布密度函数代替高斯分布密度。</p>
<h3>9.3.2 高斯混合模型参数估计的 EM 算法</h3>
<p>假设观测数据 $y_1,y_2,\cdots,y_N$ 由高斯混合模型生成
$$P(y|\theta)=\sum_{k=1}^K\alpha_k\phi(y|\theta_k)$$
其中 $\theta=(\alpha_1,\alpha_2,\cdots,\alpha_K;\theta_1,\theta_2,\cdots,\alpha_K)$。我们用 EM 算法估计高斯混合模型的参数 $\theta$。</p>
<p>$\blacksquare$ 明确隐变量，写成完全数据的对数似然函数</p>
<p>引入隐变量 $\gamma_{jk}$
$$\gamma_{jk}=\begin{cases}
1, &amp; 第\ j\ 个观测来自第\ k\ 个分模型 \
0, &amp; 否则
\end{cases} \
j=1,2,\cdots,N;\ k=1,2,\cdots,K$$
于是，完全数据似然函数可以写成：
$$\begin{eqnarray}
P(y,\gamma|\theta) &amp;=&amp; \prod_{j=1}^NP(y_j,\gamma_{j1},\gamma_{j2},\cdots,\gamma_{jK}|\theta) \
&amp;=&amp; \prod_{k=1}^K\prod_{j=1}^N\left[\alpha_k\phi(y_j|\theta_k)\right]^{\gamma_{jk}} \
&amp;=&amp; \prod_{k=1}^K\alpha_k^{n_k}\prod_{j=1}^N\left[\phi(y_j|\theta_k)\right]^{\gamma_{jk}} \
&amp;=&amp; \prod_{k=1}^K\alpha_k^{n_k}\prod_{j=1}^N\left[\frac{1}{\sqrt{2\pi}\sigma_k}\exp\left(-\frac{(y_j-\mu_k)^2}{2\sigma_k^2}\right)\right]^{\gamma_{jk}}
\end{eqnarray}$$
其中
$$n_k=\sum_{j=1}^N\gamma_{jk},\ \sum_{k=1}^Kn_k=N$$
那么，完全数据的对数似然函数为
$$\log P(y,\gamma|\theta)=\sum_{k=1}^K\left{n_k\log\alpha_k+\sum_{j=1}^N\gamma_{jk}\left[\log\left(\frac{1}{\sqrt{2\pi}}\right)-\log\sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]\right}$$</p>
<p>$\blacksquare$ EM 算法的 E 步：确定 $Q$ 函数
$$\begin{eqnarray}
Q\left(\theta,\theta^{(i)}\right) &amp;=&amp; \text{E}\left[\log P(y,\gamma|\theta)|y,\theta^{(i)}\right] \
&amp;=&amp; \text{E}\left{\sum_{k=1}^K\left{n_k\log\alpha_k+\sum_{j=1}^N\gamma_{jk}\left[\log\left(\frac{1}{\sqrt{2\pi}}\right)-\log\sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]\right}\right} \
&amp;=&amp; \sum_{k=1}^K\left{\sum_{j=1}^N\text{E}[\gamma_{jk}]\log\alpha_k+\sum_{j=1}^N\text{E}[\gamma_{jk}]\left[\log\left(\frac{1}{\sqrt{2\pi}}\right)-\log\sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]\right}
\end{eqnarray}$$
这里需要计算 $\text{E}[\gamma_{jk}|y,\theta]$，记为 $\hat{\gamma}<em jk>{jk}$
$$\begin{eqnarray}
\hat{\gamma}</em> &amp;=&amp; \text{E}[\gamma_{jk}|y,\theta]=P(\gamma_{jk}=1|y,\theta) \
&amp;=&amp; \frac{P(\gamma_{jk}=1,y_j|\theta)}{\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta)} \
&amp;=&amp; \frac{P(y_j|\gamma_{jk}=1,\theta)P(\gamma_{jk}=1|\theta)}{\sum_{k=1}^KP(y_j|\gamma_{jk}=1,\theta)P(\gamma_{jk}=1|\theta)} \
&amp;=&amp; \frac{\alpha_k\phi(y_j|\theta_k)}{\sum_{k=1}^K\alpha_k\phi(y_j|\theta_k)}
\end{eqnarray}$$
其中
$$j=1,2,\cdots,N;\ k=1,2,\cdots,K$$
$\hat{\gamma_{jk}}$ 是在当前模型参数下第 $j$ 个观测数据来及第 $k$ 分模型的概率，称为分模型 $k$ 对观测数据 $y_j$ 的响应度。</p>
<p>将 $\hat{\gamma_{jk}}=\text{E}[\gamma_{jk}]$ 及 $n_k=\sum_{j=1}^N\text{E}[\gamma_{jk}]$ 带入上式，得
$$Q\left(\theta,\theta^{(i)}\right)=\sum_{k=1}^K\left{n_k\log\alpha_k+\sum_{j=1}^N\gamma_{jk}\left[\log\left(\frac{1}{\sqrt{2\pi}}\right)-\log\sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\right]\right}$$</p>
<p>$\blacksquare$ 确定 EM 算法的 M 步</p>
<p>即令各偏导为零，得：
$$\hat{\mu}<em j="1">k\equiv\frac{\partial Q}{\partial \mu_k}=\frac{\sum</em>^N\hat{\gamma}<em j="1">{jk}y_j}{\sum</em>^N\hat{\gamma}<em j="1">{jk}} \
\hat{\sigma_k}^2\equiv\frac{\partial Q}{\partial \sigma_k^2}=\frac{\sum</em>^N\hat{\gamma}<em j="1">{jk}(y_j-\mu_k)^2}{\sum</em>^N\hat{\gamma}<em j="1">{jk}} \
\hat{\alpha}_k\equiv\frac{\partial Q}{\partial \alpha_k}=\frac{n_k}{N}=\frac{\sum</em>^N\hat{\gamma}_{jk}}{N} \
k=1,2,\cdots,K$$</p>
<p><strong>
算法 9.2（高斯混合模型参数估计的 EM 算法）<br>
输入：观测数据 $y_1,y_2,\cdots,y_N$，高斯混合模型<br>
输出：高斯混合模型参数<br>
(1) 取参数的初始值开始迭代<br>
(2) E 步：根据当前模型参数，计算分模型 $k$ 对观测数据 $y_j$ 的响应度
$$\hat{\gamma}<em k="1">{jk}=\frac{\alpha_k\phi(y_j|\theta_k)}{\sum</em>^K\alpha_k\phi(y_j|\theta_k)},\ j=1,2,\cdots,N;\ k=1,2,\cdots,K$$
(3) M 步：计算新一轮迭代的模型参数
$$\hat{\mu}<em j="1">k=\frac{\sum</em>^N\hat{\gamma}<em j="1">{jk}y_j}{\sum</em>^N\hat{\gamma}<em j="1">{jk}} \
\hat{\sigma_k}^2=\frac{\sum</em>^N\hat{\gamma}<em j="1">{jk}(y_j-\mu_k)^2}{\sum</em>^N\hat{\gamma}<em j="1">{jk}} \
\hat{\alpha}_k=\frac{n_k}{N}=\frac{\sum</em>^N\hat{\gamma}_{jk}}{N} \
k=1,2,\cdots,K$$
(4) 重复 (2)，(3)，直到收敛
</strong></p>
                <aside>
                    <hr />
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/16/likelihood-function/">
                                    似然函数
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-8/">
                                    统计学习方法 第八章 提升方法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-7_4/">
                                    统计学习方法 第七章 支持向量机（4）——序列最小最优化算法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-7_3/">
                                    统计学习方法 第七章 支持向量机（3）——非线性支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/11/sl-7_2/">
                                    统计学习方法 第七章 支持向量机（2）——线性支持向量机
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="newer">
                        <h1>
                            <font color="#771515"><em>NEWER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-10/">
                                    统计学习方法 第十章 隐马尔科夫模型
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-11/">
                                    统计学习方法 第十一章 条件随机场
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/20/sl-12/">
                                    统计学习方法 第十二章 统计学习方法总结
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/21/dl-1/">
                                    深度学习 第一章 引言
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/21/dl-2/">
                                    深度学习 第二章 线性代数
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <!-- Gitalk 评论 start  -->

                    <!-- Link Gitalk 的支持文件  -->
                    <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                    <div id="gitalk-container"></div>
                    <script type="text/javascript">
                        var dateTime = Date.now();
                        var timestamp = Math.floor(dateTime / 1000);
                        var gitalk = new Gitalk({

                            // gitalk的主要参数
                            clientID: '93f43349e9fd3154bfad',
                            clientSecret: 'd6d09d1d7261f6b62f46b39e5fcace85b81c3cd7',
                            repo: 'xutree.github.io',
                            owner: 'xutree',
                            admin: ['xutree'],
                            id: String(timestamp)

                        });
                        gitalk.render('gitalk-container');
                    </script> -->
                    <!-- Gitalk end -->
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-17T19:32:10+08:00">2018-11-17 19:32:10</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-17 19:32:59</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>17</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>15</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>