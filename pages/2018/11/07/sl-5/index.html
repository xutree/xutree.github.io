<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第五章 决策树  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
        <link href="https://xutree.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - Full Atom Feed" />
        <link href="https://xutree.github.io/feeds/读书笔记.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 读书笔记 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/基础知识.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 基础知识 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/教程.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 教程 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/其他.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 其他 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/趣闻.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 趣闻 Category Atom Feed" />
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/07/sl-5/"> 统计学习方法 第五章 决策树  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
            <div class="span8 offset2 article-content">

                <p>决策树（decision tree）是一种基本的分类与回归方法。其主要优点是模型具有可读性，分类速度快。决策树学习通常包括三个步骤：特征选择、决策树的生成和决策树的修剪。</p>
<h2>5.1 决策树模型与学习</h2>
<h3>5.1.1 决策树模型</h3>
<p><strong>
定义 5.1 （决策树）分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点（node）和有向边（directed edge）组成。节点有两种类型：内部节点（internal node）和叶节点（leaf node）。内部节点表示一个特征或属性，叶节点表示一个类。
</strong></p>
<h3>5.1.2 决策树学习</h3>
<p>假设给定训练数据集
</p>
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
其中输入实例（向量）为
</p>
<div class="math">$$x_i=\left(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)},\right)$$</div>
<p>
<span class="math">\(n\)</span> 为特征个数。类标记为
</p>
<div class="math">$$y_i\in\{c_1,c_2,\cdots,c_K\}$$</div>
<p>
其中 <span class="math">\(i\in\{1,2,\cdots,N\}\)</span>，<span class="math">\(N\)</span> 为样本容量。</p>
<p>学习的目的是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。</p>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练集不相矛盾的决策树可能有多个也可能一个也没有，我们需要的是一个与训练集矛盾较小的决策树，同时具有很好的泛化能力。</p>
<p>决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。</p>
<p>当损失函数确定以后，学习问题变成在损失函数意义下选择最有决策树的问题，因为从所有可能的决策树中选取最优决策树是 NP 完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优（sub-optimal）的。</p>
<h2>5.2 特征选择</h2>
<h3>5.2.1 特征选择的问题</h3>
<p>特征选择在于选取对训练数据具有分类能力的特征。这样就可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。通常特征选取的准则是信息增益或信息增益比。</p>
<h3>5.2.2 信息增益</h3>
<p>为了便于说明，首先给出熵与条件熵的定义。</p>
<p>熵（entropy）是表示随机变量不确定性的度量。</p>
<p>设 <span class="math">\(X\)</span> 是一个离散随机变量，其概率分布为
</p>
<div class="math">$$P(X=x_i)=p_i,i=1,2,\cdots,n$$</div>
<p>
则，随机变量 <span class="math">\(X\)</span> 的熵定义为
</p>
<div class="math">$$H(X)=-\sum_{i=1}^np_i\log p_i$$</div>
<p>
若 <span class="math">\(p_i=0\)</span>，定义 <span class="math">\(0\log0=0\)</span>。通常，上式中对数的底为 2 或自然对数 e，这是熵的单位分别为比特（bit）或纳特（nat）。</p>
<p>设有随机变量 <span class="math">\((X,Y)\)</span>，其联合概率分布为
</p>
<div class="math">$$P(X=x_i,Y=y_j)=p_{ij},i=1,2,\cdots,n;j=1,2,\cdots,m$$</div>
<p>
条件熵（conditional entropy） <span class="math">\(H(Y|X)\)</span> 表示在已知随机变量 <span class="math">\(X\)</span> 的条件下随机变量 <span class="math">\(Y\)</span> 的不确定性，定义为 <span class="math">\(X\)</span> 给定条件下 <span class="math">\(Y\)</span> 的条件概率分布的熵对 <span class="math">\(X\)</span> 的数学期望
</p>
<div class="math">$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$</div>
<p>
这里 <span class="math">\(p_i=P(X=x_i),i=1,2,\cdots,n\)</span></p>
<p>当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。</p>
<p>信息增益（information gain）表示得知特征 <span class="math">\(X\)</span> 的信息而使类 <span class="math">\(Y\)</span> 的信息的不确定度减少的程度。</p>
<p><strong>
定义 5.2 （信息增益）特征 <span class="math">\(A\)</span> 对训练数据集 <span class="math">\(D\)</span> 的信息增益 <span class="math">\(g(D,A)\)</span>，定义为集合 <span class="math">\(D\)</span> 的经验熵 <span class="math">\(H(D)\)</span> 与特征 <span class="math">\(A\)</span> 给定条件下 <span class="math">\(D\)</span> 的经验条件熵 <span class="math">\(H(D|A）\)</span>只差
<div class="math">$$g(D,A)=H(D)-H(D|A)$$</div>
</strong></p>
<p>一般的，熵与条件熵只差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类和特征的互信息。</p>
<p>根据信息增益准则的特征选择方法是：对训练数据集（或子集）<span class="math">\(D\)</span>，计算其每个特征的信息增益，选择信息增益最大的特征。</p>
<p>设训练数据集为 <span class="math">\(D\)</span>，<span class="math">\(|D|\)</span> 表示其样本容量。设有 <span class="math">\(K\)</span> 个类 <span class="math">\(C_k\)</span>，<span class="math">\(k=1,2\cdots,K\)</span>，<span class="math">\(|C_k|\)</span> 为属于类 <span class="math">\(C_k\)</span> 的样本个数。设特征 <span class="math">\(A\)</span> 有 <span class="math">\(n\)</span> 个不同的取值 ，根据特征 <span class="math">\(A\)</span> 的取值将 <span class="math">\(D\)</span> 划分为 <span class="math">\(n\)</span> 个子集，<span class="math">\(|D_i|\)</span> 为 <span class="math">\(D_i\)</span> 的样本个数。记子集 <span class="math">\(D_i\)</span> 中属于类 <span class="math">\(C_k\)</span> 的样本的集合为 <span class="math">\(D_{ik}\)</span>，<span class="math">\(|D_{ik}|\)</span> 为 <span class="math">\(D_{ik}\)</span> 的样本个数，于是信息增益的算法如下</p>
<p><strong>
算法 5.1 （信息增益的算法）<br>
输入：训练数据集 <span class="math">\(D\)</span> 和特征 <span class="math">\(A\)</span><br>
输出：特征 <span class="math">\(A\)</span> 对训练数据集 <span class="math">\(D\)</span> 的信息增益 <span class="math">\(g(D,A)\)</span><br>
(1) 计算数据集 <span class="math">\(D\)</span> 的经验熵 <span class="math">\(H(D)\)</span><br>
<div class="math">$$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$$</div>
(2) 计算特征 <span class="math">\(A\)</span> 对数据集 <span class="math">\(D\)</span> 的经验条件熵 <span class="math">\(H(D|A)\)</span><br>
<div class="math">$$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$$</div>
(3) 计算信息增益
<div class="math">$$g(D,A)=H(D)-H(D|A)$$</div>
</strong></p>
<h3>5.2.3 信息增益比</h3>
<p>以信息增益作为划分训练集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。</p>
<p>首先说明下为何信息增益会偏向选择取值较多的特性：假设特征 <span class="math">\(A\)</span> 有 <span class="math">\(N\)</span> 个不同取值（意味着每一个样本的特征 <span class="math">\(A\)</span> 都有一个不同的值），则易知 <span class="math">\(H(D|A)=0\)</span>，故一定会选取这个特性，但是这个特性（例如仅仅是不同训练数据的标号）可能对分类无用。</p>
<p><strong>
定义 5.3 （信息增益比）特征 <span class="math">\(A\)</span> 对训练数据集 <span class="math">\(D\)</span> 的信息增益比 <span class="math">\(g_R(D,A)\)</span> 定义为其信息增益 <span class="math">\(g(D,A)\)</span> 与训练数据集 <span class="math">\(D\)</span> 关于特征 <span class="math">\(A\)</span> 的值的熵 <span class="math">\(H_A(D)\)</span> 之比
<div class="math">$$g_R(D,A)=\frac{g(D,A)}{H_A{(D)}}$$</div>
其中
<div class="math">$$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$$</div>
<span class="math">\(n\)</span> 是特征 <span class="math">\(A\)</span> 取值的个数。
</strong></p>
<h2>5.3 决策树的生成</h2>
<h3>5.3.1 ID3 算法</h3>
<p>ID3 算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树，直到所有特征的信息增益均很小或没有特征可以选择为止。</p>
<p><strong>
算法 5.2 （ID3 算法）<br>
输入：训练数据集 <span class="math">\(D\)</span>，特征集 <span class="math">\(A\)</span>，阈值 <span class="math">\(\epsilon\)</span><br>
输出：决策树 <span class="math">\(T\)</span><br>
(1) 若 <span class="math">\(D\)</span> 中所有实例属于同一类 <span class="math">\(C_k\)</span>，则 <span class="math">\(T\)</span> 为单节点树，并将类 <span class="math">\(C_k\)</span> 作为该节点的类标记，返回 <span class="math">\(T\)</span><br>
(2) 若 <span class="math">\(A=\varnothing\)</span>，则 <span class="math">\(T\)</span> 为单节点树，并将 <span class="math">\(D\)</span> 中实例数最大的类 <span class="math">\(C_k\)</span> 作为该节点的类标记，返回 <span class="math">\(T\)</span><br>
(3) 否则，按算法 5.1 计算 <span class="math">\(A\)</span> 中哥特征对 <span class="math">\(D\)</span> 的信息增益，选择信息增益最大的特征 <span class="math">\(A_g\)</span><br>
(4) 如果 <span class="math">\(A_g\)</span> 的信息增益小于阈值 <span class="math">\(\epsilon\)</span>，则置 <span class="math">\(T\)</span> 为单节点树，并将 <span class="math">\(D\)</span> 中实例数最大的类 <span class="math">\(C_k\)</span> 作为该节点的类标记，返回 <span class="math">\(T\)</span><br>
(5) 否则，对 <span class="math">\(A_g\)</span> 的每一个可能值 <span class="math">\(a_i\)</span>，依 <span class="math">\(A_g=a_i\)</span> 将 <span class="math">\(D\)</span> 分割为若干非空子集 <span class="math">\(D_i\)</span>，将 <span class="math">\(D_i\)</span> 中实例数最大的类作为标记，构建子节点，由节点及其子节点构成数 <span class="math">\(T\)</span>，返回 <span class="math">\(T\)</span><br>
(6) 对第 <span class="math">\(i\)</span> 个子节点，以 <span class="math">\(D_i\)</span> 为训练集，以 <span class="math">\(A-\{A_g\}\)</span> 为特征集递归的调用步骤 (1)~(5)，得到子树 <span class="math">\(T_i\)</span>，返回 <span class="math">\(T_i\)</span>
</strong></p>
<p>ID3 算法只有树的生成。所以该算法生成的树容易产生过拟合。</p>
<h3>5.3.2 C4.5 的生成算法</h3>
<p>C4.5 算法与 ID3 算法相似，C4.5 算法对 ID3 算法进行了改进，用信息增益比选择特征。其他步骤相同。</p>
<h2>5.4 决策树的剪枝</h2>
<p>决策树生成算法递归的产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。</p>
<p>过拟合产生的原因是学习时过多的考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。</p>
<p>解决这个问题的办法是对已生成的决策树进行简化——剪枝（pruning）。也就是从生成的决策树上剪掉一些子树或者叶节点。</p>
<p>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。</p>
<p>设树 <span class="math">\(T\)</span> 的叶节点个数为 <span class="math">\(|T|\)</span>，<span class="math">\(t\)</span> 是树 <span class="math">\(T\)</span> 的叶节点，该叶节点有 <span class="math">\(N_t\)</span> 个样本点，其中 <span class="math">\(k\)</span> 类的样本点有 <span class="math">\(N_{tk}\)</span> 个，<span class="math">\(k=1,2,\cdots,K\)</span>，<span class="math">\(H_t(T)\)</span> 为叶节点 <span class="math">\(t\)</span> 上的经验熵， <span class="math">\(\alpha\geq0\)</span> 为参数，则决策树学习的损失函数可以定义为
</p>
<div class="math">$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$</div>
<p>
其中经验熵为
</p>
<div class="math">$$H_t(T)=-\sum_{k=1}^K\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}$$</div>
<p>
令
</p>
<div class="math">$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}$$</div>
<p>
则
</p>
<div class="math">$$C_\alpha(T)=C(T)+\alpha|T|$$</div>
<p>
上式中，<span class="math">\(C(T)\)</span> 表示模型对训练数据的预测误差，<span class="math">\(|T|\)</span> 表示模型复杂度，参数 <span class="math">\(\alpha\geq0\)</span> 控制两者之间的影响。</p>
<p>剪枝，就是当 <span class="math">\(\alpha\)</span> 确定时，选择损失函数最小的模型。</p>
<p>决策树生成学习局部的模型，而决策树剪枝学习整体的模型。</p>
<p>上式中定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的最大似然估计进行模型选择。</p>
<p><strong>
算法 5.4 （树的剪枝算法）<br>
输入：生成算法产生的整个树 <span class="math">\(T\)</span>，参数 <span class="math">\(\alpha\)</span><br>
输出：修剪后的子树 <span class="math">\(T_\alpha\)</span><br>
(1) 计算每个节点的经验熵<br>
(2) 递归的从树的叶节点向上回缩：设一组叶节点回缩到其父节点之前与之后的整体树分别为 <span class="math">\(T_B\)</span> 和 <span class="math">\(T_A\)</span>，其对应的损失函数分别是 <span class="math">\(C_\alpha(T_B)\)</span> 和 <span class="math">\(C_\alpha(T_A)\)</span>，如果
<div class="math">$$C_\alpha(T_A)\leq C_\alpha(T_B)$$</div>
则进行剪枝，即将父节点变为新的叶节点<br>
(3) 返回 (2)，直至不能继续为止，得到损失函数最小的子树 <span class="math">\(T_\alpha\)</span>
</strong></p>
<p>因为步骤 (2) 中只考虑两个树的损失函数的差，其计算可以在局部进行，所以可以用动态规划算法实现。</p>
<h2>5.5 CRAT 算法</h2>
<p>分类与回归树（classification and regression tree）模型是应用广泛的决策树学习方法，既可以用于分类也可以用于回归。</p>
<p>CART 是在给定输入随机变量 <span class="math">\(X\)</span> 条件下输出随机变量 <span class="math">\(Y\)</span> 的条件概率分布的学习方法。CART 假设决策树是二叉树，内部节点特征的取值为“是”和“否”。左分的取值是“是”，右分支的取值是“否”。</p>
<p>CRAT 算法由以下两步组成：</p>
<ul>
<li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</li>
<li>决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准</li>
</ul>
<h3>5.5.1 CART 生成</h3>
<p>决策树的生成就是递归的构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。</p>
<h4>回归树的生成</h4>
<p>假设 <span class="math">\(X\)</span> 和 <span class="math">\(Y\)</span> 分别为输入和输出变量，并且 <span class="math">\(Y\)</span> 是连续变量，给定训练数据集
</p>
<div class="math">$$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
一个回归树对应着输入空间（特征空间）的一个划分以及在划分的单元上的输出值。假设已经将输入空间划分为 <span class="math">\(M\)</span> 个单元 <span class="math">\(R_1,R_2,\cdots,R_M\)</span>，并且每个单元 <span class="math">\(R_m\)</span> 上有一个固定的输出值 <span class="math">\(c_m\)</span>，于是，回归树模型可表示为
</p>
<div class="math">$$f(x)=\sum_{m=1}^Mc_m\mathbb{I}(x\in R_m)$$</div>
<p>
当输入空间的划分确定时，可以用平方误差
</p>
<div class="math">$$\sum_{x_i\in R_m}\left(y_i-f(x_i)\right)^2$$</div>
<p>
来表示回归树对于训练数据集的预测误差，用平方误差最下的准则求解每个单元上的最优输出值。</p>
<p>易知，单元 <span class="math">\(R_m\)</span> 上的 <span class="math">\(c_m\)</span> 的最优值 <span class="math">\(\hat{c}_m\)</span> 是 <span class="math">\(R_m\)</span> 上所有输入实例 <span class="math">\(x_i\)</span> 对于的输出 <span class="math">\(y_i\)</span> 的均值，即
</p>
<div class="math">$$\hat{c}_m=\text{ave}(y_i|x_i\in R_m)$$</div>
<p>
问题是怎么对输入空间进行划分。</p>
<p>这里采用启发式的方法，选择第 <span class="math">\(j\)</span> 个特征 <span class="math">\(x^{(i)}\)</span> 和它的取值 <span class="math">\(s\)</span>，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域
</p>
<div class="math">$$\begin{eqnarray}
R_1(j,s) &amp;=&amp; \{x|x^{(j)}\leq s\} \\
R_2(j,s) &amp;=&amp; \{x|x^{(j)}&gt;s\}
\end{eqnarray}$$</div>
<p>
然后对于固定的 <span class="math">\(j\)</span> 可以找到最优切分的 <span class="math">\(s\)</span>，具体的，求解
</p>
<div class="math">$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$</div>
<p>
历遍所有输入变量，找到最优的切分变量 <span class="math">\(j\)</span>，构成一个对 <span class="math">\((j,s)\)</span>。依次将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这种方法生成的回归树通常称为最小二乘回归树（least squares regression tree）。</p>
<p><strong>
算法 5.5 （最小二乘回归树生成算法）<br>
输入：训练数据集 <span class="math">\(D\)</span><br>
输出：回归树 <span class="math">\(f(x)\)</span><br>
在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域的输出值，构建二叉决策树<br>
(1) 选择最优切分变量 <span class="math">\(j\)</span> 和切分点 <span class="math">\(s\)</span>，求解
<div class="math">$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$</div>
遍历 <span class="math">\(j\)</span>，对固定的切分变量 <span class="math">\(j\)</span> 扫描切分点 <span class="math">\(s\)</span>，选择使上式最小的对 <span class="math">\((j,s)\)</span><br>
(2) 用选定的对 <span class="math">\((j,s)\)</span> 划分区域并决定相应的输出值
<div class="math">$$\begin{eqnarray}
R_1(j,s) &amp;=&amp; \{x|x^{(j)}\leq s\} \\
R_2(j,s) &amp;=&amp; \{x|x^{(j)}&gt;s\}
\end{eqnarray}\\
\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,x\in R_m,m=1,2$$</div>
(3) 继续对两个子区域调用步骤 (1)，(2)，直到满足停止条件<br>
(4) 将输入空间划分为 <span class="math">\(M\)</span> 个区域 <span class="math">\(R_1,R_2,\cdots,R_M\)</span>，生成决策树
<div class="math">$$f(x)=\sum_{m=1}^M\hat{c}_m\mathbb{I}(x\in R_m)$$</div>
</strong></p>
<h4>分类树的生成</h4>
<p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<p><strong>
定义 5.4 （基尼指数）分类问题中，假设有 <span class="math">\(K\)</span> 个类，样本点属于第 <span class="math">\(k\)</span> 类的概率为 <span class="math">\(p_k\)</span>，则概率分布的基尼指数定义为
<div class="math">$$\text{Gini}(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$</div>
对于二分类问题，若样本点属于第一个类的概率是 <span class="math">\(p\)</span>，则概率分布的基尼指数为
<div class="math">$$\text{Gini}(p)=2p(1-p)$$</div>
</strong></p>
<p>对于给定的样本集合 <span class="math">\(D\)</span>，其基尼指数为
</p>
<div class="math">$$\text{Gini}(D)=1-\sum_{k=1}^K\left(\frac{|C_k|}{|D|}\right)^2$$</div>
<p>
这里，<span class="math">\(C_k\)</span> 是属于第 <span class="math">\(k\)</span> 类的样本子集，<span class="math">\(K\)</span> 是类的个数。</p>
<p>如果样本集合 <span class="math">\(D\)</span> 根据特征 <span class="math">\(A\)</span> 是否取某一可能值 <span class="math">\(a\)</span> 被分割成 <span class="math">\(D_1\)</span> 和 <span class="math">\(D_2\)</span> 两部分，即
</p>
<div class="math">$$D_1=\{(x,y)\in D|A(x)=a\},D_2=D-D_1$$</div>
<p>
则在特征 A 的条件下，集合 D 的基尼指数定义为
</p>
<div class="math">$$\text{Gini}(D,A)=\frac{|D_1|}{|D|}\text{Gini}(D_1)+\frac{|D_2|}{|D|}\text{Gini}(D_2)$$</div>
<p>
基尼指数 <span class="math">\(\text{Gini}(D)\)</span> 表示集合 <span class="math">\(D\)</span> 的不确定性，基尼指数 <span class="math">\(\text{Gini}(D,A)\)</span> 表示经 <span class="math">\(A=a\)</span> 分割后集合 <span class="math">\(D\)</span> 的不确定性。基尼指数数值越大，样本集合的不确定性也就越大，这一点与熵类似。

<strong>
算法 5.6 （CART 生成算法）<br>
输入：训练数据集 <span class="math">\(D\)</span>，停止计算的条件<br>
输出：CART 决策树<br>
根据训练数据集，从根节点开始，递归的对每个节点进行以下操作，构建二叉决策树：<br>
(1) 设节点的训练数据集为 <span class="math">\(D\)</span>，计算现有特征对该数据集的基尼指数。此时，对每一个特征 <span class="math">\(A\)</span>，对其可能取的每个值 <span class="math">\(a\)</span>，根据样本点对 <span class="math">\(A=a\)</span> 的测试为“是”或“否”将 <span class="math">\(D\)</span> 分割为两部分，计算 <span class="math">\(A=a\)</span> 时的基尼指数<br>
(2) 在所有可能的特征 <span class="math">\(A\)</span> 以及它们所有可能的切分点 <span class="math">\(a\)</span> 中，选择基尼指数最小的特征及其对应的切分点作为最优特征和最优切分点。依最优特征和最优切分点，从现节点生成两个子节点，将训练数据集依特征分配到两个子节点去<br>
(3) 对两个子节点递归的调用 (1)、(2)，直至满足停止条件<br>
(4) 生成 CART 决策树
</strong></p>
<p>算法停止的条件是节点中的样本个数少于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。</p>
<h3>5.5.2 CART 剪枝</h3>
<p>CART 剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。</p>
<p>CART 剪枝算法由两步组成：</p>
<ul>
<li>从生成算法产生的决策树 <span class="math">\(T_0\)</span> 底部开始不断剪枝，直到 <span class="math">\(T_0\)</span> 的根节点，行成一个子树序列
<div class="math">$$\{T_0,T_1,\cdots,T_n\}$$</div>
</li>
<li>通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树</li>
</ul>
<h4>剪枝，形成一个子树序列</h4>
<p>在剪枝过程中，计算子树的损失函数：
</p>
<div class="math">$$C_\alpha(T)=C(T)+\alpha|T|$$</div>
<p>
其中，<span class="math">\(T\)</span> 为任意子树，<span class="math">\(C(T)\)</span> 为对训练数据的预测误差（如基尼指数），<span class="math">\(|T|\)</span> 为子树的叶节点个数，<span class="math">\(\alpha\geq0\)</span> 为参数，<span class="math">\(C_\alpha(T)\)</span> 为参数是 <span class="math">\(\alpha\)</span> 时的子树 <span class="math">\(T\)</span> 的整体损失。参数 <span class="math">\(\alpha\)</span> 权衡训练数据的拟合程度与模型的复杂度。</p>
<p>对固定的 <span class="math">\(\alpha\)</span>，一定存在使损失函数 <span class="math">\(C_\alpha(T)\)</span> 最小的子树，将其表示为 <span class="math">\(T_\alpha\)</span>。<span class="math">\(T_\alpha\)</span> 在损失函数 <span class="math">\(C_\alpha(T)\)</span> 最小的意义下是最优的。容易验证这样的最优子树是唯一的。当 <span class="math">\(\alpha\)</span> 大的时候，最优子树 <span class="math">\(T_\alpha\)</span> 偏小；当 <span class="math">\(\alpha\)</span> 小的时候，最优子树 <span class="math">\(T_\alpha\)</span> 偏大。极端情况下，如果 <span class="math">\(\alpha=0\)</span>，那么整体树是最优的。当 <span class="math">\(\alpha\to+\infty\)</span> 时，根节点组成的单节点树是最优的。</p>
<p>Breiman 等人证明：可以用递归的方法对树进行剪枝.将 <span class="math">\(\alpha\)</span> 从小增大：
</p>
<div class="math">$$0=\alpha_0&lt;\alpha_1&lt;\cdots&lt;\alpha_n&lt;+\infty$$</div>
<p>
产生一系列的区间：
</p>
<div class="math">$$[\alpha_i,\alpha_{i+1}),i=0,1,\cdots,n$$</div>
<p>
剪枝得到的子树序列对应着区间 <span class="math">\(\alpha\in[\alpha_i,\alpha_{i+1}),i=0,1,\cdots,n\)</span> 的最优子树序列：
</p>
<div class="math">$$\{T_0,T_1,\cdots,T_n\}$$</div>
<p>
序列中的子树是嵌套的。</p>
<p>具体的，从整体树 <span class="math">\(T_0\)</span> 开始剪枝。对 <span class="math">\(T_0\)</span> 的任意内部节点 <span class="math">\(t\)</span>，以 <span class="math">\(t\)</span> 为单节点树的损失函数是
</p>
<div class="math">$$C_\alpha(t)=C(t)+\alpha$$</div>
<p>
以 <span class="math">\(t\)</span> 为根节点的子树 <span class="math">\(T_t\)</span> 的损失函数是
</p>
<div class="math">$$C_\alpha(T_t)=C(T_t)+\alpha|T_t|$$</div>
<p>
当 <span class="math">\(\alpha=0\)</span> 及 <span class="math">\(\alpha\)</span> 充分小时，有不等式
</p>
<div class="math">$$C_\alpha(T_t)&lt;C_\alpha(t)$$</div>
<p>
当 <span class="math">\(\alpha\)</span> 增大时，在某一 <span class="math">\(\alpha\)</span> 有
</p>
<div class="math">$$C_\alpha(T_t)=C_\alpha(t)$$</div>
<p>
当 <span class="math">\(\alpha\)</span> 再增大时，有
</p>
<div class="math">$$C_\alpha(T_t)&gt;C_\alpha(t)$$</div>
<p>综上所述，只要
</p>
<div class="math">$$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$$</div>
<p>
则 <span class="math">\(T_t\)</span> 与 <span class="math">\(t\)</span> 有相同的损失函数值，而 <span class="math">\(t\)</span> 的节点少，对 <span class="math">\(T_t\)</span> 进行剪枝。</p>
<p>所以，对 <span class="math">\(T_0\)</span> 中每一内部节点 <span class="math">\(t\)</span>，计算
</p>
<div class="math">$$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$$</div>
<p>
它表示剪枝后整体损失函数减少的程度。在 <span class="math">\(T_0\)</span> 中剪去 <span class="math">\(g(t)\)</span> 最小的 <span class="math">\(T_t\)</span>（这里我们是要得到一系列的 <span class="math">\(\alpha\)</span> 区间，所以是剪去最小的），将得到的子树作为 <span class="math">\(T_1\)</span>，同时将最小的 <span class="math">\(g(t)\)</span> 设为 <span class="math">\(\alpha_1\)</span>。<span class="math">\(T_1\)</span> 为区间 <span class="math">\([\alpha_1,\alpha_2)\)</span> 的最优子树。</p>
<p>如此剪枝下去，直到得到根节点。在这一过程中，不断的增加 <span class="math">\(\alpha\)</span> 的值，产生新的区间。</p>
<h4>在剪枝得到的子树序列中通过交叉验证选取最优子树 <span class="math">\(T_\alpha\)</span></h4>
<p>具体的，利用独立的验证数据集，测试子树序列中各棵子树的平方误差或基尼指数。平方误差或基尼指数小的决策树被认为是最优的决策树。</p>
<p><strong>
算法 5.7 （CART 剪枝算法）<br>
输入：CART 算法生成的决策树 <span class="math">\(T_0\)</span><br>
输出：最优决策树 <span class="math">\(T_\alpha\)</span><br>
(1) 设 <span class="math">\(k=0,T=T_0\)</span><br>
(2) 设 <span class="math">\(\alpha=+\infty\)</span>
(3) 自下而上的对各个内部节点 <span class="math">\(t\)</span> 计算 <span class="math">\(C(T_t)\)</span>，<span class="math">\(|T_t|\)</span> 以及
<div class="math">$$\begin{eqnarray}
g(t) &amp;=&amp; \frac{C(t)-C(T_t)}{|T_t|-1} \\
\alpha &amp;=&amp; \min(\alpha,g(t))
\end{eqnarray}$$</div>
这里，<span class="math">\(T_t\)</span> 表示以 <span class="math">\(t\)</span> 为根节点的子树，<span class="math">\(C(T_t)\)</span> 是对训练数据的预测误差，<span class="math">\(|T_t|\)</span> 是 <span class="math">\(T_t\)</span> 的叶节点个数<br>
(4) 对 <span class="math">\(g(t)=\alpha\)</span> 的内部节点 <span class="math">\(t\)</span> 进行剪枝，并对叶节点 <span class="math">\(t\)</span> 以多数表决法决定其类，得到树 <span class="math">\(T\)</span><br>
(5) 设 <span class="math">\(k=k+1.\alpha_k=\alpha,T_k=T\)</span><br>
(6) 如果 <span class="math">\(T_k\)</span> 不是由根节点及两个叶节点构成的树，则返回到步骤 (3)；否则，令 <span class="math">\(T_k=T_n\)</span><br>
(7) 采用交叉验证法在子树序列 <span class="math">\(T_0,T_1,\cdots,T_n\)</span> 中选择最优子树 <span class="math">\(T_\alpha\)</span>
</strong></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <aside>
                    <hr />
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/07/sl-4/">
                                    统计学习方法 第四章 朴素贝叶斯法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/05/bayes/">
                                    贝叶斯定理
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/sl-3/">
                                    统计学习方法 第三章 k 近邻法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/sl-2/">
                                    统计学习方法 第二章 感知机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/sl-1/">
                                    统计学习方法 第一章 统计学习方法概论
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="newer">
                        <h1>
                            <font color="#771515"><em>NEWER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/09/sl-6/">
                                    统计学习方法 第六章 逻辑回归与最大熵模型
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/10/lagrange_duality/">
                                    拉格朗日对偶性
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/10/directional_derivative-gradient/">
                                    方向导数和梯度
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/11/sl-7_1/">
                                    统计学习方法 第七章 支持向量机（1）——线性可分支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/11/sl-7_2/">
                                    统计学习方法 第七章 支持向量机（2）——线性支持向量机
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <!-- Gitalk 评论 start  -->

                    <!-- Link Gitalk 的支持文件  -->
                    <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                    <div id="gitalk-container"></div>
                    <script type="text/javascript">
                        var dateTime = Date.now();
                        var timestamp = Math.floor(dateTime / 1000);
                        var gitalk = new Gitalk({

                            // gitalk的主要参数
                            clientID: '93f43349e9fd3154bfad',
                            clientSecret: 'd6d09d1d7261f6b62f46b39e5fcace85b81c3cd7',
                            repo: 'xutree.github.io',
                            owner: 'xutree',
                            admin: ['xutree'],
                            id: String(timestamp)

                        });
                        gitalk.render('gitalk-container');
                    </script> -->
                    <!-- Gitalk end -->
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-07T16:52:41+08:00">2018-11-07 16:52:41</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-09 14:49:41</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>18</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>15</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>