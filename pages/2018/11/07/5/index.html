<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第五章 决策树  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
        <link href="https://xutree.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - Full Atom Feed" />
        <link href="https://xutree.github.io/feeds/读书笔记.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 读书笔记 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/基础知识.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 基础知识 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/教程.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 教程 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/其他.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 其他 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/趣闻.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 趣闻 Category Atom Feed" />
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/07/5/"> 统计学习方法 第五章 决策树  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
            <div class="span8 offset2 article-content">

                <p>决策树（decision tree）是一种基本的分类与回归方法。其主要优点是模型具有可读性，分类速度快。决策树学习通常包括三个步骤：特征选择、决策树的生成和决策树的修剪。</p>
<h2>5.1 决策树模型与学习</h2>
<h3>5.1.1 决策树模型</h3>
<p><strong>
定义 5.1 （决策树）分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点（node）和有向边（directed edge）组成。节点有两种类型：内部节点（internal node）和叶节点（leaf node）。内部节点表示一个特征或属性，叶节点表示一个类。
</strong></p>
<h3>5.1.2 决策树学习</h3>
<p>假设给定训练数据集
</p>
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
其中输入实例（向量）为
</p>
<div class="math">$$x_i=\left(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)},\right)$$</div>
<p>
<span class="math">\(n\)</span> 为特征个数。类标记为
</p>
<div class="math">$$y_i\in\{c_1,c_2,\cdots,c_K\}$$</div>
<p>
其中 <span class="math">\(i\in\{1,2,\cdots,N\}\)</span>，<span class="math">\(N\)</span> 为样本容量。</p>
<p>学习的目的是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。</p>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练集不相矛盾的决策树可能有多个也可能一个也没有，我们需要的是一个与训练集矛盾较小的决策树，同时具有很好的泛化能力。</p>
<p>决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。</p>
<p>当损失函数确定以后，学习问题变成在损失函数意义下选择最有决策树的问题，因为从所有可能的决策树中选取最优决策树是 NP 完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优（sub-optimal）的。</p>
<h2>5.2 特征选择</h2>
<h3>5.2.1 特征选择的问题</h3>
<p>特征选择在于选取对训练数据具有分类能力的特征。这样就可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。通常特征选取的准则是信息增益或信息增益比。</p>
<h3>5.2.2 信息增益</h3>
<p>为了便于说明，首先给出熵与条件熵的定义。</p>
<p>熵（entropy）是表示随机变量不确定性的度量。</p>
<p>设 <span class="math">\(X\)</span> 是一个离散随机变量，其概率分布为
</p>
<div class="math">$$P(X=x_i)=p_i,i=1,2,\cdots,n$$</div>
<p>
则，随机变量 <span class="math">\(X\)</span> 的熵定义为
</p>
<div class="math">$$H(X)=-\sum_{i=1}^np_i\log p_i$$</div>
<p>
若 <span class="math">\(p_i=0\)</span>，定义 <span class="math">\(0\log0=0\)</span>。通常，上式中对数的底为 2 或自然对数 e，这是熵的单位分别为比特（bit）或纳特（nat）。</p>
<p>设有随机变量 <span class="math">\((X,Y)\)</span>，其联合概率分布为
</p>
<div class="math">$$P(X=x_i,Y=y_j)=p_{ij},i=1,2,\cdots,n;j=1,2,\cdots,m$$</div>
<p>
条件熵（conditional entropy） <span class="math">\(H(Y|X)\)</span> 表示在已知随机变量 <span class="math">\(X\)</span> 的条件下随机变量 <span class="math">\(Y\)</span> 的不确定性，定义为 <span class="math">\(X\)</span> 给定条件下 <span class="math">\(Y\)</span> 的条件概率分布的熵对 <span class="math">\(X\)</span> 的数学期望
</p>
<div class="math">$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$</div>
<p>
这里 <span class="math">\(p_i=P(X=x_i),i=1,2,\cdots,n\)</span></p>
<p>当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。</p>
<p>信息增益（information gain）表示得知特征 <span class="math">\(X\)</span> 的信息而使类 <span class="math">\(Y\)</span> 的信息的不确定度减少的程度。</p>
<p><strong>
定义 5.2 （信息增益）特征 <span class="math">\(A\)</span> 对训练数据集 <span class="math">\(D\)</span> 的信息增益 <span class="math">\(g(D,A)\)</span>，定义为集合 <span class="math">\(D\)</span> 的经验熵 <span class="math">\(H(D)\)</span> 与特征 <span class="math">\(A\)</span> 给定条件下 <span class="math">\(D\)</span> 的经验条件熵 <span class="math">\(H(D|A）\)</span>只差
<div class="math">$$g(D,A)=H(D)-H(D|A)$$</div>
</strong></p>
<p>一般的，熵与条件熵只差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类和特征的互信息。</p>
<p>根据信息增益准则的特征选择方法是：对训练数据集（或子集）<span class="math">\(D\)</span>，计算其每个特征的信息增益，选择信息增益最大的特征。</p>
<p>设训练数据集为 <span class="math">\(D\)</span>，<span class="math">\(|D|\)</span> 表示其样本容量。设有 <span class="math">\(K\)</span> 个类 <span class="math">\(C_k\)</span>，<span class="math">\(k=1,2\cdots,K\)</span>，<span class="math">\(|C_k|\)</span> 为属于类 <span class="math">\(C_k\)</span> 的样本个数。设特征 <span class="math">\(A\)</span> 有 <span class="math">\(n\)</span> 个不同的取值 ，根据特征 <span class="math">\(A\)</span> 的取值将 <span class="math">\(D\)</span> 划分为 <span class="math">\(n\)</span> 个子集，<span class="math">\(|D_i|\)</span> 为 <span class="math">\(D_i\)</span> 的样本个数。记子集 <span class="math">\(D_i\)</span> 中属于类 <span class="math">\(C_k\)</span> 的样本的集合为 <span class="math">\(D_{ik}\)</span>，<span class="math">\(|D_{ik}|\)</span> 为 <span class="math">\(D_{ik}\)</span> 的样本个数，于是信息增益的算法如下</p>
<p><strong>
算法 5.1 （信息增益的算法）<br>
输入：训练数据集 <span class="math">\(D\)</span> 和特征 <span class="math">\(A\)</span><br>
输出：特征 <span class="math">\(A\)</span> 对训练数据集 <span class="math">\(D\)</span> 的信息增益 <span class="math">\(g(D,A)\)</span><br>
(1) 计算数据集 <span class="math">\(D\)</span> 的经验熵 <span class="math">\(H(D)\)</span><br>
<div class="math">$$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$$</div>
(2) 计算特征 <span class="math">\(A\)</span> 对数据集 <span class="math">\(D\)</span> 的经验条件熵 <span class="math">\(H(D|A)\)</span><br>
<div class="math">$$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$$</div>
(3) 计算信息增益
<div class="math">$$g(D,A)=H(D)-H(D|A)$$</div>
</strong></p>
<h3>5.2.3 信息增益比</h3>
<p>以信息增益作为划分训练集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。</p>
<p>首先说明下为何信息增益会偏向选择取值较多的特性：假设特征 <span class="math">\(A\)</span> 有 <span class="math">\(N\)</span> 个不同取值（意味着每一个样本的特征 <span class="math">\(A\)</span> 都有一个不同的值），则易知 <span class="math">\(H(D|A)=0\)</span>，故一定会选取这个特性，但是这个特性（例如仅仅是不同训练数据的标号）可能对分类无用。</p>
<p><strong>
定义 5.3 （信息增益比）特征 <span class="math">\(A\)</span> 对训练数据集 <span class="math">\(D\)</span> 的信息增益比 <span class="math">\(g_R(D,A)\)</span> 定义为其信息增益 <span class="math">\(g(D,A)\)</span> 与训练数据集 <span class="math">\(D\)</span> 关于特征 <span class="math">\(A\)</span> 的值的熵 <span class="math">\(H_A(D)\)</span> 之比
<div class="math">$$g_R(D,A)=\frac{g(D,A)}{H_A{(D)}}$$</div>
其中
<div class="math">$$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$$</div>
<span class="math">\(n\)</span> 是特征 <span class="math">\(A\)</span> 取值的个数。
</strong></p>
<h2>5.3 决策树的生成</h2>
<h3>5.3.1 ID3 算法</h3>
<p>ID3 算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树，直到所有特征的信息增益均很小或没有特征可以选择为止。</p>
<p><strong>
算法 5.2 （ID3 算法）<br>
输入：训练数据集 <span class="math">\(D\)</span>，特征集 <span class="math">\(A\)</span>，阈值 <span class="math">\(\epsilon\)</span><br>
输出：决策树 <span class="math">\(T\)</span><br>
(1) 若 <span class="math">\(D\)</span> 中所有实例属于同一类 <span class="math">\(C_k\)</span>，则 <span class="math">\(T\)</span> 为单节点树，并将类 <span class="math">\(C_k\)</span> 作为该节点的类标记，返回 <span class="math">\(T\)</span><br>
(2) 若 <span class="math">\(A=\varnothing\)</span>，则 <span class="math">\(T\)</span> 为单节点树，并将 <span class="math">\(D\)</span> 中实例数最大的类 <span class="math">\(C_k\)</span> 作为该节点的类标记，返回 <span class="math">\(T\)</span><br>
(3) 否则，按算法 5.1 计算 <span class="math">\(A\)</span> 中哥特征对 <span class="math">\(D\)</span> 的信息增益，选择信息增益最大的特征 <span class="math">\(A_g\)</span><br>
(4) 如果 <span class="math">\(A_g\)</span> 的信息增益小于阈值 <span class="math">\(\epsilon\)</span>，则置 <span class="math">\(T\)</span> 为单节点树，并将 <span class="math">\(D\)</span> 中实例数最大的类 <span class="math">\(C_k\)</span> 作为该节点的类标记，返回 <span class="math">\(T\)</span><br>
(5) 否则，对 <span class="math">\(A_g\)</span> 的每一个可能值 <span class="math">\(a_i\)</span>，依 <span class="math">\(A_g=a_i\)</span> 将 <span class="math">\(D\)</span> 分割为若干非空子集 <span class="math">\(D_i\)</span>，将 <span class="math">\(D_i\)</span> 中实例数最大的类作为标记，构建子节点，由节点及其子节点构成数 <span class="math">\(T\)</span>，返回 <span class="math">\(T\)</span><br>
(6) 对第 <span class="math">\(i\)</span> 个子节点，以 <span class="math">\(D_i\)</span> 为训练集，以 <span class="math">\(A-\{A_g\}\)</span> 为特征集递归的调用步骤 (1)~(5)，得到子树 <span class="math">\(T_i\)</span>，返回 <span class="math">\(T_i\)</span>
</strong></p>
<p>ID3 算法只有树的生成。所以该算法生成的树容易产生过拟合。</p>
<h3>5.3.2 C4.5 的生成算法</h3>
<p>C4.5 算法与 ID3 算法相似，C4.5 算法对 ID3 算法进行了改进，用信息增益比选择特征。其他步骤相同。</p>
<h2>5.4 决策树的剪枝</h2>
<p>决策树生成算法递归的产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。</p>
<p>过拟合产生的原因是学习时过多的考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。</p>
<p>解决这个问题的办法是对已生成的决策树进行简化——剪枝（pruning）。也就是从生成的决策树上剪掉一些子树或者叶节点。</p>
<p>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。</p>
<p>设树 <span class="math">\(T\)</span> 的叶节点个数为 <span class="math">\(|T|\)</span>，<span class="math">\(t\)</span> 是树 <span class="math">\(T\)</span> 的叶节点，该叶节点有 <span class="math">\(N_t\)</span> 个样本点，其中 <span class="math">\(k\)</span> 类的样本点有 <span class="math">\(N_{tk}\)</span> 个，<span class="math">\(k=1,2,\cdots,K\)</span>，<span class="math">\(H_t(T)\)</span> 为叶节点 <span class="math">\(t\)</span> 上的经验熵， <span class="math">\(\alpha\geq0\)</span> 为参数，则决策树学习的损失函数可以定义为
</p>
<div class="math">$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$</div>
<p>
其中经验熵为
</p>
<div class="math">$$H_t(T)=-\sum_{k=1}^K\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}$$</div>
<p>
令
</p>
<div class="math">$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}$$</div>
<p>
则
</p>
<div class="math">$$C_\alpha(T)=C(T)+\alpha|T|$$</div>
<p>
上式中，<span class="math">\(C(T)\)</span> 表示模型对训练数据的预测误差，<span class="math">\(|T|\)</span> 表示模型复杂度，参数 <span class="math">\(\alpha\geq0\)</span> 控制两者之间的影响。</p>
<p>剪枝，就是当 <span class="math">\(\alpha\)</span> 确定时，选择损失函数最小的模型。</p>
<p>决策树生成学习局部的模型，而决策树剪枝学习整体的模型。</p>
<p>上式中定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的最大似然估计进行模型选择。</p>
<p><strong>
算法 5.4 （树的剪枝算法）<br>
输入：生成算法产生的整个树 <span class="math">\(T\)</span>，参数 <span class="math">\(\alpha\)</span><br>
输出：修剪后的子树 <span class="math">\(T_\alpha\)</span><br>
(1) 计算每个节点的经验熵<br>
(2) 递归的从树的叶节点向上回缩：设一组叶节点回缩到其父节点之前与之后的整体树分别为 <span class="math">\(T_B\)</span> 和 <span class="math">\(T_A\)</span>，其对应的损失函数分别是 <span class="math">\(C_\alpha(T_B)\)</span> 和 <span class="math">\(C_\alpha(T_A)\)</span>，如果
<div class="math">$$C_\alpha(T_A)\leq C_\alpha(T_B)$$</div>
则进行剪枝，即将父节点变为新的叶节点<br>
(3) 返回 (2)，直至不能继续为止，得到损失函数最小的子树 <span class="math">\(T_\alpha\)</span>
</strong></p>
<p>因为步骤 (2) 中只考虑两个树的损失函数的差，其计算可以在局部进行，所以可以用动态规划算法实现。</p>
<h2>5.5 CRAT 算法</h2>
<p>分类与回归树（classification and regression tree）模型是应用广泛的决策树学习方法，既可以用于分类也可以用于回归。</p>
<p>CART 是在给定输入随机变量 <span class="math">\(X\)</span> 条件下输出随机变量 <span class="math">\(Y\)</span> 的条件概率分布的学习方法。CART 假设决策树是二叉树，内部节点特征的取值为“是”和“否”。左分的取值是“是”，右分支的取值是“否”。</p>
<p>CRAT 算法由以下两步组成：</p>
<ul>
<li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</li>
<li>决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准</li>
</ul>
<h3>5.5.1 CART 生成</h3>
<p>决策树的生成就是递归的构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。</p>
<h4>回归树的生成</h4>
<p>假设 <span class="math">\(X\)</span> 和 <span class="math">\(Y\)</span> 分别为输入和输出变量，并且 <span class="math">\(Y\)</span> 是连续变量，给定训练数据集
</p>
<div class="math">$$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
一个回归树对应着输入空间（特征空间）的一个划分以及在划分的单元上的输出值。假设已经将输入空间划分为 <span class="math">\(M\)</span> 个单元 <span class="math">\(R_1,R_2,\cdots,R_M\)</span>，并且每个单元 <span class="math">\(R_m\)</span> 上有一个固定的输出值 <span class="math">\(c_m\)</span>，于是，回归树模型可表示为
</p>
<div class="math">$$f(x)=\sum_{m=1}^Mc_m\mathbb{I}(x\in R_m)$$</div>
<p>
当输入空间的划分确定时，可以用平方误差
</p>
<div class="math">$$\sum_{x_i\in R_m}\left(y_i-f(x_i)\right)^2$$</div>
<p>
来表示回归树对于训练数据集的预测误差，用平方误差最下的准则求解每个单元上的最优输出值。</p>
<p>易知，单元 <span class="math">\(R_m\)</span> 上的 <span class="math">\(c_m\)</span> 的最优值 <span class="math">\(\hat{c}_m\)</span> 是 <span class="math">\(R_m\)</span> 上所有输入实例 <span class="math">\(x_i\)</span> 对于的输出 <span class="math">\(y_i\)</span> 的均值，即
</p>
<div class="math">$$\hat{c}_m=\text{ave}(y_i|x_i\in R_m)$$</div>
<p>
问题是怎么对输入空间进行划分。</p>
<p>这里采用启发式的方法，选择第 <span class="math">\(j\)</span> 个特征 <span class="math">\(x^{(i)}\)</span> 和它的取值 <span class="math">\(s\)</span>，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域
</p>
<div class="math">$$\begin{eqnarray}
R_1(j,s) &amp;=&amp; \{x|x^{(j)}\leq s\} \\
R_2(j,s) &amp;=&amp; \{x|x^{(j)}&gt;s\}
\end{eqnarray}$$</div>
<p>
然后对于固定的 <span class="math">\(j\)</span> 可以找到最优切分的 <span class="math">\(s\)</span>，具体的，求解
</p>
<div class="math">$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$</div>
<p>
历遍所有输入变量，找到最优的切分变量 <span class="math">\(j\)</span>，构成一个对 <span class="math">\((j,s)\)</span>。依次将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这种方法生成的回归树通常称为最小二乘回归树（least squares regression tree）。</p>
<p><strong>
算法 5.5 （最小二乘回归树生成算法）<br>
输入：训练数据集 <span class="math">\(D\)</span><br>
输出：回归树 <span class="math">\(f(x)\)</span><br>
在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域的输出值，构建二叉决策树<br>
(1) 选择最优切分变量 <span class="math">\(j\)</span> 和切分点 <span class="math">\(s\)</span>，求解
<div class="math">$$\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]$$</div>
遍历 <span class="math">\(j\)</span>，对固定的切分变量 <span class="math">\(j\)</span> 扫描切分点 <span class="math">\(s\)</span>，选择使上式最小的对 <span class="math">\((j,s)\)</span><br>
(2) 用选定的对 <span class="math">\((j,s)\)</span> 划分区域并决定相应的输出值
<div class="math">$$\begin{eqnarray}
R_1(j,s) &amp;=&amp; \{x|x^{(j)}\leq s\} \\
R_2(j,s) &amp;=&amp; \{x|x^{(j)}&gt;s\}
\end{eqnarray}\\
\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,x\in R_m,m=1,2$$</div>
(3) 继续对两个子区域调用步骤 (1)，(2)，直到满足停止条件<br>
(4) 将输入空间划分为 <span class="math">\(M\)</span> 个区域 <span class="math">\(R_1,R_2,\cdots,R_M\)</span>，生成决策树
<div class="math">$$f(x)=\sum_{m=1}^M\hat{c}_m\mathbb{I}(x\in R_m)$$</div>
</strong></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <aside>
                    <hr />
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/07/4/">
                                    统计学习方法 第四章 朴素贝叶斯法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/05/bayes/">
                                    贝叶斯定理
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/3/">
                                    统计学习方法 第三章 k 近邻法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/2/">
                                    统计学习方法 第二章 感知机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/1/">
                                    统计学习方法 第一章 统计学习方法概论
                                </a>
                            </li>
                        </ul>
                    </nav>
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-07T16:52:41+08:00">2018-11-07 16:52:41</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-07 22:05:12</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>5</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>5</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>