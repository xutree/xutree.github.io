<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第十章 隐马尔科夫模型  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
        <link href="https://xutree.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - Full Atom Feed" />
        <link href="https://xutree.github.io/feeds/读书笔记.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 读书笔记 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/基础知识.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 基础知识 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/教程.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 教程 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/其他.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 其他 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/趣闻.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 趣闻 Category Atom Feed" />
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/19/sl-10/"> 统计学习方法 第十章 隐马尔科夫模型  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
            <div class="span8 offset2 article-content">

                <p>隐马尔科夫模型（hidden Markov model，HMM）是可用于标注问题的统计学模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。隐马尔科夫模型在语音识别、自然语言处理、生物信息、模式识别等领域有着广泛的应用。</p>
<h2>10.1 隐马尔科夫模型的基本概念</h2>
<h3>10.1.1 隐马尔科夫模型的定义</h3>
<p><strong>
定义 10.1（隐马尔科夫模型的定义）隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔科夫链随机生成的状态的序列称为状态序列（state sequence）；每个状态生成一个观测，由此产生的观测随机序列称为观测序列（observation sequence）。序列的每一个位置又可以看作是一个时刻
</strong></p>
<p>隐马尔科夫模型由初始概率分布、状态转移概率分布以及观测概率分布确定。隐马尔科夫模型的形式定义如下：</p>
<p>设 <span class="math">\(Q\)</span> 是所有可能的状态的集合，<span class="math">\(V\)</span> 是所有可能的观测的集合
</p>
<div class="math">$$Q=\{q_1,q_2,\cdots,q_N\},\ V=\{v_1,v_2,\cdots,v_M\}$$</div>
<p>
其中，<span class="math">\(N\)</span> 是可能的状态数，<span class="math">\(M\)</span> 是可能的观测数。</p>
<p><span class="math">\(I\)</span> 是长度为 <span class="math">\(T\)</span> 的状态序列，<span class="math">\(O\)</span> 是对应的观测序列
</p>
<div class="math">$$I=\{i_1,i_2,\cdots,i_T\},\ O=\{o_1,o_2,\cdots,o_T\}$$</div>
<p><span class="math">\(A\)</span> 是状态转移概率矩阵
</p>
<div class="math">$$A=\left[a_{ij}\right]_{N\times N}$$</div>
<p>
其中
</p>
<div class="math">$$a_{ij}=P(i_{t+1}=q_j|i_t=q_i),\ i=1,2,\cdots,N;\ j=1,2,\cdots,N$$</div>
<p>
是在 <span class="math">\(t\)</span> 时刻处于状态 <span class="math">\(q_i\)</span> 的条件下 <span class="math">\(t+1\)</span> 时刻转移到状态 <span class="math">\(q_j\)</span> 的概率。</p>
<p><span class="math">\(B\)</span> 是观测概率矩阵
</p>
<div class="math">$$B=\left[b_j(k)\right]_{N\times M}$$</div>
<p>
其中
</p>
<div class="math">$$b_j(k)=P(o_t=v_k|i_t=q_j),\ k=1,2,\cdots,M;\ j=1,2,\cdots,N$$</div>
<p>
是在 <span class="math">\(t\)</span> 时刻处于状态 <span class="math">\(q_j\)</span> 的条件下观测生成 <span class="math">\(v_k\)</span> 的概率。</p>
<p><span class="math">\(\pi\)</span> 是初始状态概率向量
</p>
<div class="math">$$\pi=(\pi_i)$$</div>
<p>
其中
</p>
<div class="math">$$\pi_i=P(i_1=q_i),\ i=1,2,\cdots,N$$</div>
<p>
是时刻 <span class="math">\(t=1\)</span> 处于状态 <span class="math">\(q_i\)</span> 的概率。</p>
<p>隐马尔科夫模型由初始状态概率向量 <span class="math">\(\pi\)</span>，状态转移概率矩阵 <span class="math">\(A\)</span> 和观测概率矩阵 <span class="math">\(B\)</span> 决定。<span class="math">\(\pi\)</span> 和 <span class="math">\(A\)</span> 决定状态序列，<span class="math">\(B\)</span> 决定观测序列。隐马尔科夫模型 <span class="math">\(\lambda\)</span> 可以用三元符号表示，即
</p>
<div class="math">$$\lambda=(A,B,\pi)$$</div>
<p>
称为隐马尔科夫模型的三要素。</p>
<p>隐马尔科夫模型作了两个基本假设：</p>
<p>a. 齐次马尔科夫性假设
</p>
<div class="math">$$P(i_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i_t|i_{t-1}),\ t=1,2,\cdots,T$$</div>
<p>
b. 观测独立性假设
</p>
<div class="math">$$P(o_t|i_T,o_T,i_{T-1},o_{T-1},\cdots,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(o_t|i_t)$$</div>
<p>隐马尔科夫模型可以用于标注问题，这时状态对应着标记。标注问题是给定观测的序列预测其对应的标记序列。可以假设标注问题的数据是由隐马尔科夫模型生成的。这样我们可以利用隐马尔科夫模型的学习与预测算法进行标注。</p>
<h3>10.1.2 观测序列的生成过程</h3>
<p><strong>
算法 10.1（观测序列的生成)<br>
输入：隐马尔科夫模型 <span class="math">\(\lambda=(A,B,\pi)\)</span>，观测序列长度 <span class="math">\(T\)</span><br>
输出：观测序列 <span class="math">\(O=(o_1,o_2,\cdots,o_T)\)</span><br>
(1) 按照初始状态分布 <span class="math">\(\pi\)</span> 产生状态 <span class="math">\(i_1\)</span><br>
(2) 令 <span class="math">\(t=1\)</span><br>
(3) 按照状态 <span class="math">\(i_t\)</span> 的观测概率分布 <span class="math">\(b_{i_t}(k)\)</span> 生成 <span class="math">\(o_t\)</span><br>
(4) 按照状态 <span class="math">\(i_t\)</span> 的状态转移概率分布 <span class="math">\(\{a_{i_ti_{t+1}}\}\)</span> 产生状态 <span class="math">\(i_{t+1}\)</span>，<span class="math">\(i_{t+1}=1,2,\cdots,,N\)</span><br>
(5) 令 <span class="math">\(t=t+1\)</span>；如果 <span class="math">\(t&lt;T\)</span>，转 (3)；否则，终止
</strong></p>
<h3>10.1.3 隐马尔科夫模型的 3 个基本问题</h3>
<p><span class="math">\(\blacksquare\)</span> 概率计算问题</p>
<p>给定模型 <span class="math">\(\lambda=(A,B,\pi)\)</span> 和观测序列 <span class="math">\(O=(o_1,o_2,\cdots,o_T)\)</span>，计算在模型 <span class="math">\(\lambda\)</span> 下观测序列 <span class="math">\(O\)</span> 出现的概率 <span class="math">\(P(O|\lambda)\)</span> 。</p>
<p><span class="math">\(\blacksquare\)</span> 学习问题</p>
<p>已知观测序列 <span class="math">\(O=(o_1,o_2,\cdots,o_T)\)</span>，估计模型参数 <span class="math">\(\lambda=(A,B,\pi)\)</span>，使得在该模型下观测序列 <span class="math">\(P(O|\lambda)\)</span> 最大。即用极大似然估计的方法估计参数。</p>
<p><span class="math">\(\blacksquare\)</span> 预测问题</p>
<p>也称为解码（decoding）问题。已知模型 <span class="math">\(\lambda=(A,B,\pi)\)</span> 和观测序列 <span class="math">\(O=(o_1,o_2,\cdots,o_T)\)</span>，求对给定观测序列条件概率 <span class="math">\(P(I|O)\)</span> 最大的状态序列 <span class="math">\(I=(i_1,i_2,\cdots,i_T)\)</span>。即给定观测序列，求最可能的对应的状态序列。</p>
<h2>10.2 概率计算算法</h2>
<h3>10.2.1 前向算法</h3>
<p><strong>
定义 10.2（前向概率）给定隐马尔科夫模型 <span class="math">\(\lambda\)</span>，定义到时刻 <span class="math">\(t\)</span> 部分观测序列为 <span class="math">\(o_1,o_2,\cdots,o_t\)</span> 且状态为 <span class="math">\(q_i\)</span> 的概率为前向概率，记作
<div class="math">$$\alpha_i(t)=P(o_1,o_2,\cdots,o_t,t_i=q_i|\lambda)$$</div>
可以递推地求得前向概率 <span class="math">\(\alpha_t(i)\)</span> 及观测序列概率 <span class="math">\(P(O|\lambda)\)</span>。
</strong></p>
<p><strong>
算法 10.2（观测序列概率的前向算法）<br>
输入：隐马尔科夫模型 <span class="math">\(\lambda\)</span>，观测序列 <span class="math">\(O\)</span><br>
输出：观测序列概率 <span class="math">\(P(O|\lambda)\)</span> <br>
(1) 初值
<div class="math">$$\alpha_1(i)=\pi_ib_i(o_1),\ i=1,2,\cdots,N$$</div>
(2) 递推
<div class="math">$$\alpha_{t+1}(i)=\left[\sum_{j=1}^N\alpha_t(j)a_{ji}\right]b_i(o_{t+1}),\ i=1,2,\cdots,N$$</div>
(3) 终止
<div class="math">$$P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)$$</div>
</strong></p>
<p>利用前向概率计算 <span class="math">\(P(O|\lambda)\)</span> 的计算量是 <span class="math">\(O(N^2T)\)</span> 阶的，而不是直接计算的 <span class="math">\(O(TN^T)\)</span> 阶。</p>
<h3>10.2.2 后向算法</h3>
<p><strong>
定义 10.3（后向概率）给定隐马尔科夫模型 <span class="math">\(\lambda\)</span>，定义在时刻 <span class="math">\(t\)</span> 状态为 <span class="math">\(q_i\)</span> 的条件下，从 <span class="math">\(t+1\)</span> 到 <span class="math">\(T\)</span> 的部分观测序列为 <span class="math">\(o_{t+1},o_{t+2},\cdots,o_T\)</span> 的概率为后向概率，记作
<div class="math">$$\beta_t(i)=P(o_{t+1},o_{t+2},\cdots,o_{T}|i_t=q_i,\lambda)$$</div>
可以用递推的方法求得后向概率以及观测序列概率 <span class="math">\(P(O|\lambda)\)</span>。
</strong></p>
<p><strong>
算法 10.3（观测序列概率的后向算法）<br>
输入：隐马尔科夫模型 <span class="math">\(\lambda\)</span>，观测序列 <span class="math">\(O\)</span><br>
输出：观测序列概率 <span class="math">\(P(O|\lambda)\)</span><br>
(1)
<div class="math">$$\beta_T(i)=1,\ i=1,2,\cdots,N$$</div>
(2) 对 <span class="math">\(t=T-1,T-2,\cdots,1\)</span>
<div class="math">$$\beta_t(i)=\sum_{j=1}^Na_{ij}b_j(o_{t+1})\beta_{i+1}(j),\ i=1,2,\cdots,N$$</div>
(3)
<div class="math">$$P(O|\lambda)=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)$$</div>
</strong></p>
<p>利用前向概率和后向概率的定义可以将观测序列概率 <span class="math">\(P(O|\lambda)\)</span> 统一写成
</p>
<div class="math">$$P(O|\lambda)=\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j),\ t=1,2,\cdots,T-1$$</div>
<p>
当 <span class="math">\(t=1\)</span> 或 <span class="math">\(t=T-1\)</span> 时，分别为后向概率和前向概率。</p>
<p>根据定义，我们可得如下等式
</p>
<div class="math">$$\alpha_t(i)\beta_t(i)=P(i_t=q_i,O|\lambda)$$</div>
<h3>10.2.3 一些概率与期望值的计算</h3>
<p><span class="math">\(\blacksquare\)</span> 给定模型 <span class="math">\(\lambda\)</span> 和观测 <span class="math">\(O\)</span>，在时刻 <span class="math">\(t\)</span> 处于状态 <span class="math">\(q_i\)</span> 概率</p>
<p>记
</p>
<div class="math">$$\gamma_t(i)=P(i_t=q_i|O,\lambda)$$</div>
<p>
则
</p>
<div class="math">$$\gamma_t(i)=P(i_t=q_i|O,\lambda)=\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}$$</div>
<p><span class="math">\(\blacksquare\)</span> 给定模型 <span class="math">\(\lambda\)</span> 和观测 <span class="math">\(O\)</span>，在时刻 <span class="math">\(t\)</span> 处于状态 <span class="math">\(q_i\)</span> 概率且在时刻 <span class="math">\(t+1\)</span> 处于状态 <span class="math">\(q_j\)</span> 的概率</p>
<p>记
</p>
<div class="math">$$\xi_t(i,j)=P(i_t=q_i,i_{t+1}=q_j|O,\lambda)$$</div>
<p>
则
</p>
<div class="math">$$\begin{eqnarray}
\xi_t(i,j) &amp;=&amp; P(i_t=q_i,i_{t+1}=q_j|O,\lambda)=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{P(O|\lambda)} \\
&amp;=&amp; \frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{\sum_{i=1}^N\sum_{j=1}^NP(i_t=q_i,i_{t+1}=q_j,O|\lambda)} \\
&amp;=&amp; \frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}
\end{eqnarray}$$</div>
<p><span class="math">\(\blacksquare\)</span> 将 <span class="math">\(\gamma_t(i)\)</span> 和 <span class="math">\(\xi_t(i,j)\)</span> 对各个时刻 <span class="math">\(t\)</span> 求和，可以得到一些有用的期望值</p>
<p>a. 在观测 <span class="math">\(O\)</span> 下状态 <span class="math">\(i\)</span> 出现的期望值
</p>
<div class="math">$$\sum_{t=1}^T\gamma_t(i)$$</div>
<p>
b. 在观测 <span class="math">\(O\)</span> 下由状态 <span class="math">\(i\)</span> 转移的期望值
</p>
<div class="math">$$\sum_{t=1}^{T-1}\gamma_t(i)$$</div>
<p>
c. 在观察 <span class="math">\(O\)</span> 下由状态 <span class="math">\(i\)</span> 转移到状态 <span class="math">\(j\)</span> 的期望值
</p>
<div class="math">$$\sum_{t=1}^{T-1}\xi_t(i,j)$$</div>
<h2>10.3 学习算法</h2>
<p>隐马尔科夫模型的学习，根据训练数据是否包括观测序列和对应的状态序列还是只有观测序列，可以分别由监督学习与非监督学习实现。</p>
<h3>10.3.1 监督学习方法</h3>
<p>假设已给训练数据包含 <span class="math">\(S\)</span> 个长度相同的观测序列和对应的状态序列
</p>
<div class="math">$$\{(O_1,I_1),(O_2,I_2),\cdots,(O_S,I_S)\}$$</div>
<p>
那么可以用极大似然估计法来估计隐马尔科夫模型的参数。</p>
<p><span class="math">\(\blacksquare\)</span> 转移概率 <span class="math">\(a_{ij}\)</span> 的估计</p>
<p>设样本中时刻 <span class="math">\(t\)</span> 处于状态 <span class="math">\(i\)</span> 时刻 <span class="math">\(t+1\)</span> 转移到状态 <span class="math">\(j\)</span> 的频数是 <span class="math">\(A_{ij}\)</span>，那么状态转移概率 <span class="math">\(a_{ij}\)</span> 的估计是
</p>
<div class="math">$$\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^NA_{ij}},\ i=1,2,\cdots,N;\ j=1,2,\cdots,N$$</div>
<p><span class="math">\(\blacksquare\)</span> 观测概率 <span class="math">\(b_j(k)\)</span> 的估计</p>
<p>设样本中状态为 <span class="math">\(j\)</span> 并观测为 <span class="math">\(k\)</span> 的频数是 <span class="math">\(B_{jk}\)</span>，那么状态为 <span class="math">\(j\)</span> 观测为 <span class="math">\(k\)</span> 的概率 <span class="math">\(b_j(k)\)</span> 的估计是
</p>
<div class="math">$$\hat{b}_j(k)=\frac{B_{jk}}{\sum_{k=1}^MB_{jk}},\ j=1,2,\cdots,N;\ k=1,2,\cdots,M$$</div>
<p><span class="math">\(\blacksquare\)</span> 初始状态概率 <span class="math">\(\pi_i\)</span> 的估计 <span class="math">\(\hat{\pi}_i\)</span> 为 <span class="math">\(S\)</span> 个样本中初始状态为 <span class="math">\(q_i\)</span> 的频率</p>
<p>由于监督学习需要使用训练数据，而人工标注训练数据往往代价很高，有时就会利用非监督学习的方法。</p>
<h3>10.3.2 Baum-Welch 算法</h3>
<p>假设给定训练数据只包含 <span class="math">\(S\)</span> 个长度为 <span class="math">\(T\)</span> 的观测序列 <span class="math">\(\{O_1,O_2,\cdots,O_S\}\)</span> 而没有对应的状态序列，目标是学习隐马尔科夫模型的参数。</p>
<p>我们将观测序列数据看做观测数据 <span class="math">\(O\)</span>，状态序列数据看做不可观测的隐数据 <span class="math">\(I\)</span>，那么隐马尔科夫模型实际上是一个含有隐变量的概率模型
</p>
<div class="math">$$P(O|\lambda)=\sum_{I}P(O|I,\lambda)P(I|\lambda)$$</div>
<p>
它的参数学习可以由 <a href="https://xutree.github.io/pages/2018/11/17/sl-9/">EM 算法</a>实现。</p>
<p><span class="math">\(\blacksquare\)</span> 确定完全数据的对数似然函数</p>
<p>所有观测数据写成 <span class="math">\(O=(o_1,o_2,\cdots,o_T)\)</span>，所有隐数据写成 <span class="math">\(I=(i_1,i_2,\cdots,i_T)\)</span>，完全数据是
</p>
<div class="math">$$(O,I)=(o_1,o_2,\cdots,o_T,i_1,i_2,\cdots,i_T)$$</div>
<p>
完全数据的对数似然函数是 <span class="math">\(\log P(O,I|\lambda)\)</span></p>
<p><span class="math">\(\blacksquare\)</span> EM 算法的 E 步，求 <span class="math">\(Q\)</span> 函数 <span class="math">\(Q(\lambda,\bar{\lambda})\)</span></p>
<div class="math">$$Q(\lambda,\bar{\lambda})=\sum_I\log P(O,I|\lambda)P(O,I|\bar{\lambda})$$</div>
<p>其中，<span class="math">\(\bar{\lambda}\)</span> 是隐马尔科夫模型的当前估计值，<span class="math">\(\lambda\)</span> 是要极大化的隐马尔科夫模型参数，<span class="math">\(Q\)</span> 函数其中略去了对 <span class="math">\(\lambda\)</span> 而言的常数因子 <span class="math">\(1/P(O|\bar{\lambda})\)</span>
</p>
<div class="math">$$P(O,I|\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdots,a_{i_{T-1}i_T}b_{i_T}(o_T)$$</div>
<p>
于是
</p>
<div class="math">$$\begin{eqnarray}
Q(\lambda,\bar{\lambda}) &amp;=&amp; \sum_I\log\pi_{i_1}P(O,I|\bar{\lambda}) \\
&amp;+&amp; \sum_I\left(\sum_{t=1}^{T-1}\log a_{i_ti_{t+1}}\right)P(O,I|\bar{\lambda}) \\
&amp;+&amp; \sum_I\left(\sum_{t=1}^T\log b_{i_t}(o_t)\right)P(O,I|\bar{\lambda})
\end{eqnarray}$$</div>
<p>
式中求和都是对所有训练数据的序列总长度 <span class="math">\(T\)</span> 进行的。</p>
<p><span class="math">\(\blacksquare\)</span> EM 算法的 M 步：极大化 <span class="math">\(Q(\lambda,\bar{\lambda})\)</span> 求模型参数 <span class="math">\(\lambda=(A,B,\pi)\)</span></p>
<p>由于要极大化的参数单独出现在 3 个项中，所以只需对各项分别极大化。</p>
<p>a. 第一项可写成
</p>
<div class="math">$$\sum_I\log\pi_{i_1}P(O,I|\bar{\lambda})=\sum_{i=1}^N\log\pi_iP(O,i_1=q_i|\bar{\lambda})$$</div>
<p>
注意约束条件 <span class="math">\(\sum_{i=1}^N\pi_i=1\)</span>，利用拉格朗日乘子法，写出拉格朗日函数
</p>
<div class="math">$$\sum_{i=1}^N\log\pi_iP(O,i_1=q_i|\bar{\lambda})+\gamma\left(\sum_{i=1}^N\pi_i-1\right)$$</div>
<p>
求偏导并令结果为 0，得
</p>
<div class="math">$$P(O,i_1=q_i|\bar{\lambda})+\gamma\pi_i=0$$</div>
<p>
对 <span class="math">\(i\)</span> 求和得到 <span class="math">\(\gamma\)</span>
</p>
<div class="math">$$\gamma=-P(O|\bar{\lambda})$$</div>
<p>
带入得
</p>
<div class="math">$$\pi_i=\frac{P(O,i_1=q_i|\bar{\lambda})}{P(O|\bar{\lambda})}$$</div>
<p>b. 第二项可以写成
</p>
<div class="math">$$\sum_I\left(\sum_{t=1}^{T-1}\log a_{i_ti_{t+1}}\right)P(O,I|\bar{\lambda}) \\
=\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}\log a_{ij}P(O,i_t=q_i,i_{t+1}=q_j|\bar{\lambda})$$</div>
<p>
注意约束条件 <span class="math">\(\sum_{j=1}^Na_{ij}=1\)</span>，利用拉格朗日乘子法求出
</p>
<div class="math">$$a_{ij}=\frac{\sum_{t=1}^{T-1}P(O,i_t=q_i,i_{t+1}=q_j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_t=q_i|\bar{\lambda})}$$</div>
<p>c. 第三项可以写成
</p>
<div class="math">$$\sum_I\left(\sum_{t=1}^T\log b_{i_t}(o_t)\right)P(O,I|\bar{\lambda}) \\
=\sum_{j=1}^N\sum_{t=1}^T\log b_j(o_t)P(O,i_t=q_j|\bar{\lambda})$$</div>
<p>
注意约束条件 <span class="math">\(\sum_{k=1}^Mb_{j}(k)=1\)</span>，注意，只有在 <span class="math">\(o_t=v_k\)</span> 时，<span class="math">\(b_j(o_t)\)</span> 对 <span class="math">\(b_j(k)\)</span> 的偏导才不是 0，利用拉格朗日乘子法求出
</p>
<div class="math">$$b_j(k)=\frac{\sum_{t=1}^TP(O,i_t=q_j|\bar{\lambda})\mathbb{I}(o_t=v_k)}{\sum_{t=1}^TP(O,i_t=q_j|\bar{\lambda})}$$</div>
<h3>10.3.3 Baum-Welch 模型参数估计公式</h3>
<p>结合 10.2 节，得到
</p>
<div class="math">$$
a_{ij}=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)} \\
b_j(k)=\frac{\sum_{t=1,o_t=v_k}^T\gamma_t(j)}{\sum_{t=1}^T\gamma_t(j)} \\
\pi_i=\gamma_1(i)
$$</div>
<p><strong>
算法 10.4（Baum-Welch 算法）<br>
输入：观测数据 <span class="math">\(O=(o_1,o_2,\cdots,o_T)\)</span><br>
输出：隐马尔科夫模型参数<br>
(1) 初始化<br>
对 <span class="math">\(n=0\)</span>，选取 <span class="math">\(a_{ij}^{(0)}\)</span>，<span class="math">\(b_j(k)^{(0)}\)</span>，<span class="math">\(\pi_i^{(0)}\)</span>，得到模型 <span class="math">\(\lambda^{(0)}=\left(A^{(0)},B^{(0)},\pi^{(0)}\right)\)</span><br>
(2) 递推。对 <span class="math">\(n=1,2,\cdots,\)</span><br>
<div class="math">$$a_{ij}^{(n+1)}=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)} \\
b_j(k)^{(n+1)}=\frac{\sum_{t=1,o_t=v_k}^T\gamma_t(j)}{\sum_{t=1}^T\gamma_t(j)} \\
\pi_i^{(n+1)}=\gamma_1(i)$$</div>
右端各式按观测 <span class="math">\(O=(o_1,o_2,\cdots,o_T)\)</span> 和模型 <span class="math">\(\lambda^{(n)}=\left(A^{(n)},B^{(n)},\pi^{(n)}\right)\)</span> 计算，式中
<div class="math">$$\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}\\
\xi_t(i,j)=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}$$</div>
(3) 终止。得到模型参数 <span class="math">\(\lambda^{(n+1)}=\left(A^{(n+1)},B^{(n+1)},\pi^{(n+1)}\right)\)</span>
</strong></p>
<h2>10.4 预测算法</h2>
<h3>10.4.1 近似算法</h3>
<p>近似算法的思想是，在每个时刻 <span class="math">\(t\)</span> 选择在该时刻最有可能出现的状态 <span class="math">\(i_t^\star\)</span>，从而得到一个状态序列 <span class="math">\(I^\star=(i_1^\star,i_2^\star,\cdots,i_T^\star)\)</span>，并将它作为预测的结果。</p>
<p>给定隐马尔科夫模型和观测序列，在时刻 <span class="math">\(t\)</span> 处于状态 <span class="math">\(q_i\)</span> 的概率 <span class="math">\(\gamma_t(i)\)</span> 是
</p>
<div class="math">$$\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}$$</div>
<p>
在每一时刻 <span class="math">\(t\)</span> 最有可能的状态 <span class="math">\(i_t^\star\)</span> 是
</p>
<div class="math">$$i_t^\star=\arg\max_{1\leq i\leq N}[\gamma_t(i)],\ i=1,2,\cdots,T$$</div>
<p>
从而得到状态序列 <span class="math">\(I^\star=(i_1^\star,i_2^\star,\cdots,i_T^\star)\)</span></p>
<p>近似算法的优点是计算简单，其缺点是不能保证预测的状态序列整体是最有可能的状态序列，因为预测的状态序列可能有实际不发生的部分。事实上，上述方法得到的状态序列中有可能存在转移概率为 0 的相邻状态。尽管如此，近似算法仍然是有用的。</p>
<h3>10.4.2 维特比算法</h3>
<p>维特比算法实际上用动态规划隐马尔可夫模型预测问题，即，用动态规划（dynamic programming）求概率最大路径(最优路径)。这时一条路径对应着一个状态序列。</p>
<p>根据动态规划原理，最优路径具有这样的特征：如果最优路径在时刻 <span class="math">\(t\)</span> 通过节点 <span class="math">\(i_t^\star\)</span>，那么这一路径中“从节点 <span class="math">\(i_1^\star\)</span> 到终点 <span class="math">\(i_T^\star\)</span> 的部分路径”对于“从节点 <span class="math">\(i_1^\star\)</span> 到终点 <span class="math">\(i_T^\star\)</span> 的所有可能的部分路径”来说必须是最优的。</p>
<p>根据这一原理，我们只需从时刻 <span class="math">\(t=1\)</span> 开始，递推的计算在时刻 <span class="math">\(t\)</span> 状态为 <span class="math">\(i\)</span> 的各条部分路径的最大概率，直至得到时刻 <span class="math">\(t=T\)</span> 时状态为 <span class="math">\(i\)</span> 的各条路径的最大概率。时刻 <span class="math">\(t=T\)</span> 的最大概率即为最优路径的概率 <span class="math">\(P^\star\)</span>，最优路径的终点 <span class="math">\(i_T^\star\)</span> 也就同时得到了。</p>
<p>之后，为了找出最优路径的各个节点，从终结点 <span class="math">\(i_T^\star\)</span> 开始，由后向前逐步求得节点 <span class="math">\(i_{T-1}^\star,\cdots,i_1^\star\)</span>，得到最优路径 <span class="math">\(I^\star=(i_1^\star,i_2^\star,\cdots,i_T^\star)\)</span>。这就是维特比算法。</p>
<p>首先定义在时刻 <span class="math">\(t\)</span> 状态为 <span class="math">\(i\)</span> 的所有单个路径 <span class="math">\((i_1,i_2,\cdots,i_t)\)</span> 中概率最大值为
</p>
<div class="math">$$\delta_t(i)=\max_{i_1,i_2,\cdots,i_{t-1}}P(i_t=i,i_{t-1},\cdots,i_1,o_t,\cdots,o_1|\lambda),\ i=1,2,\cdots,N$$</div>
<p>
得递推公式
</p>
<div class="math">$$\begin{eqnarray}
\delta_{t+1}(i) &amp;=&amp; \max_{i_1,i_2,\cdots,i_{t}}P(i_{t+1}=i,i_{t},\cdots,i_1,o_{t+1},\cdots,o_1|\lambda) \\
&amp;=&amp; \max_{1\leq j\leq N}[\delta_t(ja_{ji})]b_i(o_{t+1}),\ i=1,2,\cdots,N;\ t=1,2,\cdots,T-1
\end{eqnarray}$$</div>
<p>
定义在时刻 <span class="math">\(t\)</span> 状态为 <span class="math">\(i\)</span> 的所有单个路径 <span class="math">\((i_1,i_2,\cdots,i_{t-1},i)\)</span> 中概率最大的路径的第 <span class="math">\(t-1\)</span> 个节点为
</p>
<div class="math">$$\psi_t(i)=\arg\max_{1\leq j\leq N}[\delta_{t-1}(j)a_{ji}],\ i=1,2,\cdots,N$$</div>
<p><strong>
算法 10.5（维特比算法）<br>
输入：模型 <span class="math">\(\lambda=(A,B,\pi)\)</span> 和观测 <span class="math">\(O=(o_1,o_2,\cdots,o_T)\)</span><br>
输出：最优路径 <span class="math">\(I^\star=(i_1^\star,i_2^\star,\cdots,i_T^\star)\)</span><br>
(1) 初始化<br>
<div class="math">$$\delta_1(i)=\pi_ib_i(o_1),\ i=1,2,\cdots,N \\
\psi_1(i)=0,\ i=1,2,\cdots,N$$</div>
(2) 递推，对 <span class="math">\(t=2,3,\cdots,T\)</span>
<div class="math">$$\delta_t(i)=\max_{1\leq j\leq N}[\delta_{t-1}(j)a_{ji}]b_i(o_t),\ i=1,2,\cdots,N \\
\psi_t(i)=\arg\max_{1\leq j\leq N}[\delta_{t-1}(j)a_{ji}],\ i=1,2,\cdots,N$$</div>
(3) 终止
<div class="math">$$P^\star=\max_{1\leq i\leq N}\delta_T(i) \\
i_T^\star=\arg\max_{1\leq i\leq N}[\delta_T(i)]$$</div>
(4) 最优路径回溯，对 <span class="math">\(t=T-1,T-2,\cdots,1\)</span>
<div class="math">$$i_t^\star=\psi_{t+1}(i_{t+1}^\star)$$</div>
求得最优路径 <span class="math">\(I^\star=(i_1^\star,i_2^\star,\cdots,i_T^\star)\)</span>
</strong></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <aside>
                    <hr />
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/17/sl-9/">
                                    统计学习方法 第九章 EM 算法及其推广
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/16/likelihood-function/">
                                    似然函数
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-8/">
                                    统计学习方法 第八章 提升方法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-7_4/">
                                    统计学习方法 第七章 支持向量机（4）——序列最小最优化算法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-7_3/">
                                    统计学习方法 第七章 支持向量机（3）——非线性支持向量机
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="newer">
                        <h1>
                            <font color="#771515"><em>NEWER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-11/">
                                    统计学习方法 第十一章 条件随机场
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/20/sl-12/">
                                    统计学习方法 第十二章 统计学习方法总结
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/21/dl-1/">
                                    深度学习 第一章 引言
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/21/dl-2/">
                                    深度学习 第二章 线性代数
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/21/dl-3/">
                                    深度学习 第三章 概率与信息论
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <!-- Gitalk 评论 start  -->

                    <!-- Link Gitalk 的支持文件  -->
                    <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                    <div id="gitalk-container"></div>
                    <script type="text/javascript">
                        var dateTime = Date.now();
                        var timestamp = Math.floor(dateTime / 1000);
                        var gitalk = new Gitalk({

                            // gitalk的主要参数
                            clientID: '93f43349e9fd3154bfad',
                            clientSecret: 'd6d09d1d7261f6b62f46b39e5fcace85b81c3cd7',
                            repo: 'xutree.github.io',
                            owner: 'xutree',
                            admin: ['xutree'],
                            id: String(timestamp)

                        });
                        gitalk.render('gitalk-container');
                    </script> -->
                    <!-- Gitalk end -->
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-19T12:38:27+08:00">2018-11-19 12:38:27</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-19 17:11:25</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>18</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>15</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>