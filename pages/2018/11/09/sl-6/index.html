<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第六章 逻辑回归与最大熵模型  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
        <link href="https://xutree.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - Full Atom Feed" />
        <link href="https://xutree.github.io/feeds/读书笔记.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 读书笔记 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/基础知识.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 基础知识 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/教程.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 教程 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/其他.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 其他 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/趣闻.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 趣闻 Category Atom Feed" />
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/09/sl-6/"> 统计学习方法 第六章 逻辑回归与最大熵模型  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
            <div class="span8 offset2 article-content">

                <p>逻辑回归（logistic regression）是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model）。</p>
<p>逻辑回归模型和最大熵模型都属于对数线性模型。</p>
<h2>6.1 逻辑回归模型</h2>
<h3>6.1.1 逻辑分布</h3>
<p><strong>
定义 6.1（逻辑分布）设 <span class="math">\(X\)</span> 是连续随机变量，<span class="math">\(X\)</span> 服从逻辑分布是指 <span class="math">\(X\)</span> 具有下列分布函数和密度函数：
<div class="math">$$\begin{eqnarray}
F(x) &amp;=&amp; P(X\leq x)=\frac{1}{1+\text{e}^{-(x-\mu)/\gamma}} \\
f(x) &amp;=&amp; F'(x)=\frac{\text{e}^{-(x-\mu)/\gamma}}{\gamma(1+\text{e}^{-(x-\mu)/\gamma})^2}
\end{eqnarray}$$</div>
式中，<span class="math">\(\mu\)</span> 为位置参数，<span class="math">\(\gamma&gt;0\)</span> 为形状参数。
</strong></p>
<p>下图中绘制了对于不用 <span class="math">\(\gamma\)</span> 逻辑分布函数和概率密度函数。分布函数是一条 S 形曲线（sigmoid curve）。该曲线以点 <span class="math">\(\left(\mu,\frac{1}{2}\right)\)</span> 为中心对称，即满足
</p>
<div class="math">$$F(-x+\mu)+F(x+\mu)=1$$</div>
<p><img alt="逻辑分布" src="https://xutree.github.io/images/statistical_learning_6.1.png"></p>
<p>曲线在中心附近增长速度较快，在两端增长速度较慢。</p>
<h3>6.1.2 二项逻辑回归模型</h3>
<p>二项逻辑回归模型（binomial logistic regression model）是一种分类模型，由条件概率 <span class="math">\(P(Y|X)\)</span> 表示，形式为参数化的逻辑分布。这里，随机变量 <span class="math">\(X\)</span> 取值为实数，随机变量 <span class="math">\(Y\)</span> 取值为 1 或 0，我们通过监督学习的方法来估计模型参数。</p>
<p><strong>
定义 6.2 （逻辑回归模型）二项逻辑回归模型是如下的条件概率分布：
<div class="math">$$\begin{eqnarray}
P(Y=1|x) &amp;=&amp; \frac{\exp(w\cdot x+b)}{1+\exp(w\cdot x+b)} \\
P(Y=0|x) &amp;=&amp; \frac{1}{1+\exp(w\cdot x+b)}
\end{eqnarray}$$</div>
这里，<span class="math">\(x\in\mathbb{R}^n\)</span> 是输入，<span class="math">\(Y\in\{0,1\}\)</span> 是输出，<span class="math">\(w\in\mathbb{R}^n\)</span> 和 <span class="math">\(b\in\mathbb{R}\)</span> 是参数（分别称为权重向量和偏置），<span class="math">\(w\cdot x\)</span> 是内积。
</strong></p>
<p>对于给定的输入实例 <span class="math">\(x\)</span>，按照定义 6.2 可以求得 <span class="math">\(P(Y=1|x)\)</span> 和 <span class="math">\(P(Y=0|x)\)</span>。逻辑回归比较两个概率值的大小，将实例 <span class="math">\(x\)</span> 分到概率值较大的那一类。</p>
<p>有时为了方便，将权重向量和输入向量加以扩充，即
</p>
<div class="math">$$w=\left(w^{(1)},w^{(2)},\cdots,w^{(n)},1\right)^\text{T}\\
x=\left(x^{(1)},x^{(2)},\cdots,x^{(n)},1\right)^\text{T}$$</div>
<p>
这时，逻辑回归模型如下：
</p>
<div class="math">$$\begin{eqnarray}
P(Y=1|x) &amp;=&amp; \frac{\exp(w\cdot x)}{1+\exp(w\cdot x)} \\
P(Y=0|x) &amp;=&amp; \frac{1}{1+\exp(w\cdot x)}
\end{eqnarray}$$</div>
<p>一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值，该事件的对数几率（log odds）或 logit 函数是
</p>
<div class="math">$$\text{logit}(p)=\log\frac{p}{1-p}$$</div>
<p>对逻辑回归而言，由扩充后的回归模型得
</p>
<div class="math">$$\text{logit}\left(P(Y=1|x)\right)=\log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\cdot x$$</div>
<p>
也就是说，输出 <span class="math">\(Y=1\)</span> 的对数几率是输入 <span class="math">\(x\)</span> 的线性函数，或者说，输出 <span class="math">\(Y=1\)</span> 的对数几率是由输入 <span class="math">\(x\)</span> 的线性函数表示的模型，即逻辑回归模型。</p>
<p>换一个角度看，考虑对输入 <span class="math">\(x\)</span> 进行分类的线性函数 <span class="math">\(w\cdot x\)</span>（扩充后的），其值域为实数域。通过逻辑回归模型定义式可以将线性函数 <span class="math">\(w\cdot x\)</span> 转换为概率
</p>
<div class="math">$$P(Y=1|x)=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}$$</div>
<p>
上面的分布实际上是逻辑分布。这是，线性函数的值越接近 <span class="math">\(+\infty\)</span>，概率值就越接近 1；线性函数的值越接近 <span class="math">\(-\infty\)</span>，概率在就越接近 0。这样的模型就是逻辑回归模型。</p>
<h3>6.1.3 模型参数估计</h3>
<p>逻辑回归模型学习时，对于给定的训练数据集
</p>
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
其中 <span class="math">\(x_i\in\mathbb{R}^n\)</span>，<span class="math">\(y_i\in\{0,1\}\)</span>，可以应用极大似然估计法估计模型参数，从而得到逻辑回归模型。</p>
<p>设
</p>
<div class="math">$$P(Y=1|x) = \pi(x) \\
P(Y=0|x) = 1-\pi(x)$$</div>
<p>
似然函数为
</p>
<div class="math">$$\prod_{i=1}^N\left[\pi(x_i)\right]^{y_i}\left[1-\pi(x_i)\right]^{1-y_i}$$</div>
<p>
对数似然函数为
</p>
<div class="math">$$\begin{eqnarray}
L(w) &amp;=&amp; \sum_{i=1}^N\left[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))\right] \\
&amp;=&amp; \sum_{i=1}^N\left[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))\right] \\
&amp;=&amp; \sum_{i=1}^N[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i))]
\end{eqnarray}$$</div>
<p>
对 <span class="math">\(L(w)\)</span> 求极大值，得到 <span class="math">\(w\)</span> 的估计值。</p>
<p>这样，问题就变成以对数似然函数为目标函数的最优化问题。逻辑回归学习中通常采用的方法是梯度下降法和拟牛顿法。</p>
<p>假设 <span class="math">\(w\)</span> 的极大似然估计值是 <span class="math">\(\hat{w}\)</span>，那么学到的逻辑回归模型为
</p>
<div class="math">$$P(Y=1|x)=\frac{\exp(\hat{w}\cdot x)}{1+\exp(\hat{w}\cdot x)} \\
P(Y=0|x)=\frac{1}{1+\exp(\hat{w}\cdot x)}$$</div>
<h3>6.1.4 多项逻辑回归</h3>
<p>可以将二项回归模型推广到多项逻辑回归模型（multi-nominal logistic regression model），用于多类分类。</p>
<p>假设离散型随机变量 <span class="math">\(Y\)</span> 的取值集合是 <span class="math">\(\{1,2,\cdots,K\}\)</span>，那么多项逻辑回归模型是
</p>
<div class="math">$$P(Y=k|x)=\frac{\exp(w_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)},k=1,2,\cdots,K-1 \\
P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}$$</div>
<p>
这里，<span class="math">\(x\in\mathbb{R}^{n+1}\)</span>，<span class="math">\(w_k\in\mathbb{R}^{n+1}\)</span>
二项逻辑回归的参数估计法也可以推广到多项逻辑回归。</p>
<h2>6.2 最大熵模型</h2>
<p>最大熵模型（maximum entropy model）由最大熵原理推导实现。</p>
<h3>6.2.1 最大熵原理</h3>
<p>最大熵原理是概率模型学习的一个准则。最大熵原理认为：学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型时最好的模型。</p>
<p>通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</p>
<p>假设离散随机变量 <span class="math">\(X\)</span> 的概率分布是 <span class="math">\(P(X)\)</span>，则其熵是
</p>
<div class="math">$$H(P)=-\sum_xP(x)\log P(x)$$</div>
<p>
熵满足下列不等式
</p>
<div class="math">$$0\leq H(P)\leq\log |X|$$</div>
<p>
式中，<span class="math">\(|X|\)</span> 是 <span class="math">\(X\)</span> 取值个数，当且仅当 <span class="math">\(X\)</span> 的分布是均匀分布时右边的等号成立。也就是说，当 <span class="math">\(X\)</span> 服从均匀分布时，熵最大。</p>
<h3>6.2.2 最大熵模型的定义</h3>
<p>最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型。</p>
<p>假设分类模型是一个条件概率分布 <span class="math">\(P(Y|X)\)</span>，给定一个训练数据集
</p>
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
学习的目标是用最大熵原理选择最好的分类模型。</p>
<p>首先考虑模型应该满足的条件。给定训练数据集，可以确定联合概率分布 <span class="math">\(P(X,Y)\)</span> 和边缘分布 <span class="math">\(P(X)\)</span> 的经验分布，分别用 <span class="math">\(\tilde{P}(X,Y)\)</span> 和 <span class="math">\(\tilde{P}(X)\)</span> 表示
</p>
<div class="math">$$\tilde{P}(X=x,Y=y)=\frac{v(X=x,Y=y)}{N} \\
\tilde{P}(X=x)=\frac{v(X=x)}{N}$$</div>
<p>
其中，<span class="math">\(v(\cdot)\)</span> 表示频数，<span class="math">\(N\)</span> 是训练样本容量。</p>
<p>用特征函数（feature function）<span class="math">\(f(x,y)\)</span> 描述输入 <span class="math">\(x\)</span> 和输出 <span class="math">\(y\)</span> 之间的某一个事实。其定义是
</p>
<div class="math">$$f(x,y)=\begin{cases}
1, &amp; x\ 与\ y\ 满足某一事实 \\
0, &amp; 否则
\end{cases}$$</div>
<p>
它是一个二值函数（一般的，特征函数可以是任意的实值函数）。</p>
<p>特征函数 <span class="math">\(f(x,y)\)</span> 关于经验分布 <span class="math">\(\tilde{P}(X,Y)\)</span> 的期望值，用 <span class="math">\(\text{E}_{\tilde{P}}(f)\)</span> 表示
</p>
<div class="math">$$\text{E}_{\tilde{P}}(f)=\sum_{x,y}\tilde{P}(x,y)f(x,y)$$</div>
<p>特征函数 <span class="math">\(f(x,y)\)</span> 关于模型 <span class="math">\(P(Y|X)\)</span> 与经验分布 <span class="math">\(\tilde{P}(X)\)</span> 的期望值，用 <span class="math">\(\text{E}_P(f)\)</span> 表示
</p>
<div class="math">$$\text{E}_P(f)=\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$</div>
<p>如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即
</p>
<div class="math">$$\text{E}_P(f)=\text{E}_{\tilde{P}}(f)$$</div>
<p>
即
</p>
<div class="math">$$\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)=\sum_{x,y}\tilde{P}(x,y)f(x,y)$$</div>
<p>
我们将上式作为模型学习的约束条件。假如有 <span class="math">\(n\)</span> 个特征函数 <span class="math">\(f_i(x,y)\)</span>，<span class="math">\(i=1,2,\cdots,n\)</span>，那么就有 <span class="math">\(n\)</span> 个约束条件。</p>
<p><strong>
定义 6.3（最大熵模型）假设满足所有约束条件的模型集合为
<div class="math">$${\cal C}\equiv\left\{P\in{\cal P}|\text{E}_P(f_i)=\text{E}_{\tilde{P}}(f_i),i=1,2,\cdots,n\right\}$$</div>
定义在条件概率分布 <span class="math">\(P(Y|X)\)</span> 上的条件熵为
<div class="math">$$H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)$$</div>
则模型集合 <span class="math">\({\cal C}\)</span> 中条件熵 <span class="math">\(H(P)\)</span> 最大的模型称为最大熵模型。式中的对数为自然对数。
</strong></p>
<h3>6.2.3 最大熵模型的学习</h3>
<p>最大熵模型的学习过程就是求解最大熵模型的过程。最大熵模型的学习可以形式化为约束最优化问题。</p>
<p>对于给定的训练数据集
</p>
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
以及特征函数
</p>
<div class="math">$$f_i(x,y),\ i=1,2,\cdots,n$$</div>
<p>
最大熵模型的学习等价于约束优化问题：
</p>
<div class="math">$$\max_{P\in{\cal C}}\ \ \ \ \ H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x) \\
\text{s.t.}\ \ \ \ \ \text{E}_P(f_i)=\text{E}_{\tilde{P}}(f_i),\ i=1,2,\cdots,n \\
\sum_{y}P(y|x)=1$$</div>
<p>按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题：
</p>
<div class="math">$$\min_{P\in{\cal C}}\ \ \ \ \ -H(P)=\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x) \\
\text{s.t.}\ \ \ \ \ \text{E}_P(f_i)-\text{E}_{\tilde{P}}(f_i)=0,\ i=1,2,\cdots,n \\
\sum_{y}P(y|x)=1$$</div>
<p>这里，将约束最优化问题的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题。</p>
<p>首先，引进拉格朗日乘子 <span class="math">\(w_0,w_1,w_2,\cdots,w_n\)</span>，定义拉格朗日函数
</p>
<div class="math">$$\begin{eqnarray}
L(P,w) &amp;\equiv&amp; -H(P)+w_0\left(1-\sum_yP(y|x)\right)+\sum_{i=1}^nw_i\left(\text{E}_P(f_i)-\text{E}_{\tilde{P}}(f_i)\right) \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)+w_0\left(1-\sum_yP(y|x)\right) \\
&amp;+&amp; \sum_{i=1}^nw_i\left(\sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_{x,y}\tilde{P}(x)P(y|x)f_i(x,y)\right)
\end{eqnarray}$$</div>
<p>
最优化的原始问题是
</p>
<div class="math">$$\min_{P\in{\cal C}}\max_wL(P,w)$$</div>
<p>
对偶问题是
</p>
<div class="math">$$\max_w\min_{P\in{\cal C}}L(P,w)$$</div>
<p>
由于拉格朗日函数 <span class="math">\(L(P,w)\)</span> 是 <span class="math">\(P\)</span> 的凸函数，原始问题的解和对偶问题的解是等价的。</p>
<p>首先，求解对偶问题内部的极小化问题，记作
</p>
<div class="math">$$\Psi(w)=\min_{P\in{\cal C}}L(P,w)=L(P_w,w)$$</div>
<p>
<span class="math">\(\Psi(w)\)</span> 称为对偶函数，同时，将其解记作
</p>
<div class="math">$$P_w=\arg\min_{P\in{\cal C}}L(P,w)=P_w(y|x)$$</div>
<p>
通过令偏导数等于 0
</p>
<div class="math">$$\begin{eqnarray}
\frac{\partial L(P,w)}{\partial P(y|x)} &amp;=&amp; \sum_{x,y}\tilde{P}(x)\left(\log P(y|x)+1\right)-\sum_yw_0-\sum_{x,y}\left(\tilde{P}(x)\sum_{i=1}^nw_if_i(x,y)\right) \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x)\left(\log P(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y)\right)
\end{eqnarray}$$</div>
<p>
在 <span class="math">\(\tilde{P}(x)&gt;0\)</span> 的情况下，解得
</p>
<div class="math">$$\begin{eqnarray}
P(y|x) &amp;=&amp; \exp\left(\sum_{i=1}^nw_if_i(x,y)+w_0-1\right) \\
&amp;=&amp; \frac{\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)}{\exp(1-w_0)}
\end{eqnarray}$$</div>
<p>
由于 <span class="math">\(\sum_yP(y|x)=1\)</span>，得
</p>
<div class="math">$$P_w(y|x)=\frac{1}{Z_w(x)}\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)$$</div>
<p>
其中
</p>
<div class="math">$$Z_w(x)=\sum_y\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)$$</div>
<p>
称为规范化因子；<span class="math">\(f_i(x,y)\)</span> 是特征函数，<span class="math">\(w_i\)</span> 是特征函数的权值。由以上两式表示的 <span class="math">\(P_w(y|x)\)</span> 就是最大熵模型，<span class="math">\(w\)</span> 是最大熵模型中的参数向量。</p>
<p>之后，求解对偶问题外部的极大化问题
</p>
<div class="math">$$\max_w\Psi(w)$$</div>
<p>
将其解记为 <span class="math">\(w^\star\)</span>，即
</p>
<div class="math">$$w^\star=\arg\max_w\Psi(w)$$</div>
<p>
故最终的最大熵模型是
</p>
<div class="math">$$P^\star=P_{w^\star}=P_{w^\star}(y|x)$$</div>
<h3>6.2.4 极大似然估计</h3>
<p>从最大熵模型学习中可以看出，最大熵模型是由下面两个式子表示的条件概率分布：
</p>
<div class="math">$$P_w(y|x)=\frac{1}{Z_w(x)}\exp\left(\sum_{i=1}^nw_if_i(x,y)\right) \\
Z_w(x)=\sum_y\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)$$</div>
<p>
下面证明，对偶函数的极大化等价于最大熵模型的极大似然估计。</p>
<p>最大似然函数的一般形式是样本集中各个样本的联合概率：
</p>
<div class="math">$$L(x_1,x_2,\cdots,x_N;\theta)=\prod_{i=1}^{N} p(x_i;\theta)$$</div>
<p>
其中利用了数据是独立同分布产生的假设。<span class="math">\(x_1,x_2,\cdots,x_N\)</span> 是样本具体观测值。随机变量 <span class="math">\(X\)</span> 是离散的，所以它的取值范围是一个集合，假设样本集的大小为 <span class="math">\(N\)</span>，<span class="math">\(X\)</span> 的取值有 <span class="math">\(K\)</span> 个，分别是 <span class="math">\(c_1,c_2,\cdots,c_K\)</span>。用 <span class="math">\(v(X=c_i)\)</span>表示在观测值中样本 <span class="math">\(c_i\)</span> 出现的频数。则：
</p>
<div class="math">$$L(x_1,x_2,\cdots,x_N;\theta)=\prod_{i=1}^{k}p(v_i;\theta)^{v(X=c_i)}$$</div>
<p>
对等式两边同时开 <span class="math">\(N\)</span> 次方，可得：
</p>
<div class="math">$$L(x_1,x_2,\cdots,x_N;\theta)^{\frac{1}{N}}=\prod_{i=1}^{k}p(v_i;\theta)^{\frac{v(X=c_i)}{N}}$$</div>
<p>
因为经验概率 <span class="math">\(\tilde{p}(x)=\frac{v(X=c_i)}{N}\)</span>，所以：
</p>
<div class="math">$$L(x_1,x_2,\cdots,x_N;\theta)^{\frac{1}{N}}=\prod_{x}p(x;\theta)^{\tilde{p}(x)}$$</div>
<p>
很明显对 <span class="math">\(L(x_1,x_2,\cdots,x_N;\theta)\)</span> 求最大值和对 <span class="math">\(L(x_1,x_2,\cdots,x_N;\theta)^{\frac{1}{N}}\)</span>求最大值的优化的结果是一样的。整理上式所以最终的最大似然函数可以表示为：
</p>
<div class="math">$$L(x;\theta)=\prod_{x} p(x;\theta)^{\tilde{p}(x)}$$</div>
<p>已知训练数据的经验概率分布 <span class="math">\(\tilde{P}(X,Y)\)</span>，条件概率分布 <span class="math">\(P(Y|X)\)</span> 的对数似然函数表示为：
</p>
<div class="math">$$L_{\tilde{P}}(P_w)=\log\prod_{x,y}P(y|x)^{\tilde{P}(x,y)}=\sum_{x,y}\tilde{P}(x,y)\log P(y|x)$$</div>
<p>
当条件概率分布 <span class="math">\(P(y|x)\)</span> 是最大熵模型时，对数似然函数为
</p>
<div class="math">$$\begin{eqnarray}
L_{\tilde{P}}(P_w) &amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\log P(y|x) \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\tilde{P}(x,y)\log Z_w(x) \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)\log Z_w(x)
\end{eqnarray}$$</div>
<p>
再看对偶函数 <span class="math">\(\Psi(w)\)</span>:
</p>
<div class="math">$$\begin{eqnarray}
\Psi(w) &amp;=&amp; \sum_{x,y}\tilde{P}(x)P_w(y|x)\log P_w(y|x)+w_0\left(1-\sum_yP_w(y|x)\right) \\
&amp;+&amp; \sum_{i=1}^nw_i\left(\sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y)\right) \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y) \\
&amp;+&amp; \sum_{x,y}\tilde{P}(x)P_w(y|x)\left(\log P_w(y|x)-\sum_{i=1}^nw_if_i(x,y)\right) \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x,y}\tilde{P}(x)P_w(y|x)\log Z_w(x) \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_{x}\tilde{P}(x)\log Z_w(x)
\end{eqnarray}$$</div>
<p>
其中利用了 <span class="math">\(\sum_yP(y|x)=1\)</span>。</p>
<p>比较易得 <span class="math">\(\Psi(w)=L_{\tilde{P}}(P_w)\)</span>，于是证明了最大熵学习中的对偶函数极大化等价于最大熵模型的极大似然估计这一事实。这样，最大熵模型的学习问题就转换为具体求解对数似然函数极大化或对偶函数极大化的问题。</p>
<p>可以将最大熵模型写成更一般的形式：
</p>
<div class="math">$$P_w(y|x)=\frac{1}{Z_w(x)}\exp\left(\sum_{i=1}^nw_if_i(x,y)\right) \\
Z_w(x)=\sum_y\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)$$</div>
<p>
这里，<span class="math">\(x\in\mathbb{R}^n\)</span> 为输入，<span class="math">\(y\in\{1,2,\cdots,K\}\)</span> 为输出，<span class="math">\(w\in\mathbb{R}^n\)</span> 为权值向量，<span class="math">\(f_i(x,y)\)</span>，<span class="math">\(i=1,2,\cdots,n\)</span> 为任意实值特征函数。</p>
<p>最大熵模型与逻辑回归模型有着类似的形式，它们又称为对数线性模型（log linear model）。模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计。</p>
<h2>6.3 模型学习的最优化算法</h2>
<h3>6.3.1 改进的迭代尺度法</h3>
<p>改进的迭代尺度法（improved iterative scaling，IIS）是一种最大熵模型学习的最优化算法。</p>
<p>已知最大熵模型：
</p>
<div class="math">$$P_w(y|x)=\frac{1}{Z_w(x)}\exp\left(\sum_{i=1}^nw_if_i(x,y)\right) \\
Z_w(x)=\sum_y\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)$$</div>
<p>对数似然函数为：
</p>
<div class="math">$$L(w)=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)\log Z_w(x)
$$</div>
<p>
目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值 <span class="math">\(w^\star\)</span>。</p>
<p>首先建立对数似然函数改变量的下界
</p>
<div class="math">$$\begin{eqnarray}
L(w+\delta)-L(w) &amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\log P_{w+\delta}(y|x)-\sum_{x,y}\tilde{P}(x,y)\log P_{w}(y|x) \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)-\sum_x\tilde{P}(x)\log \frac{Z_{w+\delta}(x)}{Z_w(x)}
\end{eqnarray}$$</div>
<p>
利用不等式
</p>
<div class="math">$$-\log\alpha\geq1-\alpha,\alpha&gt;0$$</div>
<p>
得
</p>
<div class="math">$$\begin{eqnarray}
L(w+\delta)-L(w) &amp;\geq&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\frac{Z_{w+\delta}(x)}{Z_w(x)} \\
&amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1 \\
&amp;-&amp; \sum_x\tilde{P}(x)\sum_yP_w(y|x)\exp\left(\sum_{i=1}^n\delta_if_i(x,y)\right)
\end{eqnarray}$$</div>
<p>
将右端记为
</p>
<div class="math">$$A(\delta|w)=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)\exp\left(\sum_{i=1}^n\delta_if_i(x,y)\right)$$</div>
<p>
于是有
</p>
<div class="math">$$L(w+\delta)-L(w)\geq A(\delta|w)$$</div>
<p>如果能找到适当的 <span class="math">\(\delta\)</span> 使下界 <span class="math">\(A(\delta|w)\)</span> 提高，那么对数似然函数也会提高，然而 <span class="math">\(A(\delta|w)\)</span> 中的 <span class="math">\(\delta\)</span> 是个向量，不易同时优化，IIS 试图一次只优化其中一个变量 <span class="math">\(\delta_i\)</span>，而固定其他变量 <span class="math">\(\delta_j\)</span>，<span class="math">\(i\neq j\)</span>。</p>
<p>为达到这一目的，IIS 进一步降低下界 <span class="math">\(A(\delta|w)\)</span>。引入一个量
</p>
<div class="math">$$f^\sharp(x,y)=\sum_if_i(x,y)$$</div>
<p>
因为 <span class="math">\(f_i\)</span> 是二值函数，故 <span class="math">\(f^\sharp(x,y)\)</span> 表示所有特征在 <span class="math">\((x,y)\)</span> 出现的次数，这样 <span class="math">\(A(\delta|w)\)</span> 可改写为
</p>
<div class="math">$$\begin{eqnarray}
A(\delta|w) &amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1 \\
&amp;-&amp; \sum_x\tilde{P}(x)\sum_yP_w(y|x)\exp\left(f^\sharp(x,y)\sum_{i=1}^n\frac{\delta_if_i(x,y)}{f^\sharp(x,y)}\right)
\end{eqnarray}$$</div>
<p>
利用指数函数的凸性以及对任意 <span class="math">\(i\)</span>，有
</p>
<div class="math">$$\frac{f_i(x,y)}{f^\sharp(x,y)}\geq0\ 且\ \sum_{i=1}^n\frac{f_i(x,y)}{f^\sharp(x,y)}=1$$</div>
<p>
根据琴生不等式（Jensen's inequality），得到
</p>
<div class="math">$$\exp\left(\sum_{i=1}^n\frac{f_i(x,y)}{f^\sharp(x,y)}\delta_if^\sharp(x,y)\right)\leq\sum_{i=1}^n\frac{f_i(x,y)}{f^\sharp(x,y)}\exp\left(\delta_if^\sharp(x,y)\right)$$</div>
<p>
于是得到对数似然函数改变量的一个新的（相对不紧的）下界
</p>
<div class="math">$$\begin{eqnarray}
B(\delta|w) &amp;=&amp; \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1 \\
&amp;-&amp; \sum_x\tilde{P}(x)\sum_yP_w(y|x)\sum_{i=1}^n\left(\frac{f_i(x,y)}{f^\sharp(x,y)}\right)\exp\left(\delta_if^\sharp(x,y)\right)
\end{eqnarray}$$</div>
<p>
求 <span class="math">\(B(\delta|w)\)</span> 对 <span class="math">\(\delta_i\)</span> 的偏导数
</p>
<div class="math">$$\begin{eqnarray}
\frac{\partial B(\delta|w)}{\partial \delta_i} &amp;=&amp; \sum_{x,y}\tilde{P}(x,y)f_i(x,y) \\ &amp;-&amp; \sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)\exp\left(\delta_if^\sharp(x,y)\right)
\end{eqnarray}$$</div>
<p>
上式只含有 <span class="math">\(\delta_i\)</span>，令偏导数等于 0 得到
</p>
<div class="math">$$\sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)\exp\left(\delta_if^\sharp(x,y)\right)=\text{E}_{\tilde{P}}(f_i)$$</div>
<p>
于是，依次对 <span class="math">\(\delta_i\)</span> 求解上述方程就可以求出 <span class="math">\(\delta\)</span>。</p>
<p><strong>
算法 6.1（改进的迭代尺度算法 IIS）<br>
输入：特征函数 <span class="math">\(f_1,f_2,\cdots,f_n\)</span>；经验分布函数 <span class="math">\(\tilde{P}(X,Y)\)</span>，模型 <span class="math">\(P_w(y|x)\)</span><br>
输出：最优参数值 <span class="math">\(w_i^\star\)</span>；最优模型 <span class="math">\(P_{w^\star}\)</span><br>
(1) 对所有 <span class="math">\(i\in\{1,2,\cdots,n\}\)</span>，取初值 <span class="math">\(w_i=0\)</span><br>
(2) 对每一 <span class="math">\(i\in\{1,2,\cdots,n\}\)</span>：<br>
(2.a) 令 <span class="math">\(\delta_i\)</span> 是方程
<div class="math">$$\sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)\exp\left(\delta_if^\sharp(x,y)\right)=\text{E}_{\tilde{P}}(f_i)$$</div>
的解，这里
<div class="math">$$f^\sharp(x,y)=\sum_{i=1}^nf_i(x,y)$$</div><br>
(2.b) 更新：<span class="math">\(w_i\leftarrow w_i+\delta_i\)</span><br>
(3) 如果不是所有 <span class="math">\(w_i\)</span> 都收敛，重复步骤 (2)
</strong></p>
<p>这一算法关键步骤是 (2.a)，如果 <span class="math">\(f^\sharp(x,y)\)</span> 是常数，即对任意的 <span class="math">\(x\)</span>，<span class="math">\(y\)</span>，有 <span class="math">\(f^\sharp(x,y)=M\)</span>，那么 <span class="math">\(\delta_i\)</span> 可以显示的表示为
</p>
<div class="math">$$\delta_i=\frac{1}{M}\log\frac{\text{E}_\tilde{P}(f_i)}{\text{E}_P(f_i)}$$</div>
<p>
如果 <span class="math">\(f^\sharp(x,y)\)</span> 不是常数，就必须数值计算求 <span class="math">\(\delta_i\)</span>，简单有效的方法是牛顿法，由于方程有单根，因此牛顿法恒收敛，且收敛速度很快。</p>
<h3>6.3.2 拟牛顿法</h3>
<p>最大熵模型学习还可以应用牛顿法或拟牛顿法。</p>
<p>对于最大熵模型
</p>
<div class="math">$$P_w(y|x)=\frac{\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)}{\sum_y\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)}$$</div>
<p>目标函数为：
</p>
<div class="math">$$\min_{w\in\mathbb{R}^n}\ \ f(w)=\sum_x\tilde{P}(x)\log\sum_y\exp\left(\sum_{i=1}^nw_if_i(x,y)\right)-\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)
$$</div>
<p>
梯度：
</p>
<div class="math">$$g(w)=\left(\frac{\partial f(w)}{\partial w_1},\frac{\partial f(w)}{\partial w_2},\cdots,\frac{\partial f(w)}{\partial w_n}\right)^\text{T}$$</div>
<p>
其中
</p>
<div class="math">$$\frac{\partial f(w)}{\partial w_i}=\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y)-\text{E}_{\tilde{P}}(f_i),i=1,2,\cdots,n$$</div>
<p><strong>
算法 6.2（最大熵模型学习的 BFGS）算法<br>
输入：特征函数 <span class="math">\(f_1,f_2,\cdots,f_n\)</span>；经验分布 <span class="math">\(\tilde{P}(x,y)\)</span>，目标函数 <span class="math">\(f(w)\)</span>，梯度 <span class="math">\(g(w)=\nabla f(w)\)</span>，精度要求 <span class="math">\(\epsilon\)</span><br>
输出：最优化参数值 <span class="math">\(w^\star\)</span>；最优模型 <span class="math">\(P_{w^\star}(y|x)\)</span><br>
(1) 选定初始点 <span class="math">\(w^{(0)}\)</span>，取 <span class="math">\({\bf B_0}\)</span> 为正定对称矩阵，置 <span class="math">\(k=0\)</span><br>
(2) 计算 <span class="math">\(g_k=g(w^{(k)})\)</span>，若 <span class="math">\(\|g_k\|&lt;\epsilon\)</span>，则停止计算，得 <span class="math">\(w^\star=w^{(k)}\)</span>；否则，转到 (3)<br>
(3) 由 <span class="math">\({\bf B_k}p_k=-g_k\)</span> 求出 <span class="math">\(p_k\)</span><br>
(4) 一维搜索：求 <span class="math">\(\lambda_k\)</span> 使得
<div class="math">$$f(w^{(k)}+\lambda_kp_k)=\min_{\lambda\geq0}f(w^{(k)}+\lambda p_k)$$</div>
(5) 置 <span class="math">\(w^{(k+1)}=w^{(k)}+\lambda_k p_k\)</span><br>
(6) 计算 <span class="math">\(g_{k+1}=g(w^{(k+1)})\)</span>，若 <span class="math">\(\|g_{k+1}\|&lt;\epsilon\)</span>，则停止计算，得 <span class="math">\(w^\star=w^{(k+1)}\)</span>；否则，按照下式求出 <span class="math">\({\bf B_{k+1}}\)</span>
<div class="math">$${\bf B_{k+1}}={\bf B_k}+\frac{y_ky_k^\text{T}}{y_k^\text{T}\delta_k}-\frac{{\bf B_k}\delta_k\delta_k^\text{T}{\bf B_k}}{\delta_k^\text{T}{\bf B_k}\delta_k}$$</div>
其中，
<div class="math">$$y_k=g_{k+1}-g_k,\delta_k=w^{(k+1)}-w^{(k)}$$</div>
(7) 置 <span class="math">\(k=k+1\)</span>，转到 (3)
</strong></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <aside>
                    <hr />
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/07/sl-5/">
                                    统计学习方法 第五章 决策树
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/07/sl-4/">
                                    统计学习方法 第四章 朴素贝叶斯法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/05/bayes/">
                                    贝叶斯定理
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/sl-3/">
                                    统计学习方法 第三章 k 近邻法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/sl-2/">
                                    统计学习方法 第二章 感知机
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="newer">
                        <h1>
                            <font color="#771515"><em>NEWER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/10/lagrange_duality/">
                                    拉格朗日对偶性
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/10/directional_derivative-gradient/">
                                    方向导数和梯度
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/11/sl-7_1/">
                                    统计学习方法 第七章 支持向量机（1）——线性可分支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/11/sl-7_2/">
                                    统计学习方法 第七章 支持向量机（2）——线性支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-7_3/">
                                    统计学习方法 第七章 支持向量机（3）——非线性支持向量机
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <!-- Gitalk 评论 start  -->

                    <!-- Link Gitalk 的支持文件  -->
                    <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                    <div id="gitalk-container"></div>
                    <script type="text/javascript">
                        var dateTime = Date.now();
                        var timestamp = Math.floor(dateTime / 1000);
                        var gitalk = new Gitalk({

                            // gitalk的主要参数
                            clientID: '93f43349e9fd3154bfad',
                            clientSecret: 'd6d09d1d7261f6b62f46b39e5fcace85b81c3cd7',
                            repo: 'xutree.github.io',
                            owner: 'xutree',
                            admin: ['xutree'],
                            id: String(timestamp)

                        });
                        gitalk.render('gitalk-container');
                    </script> -->
                    <!-- Gitalk end -->
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-09T16:17:17+08:00">2018-11-09 16:17:17</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-11 13:21:32</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>18</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>15</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>