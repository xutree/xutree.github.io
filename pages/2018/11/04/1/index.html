<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第一章 统计学习方法概论  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
        <link href="https://xutree.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - Full Atom Feed" />
        <link href="https://xutree.github.io/feeds/读书笔记.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 读书笔记 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/基础知识.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 基础知识 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/教程.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 教程 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/其他.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 其他 Category Atom Feed" />
        <link href="https://xutree.github.io/feeds/趣闻.atom.xml" type="application/atom+xml" rel="alternate" title="You Know Nothing - 趣闻 Category Atom Feed" />
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/04/1/"> 统计学习方法 第一章 统计学习方法概论  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
            <div class="span8 offset2 article-content">

                <h2>1. 统计学习</h2>
<h3>1.1 特点</h3>
<p>统计学习（statistical learning）是关于计算机基于<strong>数据</strong>构建概率统计模型并运用模型对数据进行<strong>预测与分析</strong>的一门学科。</p>
<p>赫尔伯特·西蒙（Herbert A.Simon）对“学习”给出如下定义：如果一个系统能够通过执行某个过程改变它的性能，这就是学习。</p>
<h3>1.2 对象</h3>
<p>统计学习的对象是数据。</p>
<p>统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。</p>
<h3>1.3 目的</h3>
<p>统计学习用于对数据进行预测和分析，特别是对未知新数据进行预测与分析。</p>
<h3>1.4 方法</h3>
<ul>
<li>监督学习（supervised learning）</li>
<li>非监督学习（unsupervised learning）</li>
<li>半监督学习（semi-supervised learning）</li>
<li>强化学习（reinforcement learning）</li>
</ul>
<h3>1.5 统计学习方法三要素</h3>
<ul>
<li><strong>模型（model）</strong>：即假设空间（hypothesis space），假设空间是一个集合，这个集合包含要学习的模型</li>
<li><strong>策略（strategy）</strong>：模型选择的准则</li>
<li><strong>算法（algorithm）</strong>：模型学习的算法</li>
</ul>
<h2>2. 监督学习</h2>
<p>监督学习（supervised learning）的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。</p>
<h3>2.1 基本概念</h3>
<ul>
<li><strong>输入空间（input space）</strong>：输入的所有可能取值的集合，表示为 <span class="math">\({\cal X}\)</span></li>
<li><strong>输出空间（output space）</strong>：输出的所有可能取值的集合，表示为 <span class="math">\({\cal Y}\)</span></li>
<li><strong>实例（instance）</strong>：每个具体的输入，通常由特征向量（feature vector）表示</li>
<li><strong>特征空间（feature space）</strong>：所有特征向量存在的空间，每一维度对应一个特征</li>
<li><strong>训练数据（training data）</strong>：由输入（或特征向量）与输出对组成</li>
<li><strong>联合概率分布（joint probability distribution）</strong>：监督学习假设输入与输出的随机变量 <span class="math">\(X\)</span> 和 <span class="math">\(Y\)</span> 遵循联合概率分布 <span class="math">\(P(X,Y)\)</span></li>
<li><strong>假设空间（hypothesis space）</strong>：模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。假设空间的确定意味着学习范围的确定</li>
</ul>
<p><strong>注1</strong>：输入和输出空间可以使有限集也可以是整个欧式空间；输入与输出空间可以使用一个空间，也可以是不同的空间；通常输出空间远小于输入空间。</p>
<p><strong>注2</strong>：有时假设输入空间与特征空间为相同的空间，对它们不予区分；有时假设输入空间与特征空间为不同的空间，将实例从输入空间映射到特征空间。模型实际上都是定义在特征空间上的。</p>
<p><strong>注3</strong>：在学习过程中，假设联合概率分布存在，但对于学习系统来说，联合概率分布的具体定义是未知的。训练数据与测试数据被看作是依联合概率分布 <span class="math">\(P(X,Y)\)</span> 独立同分布（independent and identically distribution）产生的。统计学习假设数据存在一定的统计规律，<span class="math">\(X\)</span> 和 <span class="math">\(Y\)</span> 遵循联合概率分布 <span class="math">\(P(X,Y)\)</span> 就是监督学习关于数据的基本假设。</p>
<p>在监督学习过程中，将输入和输出看做是定义在输入（特征）空间与输出空间上的随机变量的取值。输入、输出变量用大写字母表示，输入、输出变量所取的值用小写字母表示。</p>
<p>输入实例 <span class="math">\(x\)</span> 的特征向量记作：<span class="math">\(x=\left(x^{(1)},x^{(2)},\dots,x^{(n)}\right)^\text{T}\)</span>，<span class="math">\(x^{(i)}\)</span> 表示 <span class="math">\(x\)</span> 的第 <span class="math">\(i\)</span> 个特征。</p>
<p>多个输入变量的第 <span class="math">\(i\)</span> 个记作：<span class="math">\(x_i=\left(x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)}\right)^\text{T}\)</span></p>
<p>训练集：<span class="math">\(T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span></p>
<p>监督学习的模型可以是概率模型或非概率模型，由条件概率分布 <span class="math">\(P(Y|X)\)</span> 或决策函数（decision function）<span class="math">\(Y=f(X)\)</span> 表示。对具体的输入进行相应的输出预测时，写作 <span class="math">\(P(y|x)\)</span> 或 <span class="math">\(y=f(x)\)</span>。</p>
<h2>3. 统计学习三要素</h2>
<h3>3.1 模型</h3>
<p>统计学习首要考虑的问题是学习什么样的模型。模型的假设空间包含所有可能的条件概率分布或决策函数。假设空间中的模型一般有无穷多个。</p>
<p>假设空间用 <span class="math">\(\cal{F}\)</span> 表示。假设空间可以定义为决策函数或条件概率分布的集合
</p>
<div class="math">$${\cal F}=\{f|Y=f_\theta(X),\theta \in \mathbb R^n\}\ \ 或\ \ {\cal F}=\{P|P_\theta(Y|X),\theta \in \mathbb R^n\}$$</div>
<p>
参数向量 <span class="math">\(\theta\)</span> 取值于 <span class="math">\(n\)</span> 维欧式空间 <span class="math">\(\mathbb R^n\)</span>，称为参数空间（parameter space）。</p>
<h3>3.2 策略</h3>
<p>有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。</p>
<h4>3.2.1 损失函数和风险函数</h4>
<p>损失函数（loss function）或代价函数（cost function）用来度量预测错误的程度。损失函数是 <span class="math">\(f(X)\)</span> 和 <span class="math">\(Y\)</span> 的非负实值函数，记作 <span class="math">\(L(Y,f(X))\)</span>。</p>
<p>统计学习常用的损失函数有以下几种：</p>
<ol>
<li>
<p>0-1 损失函数（0-1 loss function）
<div class="math">$$L(Y,f(X)=\begin{cases}
1, &amp; Y\neq f(X) \\
0, &amp; Y=f(X)
\end{cases}$$</div>
</p>
</li>
<li>
<p>平方损失函数（quadratic loss function）
<div class="math">$$L(Y,f(X))=(Y-f(X))^2$$</div>
</p>
</li>
<li>
<p>绝对损失函数（absolute loss function）
<div class="math">$$L(Y,f(X))=|Y-f(X)|$$</div>
</p>
</li>
<li>
<p>对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）
<div class="math">$$L(Y,P(Y|X)=-\log P(Y|X)$$</div>
</p>
</li>
</ol>
<p>风险函数（risk function）或期望损失（expected loss）：理论上模型 <span class="math">\(f(X)\)</span> 关于联合分布 <span class="math">\(P(X,Y)\)</span> 评价意义下的损失，记作 <span class="math">\(R_\text{exp}\)</span>。
</p>
<div class="math">$$R_\text{exp}(f)=\text{E}_P[L(Y,f(X))]=\int_{{\cal X}\times{\cal Y}}L(y,f(x))P(x,y)dxdy$$</div>
<p>经验风险（empirical risk）或经验损失（empirical loss）：模型 <span class="math">\(f(X)\)</span> 关于训数据集的平均损失，记作 <span class="math">\(R_\text{emp}\)</span>。
</p>
<div class="math">$$R_\text{emp}(f)=\frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)$$</div>
<h4>3.2.2 经验风险最小化</h4>
<p>经验风险最小化（empirical risk minimization，ERM）策略认为：经验风险最小的模型是最优的模型。即求解下面的最优化问题
</p>
<div class="math">$$ \min_{f\in {\cal F}} \frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)$$</div>
<p>
其中 <span class="math">\({\cal F}\)</span> 是假设空间。</p>
<p>当样本容量足够大时，经验风险最小化能保证有很好的学习效果。例如，极大似然估计（maximum likelihood estimation）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。</p>
<p>但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合（over-fitting）现象”。</p>
<h4>3.2.3 结构风险最小化</h4>
<p>结构风险最小化（structural risk minimization，SRM）是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），定义如下
</p>
<div class="math">$$R_\text{srm}=\frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)+\lambda J(f)$$</div>
<p>
其中 <span class="math">\(J(f)\)</span> 为模型的复杂度，是定义在假设空间 <span class="math">\({\cal F}\)</span> 上的泛函。模型 <span class="math">\(f\)</span> 越复杂，<span class="math">\(J(f)\)</span> 越大。也就是说，复杂度表示了对复杂模型的惩罚，<span class="math">\(\lambda\geq 0\)</span> 是系数，用以权衡经验风险和模型复杂度。</p>
<p>结构风险小需要经验风险与模型复杂度同时小，结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。结构化风险最小化策略认为：结构风险最小的模型是最优的模型。即求解下面的最优化问题
</p>
<div class="math">$$ \min_{f\in {\cal F}} \frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)+\lambda J(f)$$</div>
<p>
贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation，MAP）就是结构化风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。</p>
<h3>3.3 算法</h3>
<p>算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。</p>
<p>如果最优化问题有显式的解析解，这个最优化问题就比较简单，但通常解析解不存在，这就需要用数值方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。</p>
<h2>4. 模型评估与模型选择</h2>
<h3>4.1 训练误差</h3>
<p>假设学习到的模型是 <span class="math">\(Y=\hat{f}(X)\)</span>，训练误差（training error）是模型 <span class="math">\(Y=\hat{f}(X)\)</span> 关于训练数据集的平均损失
</p>
<div class="math">$$R_\text{emp}(\hat{f})=\frac{1}{N}\sum_{i=1}^NL(y_i,\hat{f}(x_i))$$</div>
<p>其中 <span class="math">\(N\)</span> 是训练样本容量。</p>
<h3>4.2 测试误差</h3>
<p>测试误差是模型 <span class="math">\(Y=\hat{f}(X)\)</span> 关于测试数据集的平均损失
</p>
<div class="math">$$e_\text{test}(\hat{f})=\frac{1}{N'}\sum_{i=1}^{N'}L(y_i,\hat{f}(x_i))$$</div>
<p>其中 <span class="math">\(N'\)</span> 是测试样本容量。</p>
<p>例如，当损失函数是 0-1 损失时，测试误差就变成了常见的测试数据集上的误差率（error rate）
</p>
<div class="math">$$e_\text{test}=\frac{1}{N'}\sum_{i=1}^{N'}{\mathbb I}(y_i\neq \hat{f}(x_i))$$</div>
<p>
这里 <span class="math">\({\mathbb I}\)</span> 是指示函数，即 <span class="math">\(y_i\neq \hat{f}(x_i)\)</span> 时为 1，否则为 0。</p>
<p>相应的，常见的测试数据集上的准确率（accuracy）为
</p>
<div class="math">$$r_\text{test}=\frac{1}{N'}\sum_{i=1}^{N'}{\mathbb I}(y_i=\hat{f}(x_i))$$</div>
<p>训练误差的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，但本质上不重要。</p>
<p>测试误差反应了学习方法对未知的测试数据集的预测能力，是学习中的重要概念。</p>
<h3>4.3 过拟合</h3>
<p>当假设空间含有不同复杂度（例如，不同的参数个数）的模型时，就要面临模型选择（model selection）的问题。我们希望选择或学习一个合适的模型。</p>
<p>如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，这种现象称为过拟合。</p>
<p>过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测很差的现象。</p>
<p>训练误差与测试误差和模型复杂度关系如下：</p>
<p><img alt="训练误差与测试误差和模型复杂度" src="https://xutree.github.io/images/statistical_learning_1.3.png"></p>
<h2>5. 正则化与交叉验证</h2>
<p>正则化与交叉验证是两种常用的模型选择方法。</p>
<h3>5.1 正则化</h3>
<p>正则化是结构风险最小化策略的实现。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。</p>
<p>正则化一般形式形式
</p>
<div class="math">$$ \min_{f\in {\cal F}} \frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)+\lambda J(f)$$</div>
<p>
其中，第一项是经验风险，第二项是正则化项，<span class="math">\(\lambda\geq 0\)</span> 为调整两者关系之间的系数。</p>
<p>正则化项可以取不同的形式。例如：回归问题中，损失函数是平方损失，正则化项可以是参数向量的 <span class="math">\(L_2\)</span> 范数
</p>
<div class="math">$$L(w)=\frac{1}{N}\sum_{i=1}^N\left(f(x_i;w)-y_i\right)^2+\frac{\lambda}{2}||w||^2$$</div>
<p>
这里，<span class="math">\(||w||\)</span> 表示参数向量 <span class="math">\(w\)</span> 的 <span class="math">\(L_2\)</span> 范数。</p>
<h3>5.2 交叉验证</h3>
<p>另一种常用的模型选择方法是交叉验证（cross validation）。</p>
<p>如果给定的样本数据充足，进行模型选择的一种简单方法是随机的将数据集切成三部分：训练集、验证集和测试集。训练集用来训练模型，验证集用于模型选择，测试集用于最终对学习方法的评估。</p>
<p>但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证的方法。</p>
<p>交叉验证的基本思想是重复的使用数据：把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复进行训练、测试以及模型选择。</p>
<h4>5.2.1 简单交叉验证</h4>
<p>首先随机的将已给数据分成两部分：训练集、测试集。然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型。然后，在测试集上评价各个模型的测试误差，选出测试误差最小的模型。</p>
<h4>5.2.2 <span class="math">\(S\)</span> 折交叉验证</h4>
<p>应用最多的是 <span class="math">\(S\)</span> 折交叉验证（S-fold cross validation），方法如下：首先随机的将已给数据切分为 <span class="math">\(S\)</span> 个互不相交的大小相同的子集；然后利用 <span class="math">\(S-1\)</span> 个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的 <span class="math">\(S\)</span> 种选择重复进行；最后选出 <span class="math">\(S\)</span> 次评估中平均测试误差最小的模型。</p>
<h4>5.2.3 留一交叉验证</h4>
<p><span class="math">\(S\)</span> 折交叉验证的特殊情形是 <span class="math">\(S=N\)</span>，称为留一交叉验证（leave-one-out cross validation），往往在数据缺乏的情况下使用。这里 <span class="math">\(N\)</span> 是给定数据集的容量。</p>
<h2>1.6 泛化能力</h2>
<h3>1.6.1 泛化误差</h3>
<p>学习方法的泛化能力（generalization ability）是指由该学习方法学到的模型对未知数据的预测能力。</p>
<p>泛化误差（generalization error）：如果学到的模型是 <span class="math">\(\hat{f}\)</span>，那么用这个模型对未知数据预测的误差即为泛化误差。
</p>
<div class="math">$$R_\text{exp}(\hat{f})=\text{E}_P[L(Y,f(X))]=\int_{{\cal X}\times{\cal Y}}L(y,\hat{f}(x))P(x,y)dxdy$$</div>
<p>
实际上，泛化误差就是所学习到的模型的期望风险。</p>
<h3>1.6.2 泛化误差上界</h3>
<p>学习方法的泛化能力分析往往使用过研究泛化误差的概率上界进行的，简称为泛化误差上界（generalization error bound）。也就是说，可以通过比较两种学习方法的泛化误差上界的大小来比较它们的优劣。</p>
<p>泛化误差上界具有如下性质：</p>
<ul>
<li>它是样本容量的函数，当样本容量增加时，泛化误差上界趋于 0</li>
<li>它是假设空间容量（capacity）的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大</li>
</ul>
<h3>1.6.3 二分类问题的泛化误差上界</h3>
<p>考虑二分类问题。已知训练数据集 <span class="math">\(T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，它是从联合概率分布 <span class="math">\(P(X,Y)\)</span> 独立同分布产生的，<span class="math">\(X\in \mathbb R^n\)</span>，<span class="math">\(Y\in \{-1,1\}\)</span>。假设空间是函数的有限集合 <span class="math">\({\cal F}=\{f_1.f_2.\dots,f_d\}\)</span>，<span class="math">\(d\)</span> 是函数个数。设 <span class="math">\(f\)</span> 是从 <span class="math">\({\cal F}\)</span> 中选取的函数。损失函数是 0-1 损失。关于 <span class="math">\(f\)</span> 的期望风险和经验风险分别是
</p>
<div class="math">$$R_\text{exp}(f)=\text{E}_P[L(Y,f(X))]$$</div>
<div class="math">$$R_\text{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$</div>
<p>
经验风险最小化函数是
</p>
<div class="math">$$f_N=\arg \min_{f\in{\cal F}}R_\text{emp}(f)$$</div>
<p>
人们更关心的是 <span class="math">\(f_N\)</span> 的泛化能力
</p>
<div class="math">$$R(f_N)=\text{E}_P[L(Y,f_N(X))]$$</div>
<p><strong>定理 1.1（泛化误差上界）对二分类问题，当假设空间是有限个函数的集合 <span class="math">\({\cal F}=\{f_1.f_2.\dots,f_d\}\)</span> 时，对任意一个函数 <span class="math">\(f\in{\cal F}\)</span>，至少以概率 <span class="math">\(1-\delta\)</span>，以下不等式成立
<div class="math">$$R(f)\leq R_\text{emp}(f)+\epsilon(d,N,\delta)$$</div>
其中，
<div class="math">$$\epsilon(d,N,\delta)=\sqrt{\frac{1}{2N}\left(\log d+\log \frac{1}{\delta}\right)}$$</div></strong></p>
<p>上述定理表明：训练误差越小，泛化误差也越小；当 <span class="math">\(N\)</span> 趋于无穷时，第二项为 0；假设空间包含的函数越多，泛化误差越大。</p>
<p>证明：在证明过程中要用到 Hoeffding 不等式，先叙述如下：</p>
<p>Hoeffding 不等式适用于有界的随机变量。设有两两独立的一系列随机变量 <span class="math">\(X_{1},\dots ,X_{n}\)</span>。假设对所有的 <span class="math">\(1\leq i\leq n\)</span>，<span class="math">\(X_{i}\)</span> 都是几乎有界的变量，即满足
</p>
<div class="math">$$\mathbb {P} (X_{i}\in [a_{i},b_{i}])=1$$</div>
<p>
那么这 <span class="math">\(n\)</span> 个随机变量的经验期望
</p>
<div class="math">$$\overline {X}={\frac {X_{1}+\cdots +X_{n}}{n}}$$</div>
<p>
满足以下的不等式
</p>
<div class="math">$$\mathbb {P}({\overline {X}}-\mathbb {E} [{\overline {X}}]\geq t)\leq \exp \left(-{\frac {2t^{2}n^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}}\right)$$</div>
<div class="math">$$\mathbb {P} (|{\overline {X}}-\mathbb {E} [{\overline {X}}]|\geq t)\leq 2\exp \left(-{\frac {2t^{2}n^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}}\right)$$</div>
<p>
对任意函数 <span class="math">\(f\in{\cal F}\)</span>，<span class="math">\(R_\text{emp}(f)\)</span> 是 <span class="math">\(N\)</span> 个随机变量 <span class="math">\(L(Y,f(X))\)</span> 的样本均值，<span class="math">\(R(f)\)</span> 是随机变量 <span class="math">\(L(Y,f(X))\)</span> 的期望值。如果损失函数取值于区间 [0,1]，则由 Hoeffding 不等式得到，对 <span class="math">\(\epsilon&gt;0\)</span>，以下不等式成立
</p>
<div class="math">$$P(R(f)-R_\text{emp}(f)\geq\epsilon)\leq\exp (-2N\epsilon^2)$$</div>
<p>
由于 <span class="math">\({\cal F}=\{f_1;f_2,\dots,f_d\}\)</span> 是一个有限集合，故
</p>
<div class="math">$$P\left(\exists f\in{\cal F}:R(f)-R_\text{emp}(f\right)\geq\epsilon)=P\left(\bigcup_{f\in{\cal F}}\{R(f)-R_\text{emp}(f)\geq\epsilon\}\right)\\
\leq\sum_{f\in{\cal F}}P(R(f)-R_\text{emp}(f)\geq\epsilon)\\
\leq d\exp(-2N\epsilon^2)$$</div>
<p>
或者等价的，对任意的 <span class="math">\(f\in{\cal F}\)</span>，有 </p>
<div class="math">$$P(R(f)-R_\text{emp}(f)\geq\epsilon))\geq1-d\exp(-2N\epsilon^2)$$</div>
<p>
令
</p>
<div class="math">$$\delta=d \exp(-2N\epsilon^2)$$</div>
<p>
则
</p>
<div class="math">$$P(R(f)&lt;R_\text{emp}(f)+\epsilon)\geq1-\delta$$</div>
<p>
证毕。</p>
<h2>1.7 生成模型与判别模型</h2>
<p>监督学习方法也可以分为生成方法（generative approach）和判别方法（discriminative approach）。所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）。</p>
<h3>1.7.1 生成方法</h3>
<p>生成方法由数据学习联合概率分布 <span class="math">\(P(X,Y)\)</span>，然后求出条件概率分布 <span class="math">\(P(Y|X)\)</span> 作为预测的模型，即生成模型
</p>
<div class="math">$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$</div>
<p>
这样的方法之所以称为生成方法，是因为模型表示了给定输入 <span class="math">\(X\)</span> 产生输出 <span class="math">\(Y\)</span> 的生成关系。</p>
<p>典型的生成模型有：朴素贝叶斯法和隐马尔科夫模型。</p>
<h3>1.7.2 判别方法</h3>
<p>判别方法由数据直接学习策略函数 <span class="math">\(f(X)\)</span> 或者条件概率分布 <span class="math">\(P(Y|X)\)</span> 作为预测的模型。判别方法关心的是对给定的输入 <span class="math">\(X\)</span>，应该预测什么样的输出 <span class="math">\(Y\)</span>。</p>
<p>典型的判别模型有：<span class="math">\(k\)</span> 近邻法，感知机，决策树，logistic 回归模型，最大熵模型，支持向量机，提升方法和条件随机场等。</p>
<h3>1.7.3 不同方法的特点</h3>
<p>生成方法：</p>
<ul>
<li>生成方法可以还原出联合概率分布 <span class="math">\(P(X,Y)\)</span>，而判别方法则不能</li>
<li>生成方法的学习收敛速度更快，即当样本容量增加时，学到的模型可以更快的收敛到真实模型</li>
<li>当存在隐变量时，仍可以使用生成方法，此时判别方法就不能用了</li>
</ul>
<p>判别方法：</p>
<ul>
<li>判别方法直接学习的是条件概率 <span class="math">\(P(Y|X)\)</span> 或决策函数 <span class="math">\(f(X)\)</span>，直接面对预测，往往学习的准确率更高</li>
<li>由于直接学习 <span class="math">\(P(Y|X)\)</span> 或 <span class="math">\(f(X)\)</span>，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题</li>
</ul>
<h2>1.8 分类问题</h2>
<p>在监督学习中，当输出变量 <span class="math">\(Y\)</span> 取有限个离散值时，预测问题便成为分类问题。</p>
<p><strong>分类器（classifier）</strong>：从数据中学习到的一个分类模型或分类决策函数。</p>
<p>评价分类器性能的指标一般是<strong>分类准确率（accuracy）</strong>：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。</p>
<p>对于二分类问题常用的评价指标是<strong>精确率（precision）</strong>和<strong>召回率（recall）</strong>。</p>
<p>通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4 种情况出现的总数分别记为：</p>
<ul>
<li>TP：将正类预测为正类的数目</li>
<li>FN：将正类预测为负类的数目</li>
<li>FP：将负类预测为正类的数目</li>
<li>TN：将负类预测为负类的数目</li>
</ul>
<p>精确率定义
</p>
<div class="math">$$P=\frac{TP}{TP+FP}$$</div>
<p>
召回率定义
</p>
<div class="math">$$R=\frac{TP}{TP+FN}$$</div>
<p>
<span class="math">\(F_1\)</span> 值
</p>
<div class="math">$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}\longrightarrow
F_1=\frac{2TP}{2TP+FP+FN}$$</div>
<p>许多统计学习方法可以用于分类，包括 <span class="math">\(k\)</span> 近邻法，感知机，朴素贝叶斯法，决策树，决策列表，logistic 回归模型，支持向量机，提升方法，贝叶斯网络，神经网络，Winnow 等。</p>
<p>一个分类应用的例子：垃圾邮件、非垃圾邮件分类。</p>
<h2>1.9 标注问题</h2>
<p>标注（tagging）也是一个监督学习问题，可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测（structure prediction）问题的简单形式。</p>
<p>标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。注意，可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的。</p>
<p>评价标注模型的指标和分类模型一样，常用的有标注准确率、精确率和召回率。</p>
<p>标注常用的统计学习方法有：隐马尔科夫模型，条件随机场。</p>
<p>一个标注的例子：对英文文章进行标注，英文单词是一个观察，英文句子是一个观察序列，标记表示名词短语的“开始”、“结束”和“其他”。</p>
<h2>1.10 回归问题</h2>
<p>回归（regression）是监督学习的另一个重要问题。回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化。回归问题等价于函数拟合：选择一条函数曲线使其很好的拟合已知数据且很好的预测未知数据。</p>
<p>回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间的关系分为线性回归和非线性回归。</p>
<p>回归学习中最常用的损失函数是平方损失函数，在此情况下，回归问题可以用最小二乘法（least squares）求解。</p>
<p>一个回归的例子：市场趋势预测。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <aside>
                    <hr />
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/28/指示器随机变量/">
                                    指示器随机变量
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/27/find_maximum_subarray/">
                                    最大子数组问题
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/26/sort/">
                                    排序算法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/23/矩阵求导/">
                                    矩阵微积分
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/21/C++_Primer_Chapter_17/">
                                    C++ Primer 第十七章 标准库特殊设施
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="newer">
                        <h1>
                            <font color="#771515"><em>NEWER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/2/">
                                    统计学习方法 第二章 感知机
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <!-- Gitalk 评论 start  -->

                    <!-- Link Gitalk 的支持文件  -->
                    <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                    <div id="gitalk-container"></div>
                    <script type="text/javascript">
                        var dateTime = Date.now();
                        var timestamp = Math.floor(dateTime / 1000);
                        var gitalk = new Gitalk({

                            // gitalk的主要参数
                            clientID: '93f43349e9fd3154bfad',
                            clientSecret: 'd6d09d1d7261f6b62f46b39e5fcace85b81c3cd7',
                            repo: 'xutree.github.io',
                            owner: 'xutree',
                            admin: ['xutree'],
                            id: String(timestamp)

                        });
                        gitalk.render('gitalk-container');
                    </script> -->
                    <!-- Gitalk end -->
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-04T12:54:21+08:00">2018-11-04 12:54:21</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-04 22:09:30</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>2</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>2</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>