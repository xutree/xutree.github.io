<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第二章 感知机  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/04/sl-2/"> 统计学习方法 第二章 感知机  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
        <div class="span2" style="float:left;font-size:1em;">
            <nav>
                <!-- <h4>目录</h4> -->
                <div class="toc">
<ul>
<li><a href="#21">2.1 感知机模型</a></li>
<li><a href="#22">2.2 感知机学习策略</a><ul>
<li><a href="#221">2.2.1 数据集的线性可分性</a></li>
<li><a href="#222">2.2.2 感知机学习策略</a></li>
</ul>
</li>
<li><a href="#23">2.3 感知机学习算法</a><ul>
<li><a href="#231">2.3.1 感知机学习算法的原始形式</a></li>
<li><a href="#232">2.3.2 算法的收敛性</a></li>
<li><a href="#233">2.3.3 感知机学习算法的对偶形式</a></li>
</ul>
</li>
</ul>
</div>
            </nav>
        </div>
        <div class="span8 article-content">

                
<p>感知机（perceptron）是二分类的线性分类模型，其输入是实例的特征向量，输出为实例的类别，取 +1 和 -1 二值。感知机对应于输入空间（特征空间）中将实例划分为正负类别的分离超平面，属于判别模型。</p>
<h2 id="21">2.1 感知机模型</h2>
<p><strong>定义 2.1（感知机）假设输入空间（特征空间）是 <span class="math">\({\cal X}\subseteq\mathbb{R}^n\)</span>，输出空间是 <span class="math">\({\cal Y}=\{+1,-1\}\)</span>。输入 <span class="math">\(x\in{\cal X}\)</span> 表示实例的特征向量，对应于输入空间（特征空间）的点；输出 <span class="math">\(y\in{\cal Y}\)</span> 表示实例的类别。由输入空间到输出空间的如下函数：
<div class="math">$$f(x)=\text{sign}(w\cdot x+b)$$</div>
称为感知机。其中，<span class="math">\(w\)</span> 和 <span class="math">\(b\)</span> 为感知机模型参数，<span class="math">\(w\in\mathbb{R}^n\)</span> 叫做权值（weight）或权值向量（weight vector），<span class="math">\(b\in\mathbb{R}\)</span> 叫做偏置（bias），<span class="math">\(w\cdot x\)</span> 表示 <span class="math">\(w\)</span> 和 <span class="math">\(x\)</span> 的内积。sign 是符号函数，即：
<div class="math">$$\text{sign}(x)=\begin{cases}+1, &amp; x\geq0 \\ -1, &amp; x&lt;0\end{cases}$$</div></strong></p>
<p>感知机是一种线性分类模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合：<span class="math">\(\{f|f(x)=w\cdot x+b\}\)</span>。</p>
<p>感知机的几何解释：对于线性方程
</p>
<div class="math">$$w\cdot x+b=0$$</div>
<p>
对应于特征空间 <span class="math">\(\mathbb{R}^n\)</span> 中的一个超平面 <span class="math">\(S\)</span>，称为分离超平面（separating hyperplane），其中 <span class="math">\(w\)</span> 是超平面的法向量，<span class="math">\(b\)</span> 是超平面的截距。这个超平面将特征空间划分为两个部分。</p>
<h2 id="22">2.2 感知机学习策略</h2>
<h3 id="221">2.2.1 数据集的线性可分性</h3>
<p><strong>定义 2.2 （数据集的线性可分性）给定一个数据集
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
其中，<span class="math">\(x_i\in{\cal X}=\mathbb{R}^n\)</span>，<span class="math">\(y_i\in{\cal Y}=\{+1,-1\}\)</span>，<span class="math">\(i=1,2,\cdots,N\)</span>，如果存在某个超平面 <span class="math">\(S\)</span>
<div class="math">$$w\cdot x+b=0$$</div>
能够将数据集的正负实例完全正确的划分到超平面的两侧，则称数据集 <span class="math">\(T\)</span> 是线性可分数据集（linearly separable data set）；否则，则称数据集 <span class="math">\(T\)</span> 线性不可分。</strong></p>
<h3 id="222">2.2.2 感知机学习策略</h3>
<p>为了找出超平面，需要一个学习策略，即定义（经验）损失函数并将损失函数极小化。</p>
<p>损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数 <span class="math">\(w\)</span>、<span class="math">\(b\)</span> 的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面 <span class="math">\(S\)</span> 的总距离，这是感知机所采用的。</p>
<p>首先，输入空间 <span class="math">\(\mathbb{R}^n\)</span> 中任一点 <span class="math">\(x_0\)</span> 到超平面 <span class="math">\(S\)</span> 的距离为：
</p>
<div class="math">$$\frac{1}{||w||}|w\cdot x+b|$$</div>
<p>
这里，<span class="math">\(||w||\)</span> 是 <span class="math">\(w\)</span> 的 <span class="math">\(L_2\)</span> 范数。</p>
<p>其次，注意到对于误分类点数据 <span class="math">\((x_i,y_i)\)</span>，下式成立：
</p>
<div class="math">$$-y_i(w\cdot x_i+b)&gt;0$$</div>
<p>
因此，误分类点到超平面的总距离为：
</p>
<div class="math">$$-\frac{1}{||w||}\sum_{x_i\in M}y_i(w\cdot x_i+b)$$</div>
<p>
其中，<span class="math">\(M\)</span> 是误分类点集合。不考虑 <span class="math">\(\frac{1}{||w||}\)</span>，就得到感知机学习的损失函数。</p>
<p>给定训练数据集：
</p>
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
感知机 <span class="math">\(\text{sign}(w\cdot x+b)\)</span> 学习的损失函数定义为：
</p>
<div class="math">$$L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$</div>
<p>感知机学习的策略是在假设空间中选取使上面的损失函数最小的模型参数 <span class="math">\(w\)</span>，<span class="math">\(b\)</span>，即感知机模型。</p>
<h2 id="23">2.3 感知机学习算法</h2>
<p>为最优化上节的损失函数，采取随机梯度下降法（stochastic gradient descent）。</p>
<h3 id="231">2.3.1 感知机学习算法的原始形式</h3>
<p>感知机学习算法是对以下最优化问题的算法。给定训练数据集：
</p>
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
<p>
其中 <span class="math">\(x_i\in{\cal X}=\mathbb{R}^n\)</span>，<span class="math">\(y_i\in{\cal Y}=\{+1,-1\}\)</span>，<span class="math">\(i=1,2,\cdots,N\)</span>，求参数 <span class="math">\(w\)</span>，<span class="math">\(b\)</span>，使其为以下损失函数极小化的一个解：
</p>
<div class="math">$$\min_{w,b} L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$</div>
<p>
其中，<span class="math">\(M\)</span> 是误分类点集合。</p>
<p>随机梯度下降法步骤如下：首先，任意选取一个超平面 <span class="math">\(w_0\)</span>，<span class="math">\(b_0\)</span>，然后用梯度下降法不断地极小化目标函数。极小化过程不是一次使 <span class="math">\(M\)</span> 中所有误分类法的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</p>
<p>假设误分类点集合 <span class="math">\(M\)</span> 是固定的，那么损失函数 <span class="math">\(L(w,b)\)</span> 的梯度为：
</p>
<div class="math">$$\begin{eqnarray}
\nabla_wL(w,b) &amp;=&amp; -\sum_{x_i\in M}y_ix_i \\
\nabla_bL(w,b) &amp;=&amp; -\sum_{x_i\in M}y_i
\end{eqnarray}$$</div>
<p>随机选取一个误分类点 <span class="math">\((x_i,y_i)\)</span> 对 <span class="math">\(w\)</span>，<span class="math">\(b\)</span> 进行更新：
</p>
<div class="math">$$\begin{eqnarray}
w &amp;\longleftarrow&amp; w+\eta y_ix_i \\
b &amp;\longleftarrow&amp; b+\eta y_i
\end{eqnarray}$$</div>
<p>
式中 <span class="math">\(\eta\ (0&lt;\eta\leq 1)\)</span> 是步长，也称学习率（learning rate）。</p>
<p><strong>算法 2.1 （感知机学习算法的原始形式)<br/>
输入：训练数据集
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
其中
<div class="math">$$x_i\in{\cal X}=\mathbb{R}^n,y_i\in{\cal Y}=\{+1,-1\},i=1,2,\cdots,N$$</div>
学习率
<div class="math">$$\eta\ (0&lt;\eta\leq 1)$$</div>
输出：<span class="math">\(w\)</span>，<span class="math">\(b\)</span>；
感知机模型
<div class="math">$$f(x)=\text{sign}(w\cdot x+b)$$</div>
(1) 选取初值 <span class="math">\(w_0\)</span>，<span class="math">\(b_0\)</span><br/>
(2) 在训练集中选取数据 <span class="math">\((x_i,y_i)\)</span><br/>
(3) 如果 <span class="math">\(y_i(w\cdot x_i+b)\leq 0\)</span>
<div class="math">$$\begin{eqnarray}
w &amp;\longleftarrow&amp; w+\eta y_ix_i \\
b &amp;\longleftarrow&amp; b+\eta y_i
\end{eqnarray}$$</div>
(4) 转至 (2)，直至训练集中没有误分类点</strong></p>
<h3 id="232">2.3.2 算法的收敛性</h3>
<p>为便于推导，将偏置并入权重向量，记为 <span class="math">\(\hat{w}=(w^\text{T},b)^\text{T}\)</span>，同样也将输入向量加以扩充，加进常数 1，记作 <span class="math">\(\hat{x}=(x^\text{T},1)^\text{T}\)</span>。这样，<span class="math">\(\hat{x}\in\mathbb{R}^{n+1}\)</span>，<span class="math">\(\hat{w}\in\mathbb{R}^{n+1}\)</span>，显然 <span class="math">\(\hat{w}\cdot\hat{x}=w\cdot x+b\)</span>。</p>
<p><strong>定理 2.1 （Novikoff）设训练集
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
线性可分，则<br/>
(1). 存在满足条件 <span class="math">\(||\hat{w}_\text{opt}||=1\)</span> 的超平面
<div class="math">$$\hat{w}_\text{opt}\cdot\hat{x}=w_\text{opt}\cdot x_+b_\text{opt}=0$$</div>
将训练数据集完全正确分开；切存在 <span class="math">\(\gamma&gt;0\)</span>，对所有 <span class="math">\(i=1,2,\cdots,N\)</span>
<div class="math">$$y_i(\hat{w}_\text{opt}\cdot\hat{x}_i)=y_i(w_\text{opt}\cdot x_i+b_\text{opt})\geq\gamma$$</div>
(2). 令
<div class="math">$$R=\max_{1\leq i\leq N}||\hat{x}_i||$$</div>，则感知机算法 2.1 在训练数据集上的误分类次数 <span class="math">\(k\)</span> 满足不等式
<div class="math">$$k\leq\left(\frac{R}{\gamma}\right)^2$$</div></strong></p>
<p>证明：（1）由线性可分性的定义即可证明，其中：
</p>
<div class="math">$$\gamma=\min_i\{y_i(w_\text{opt}\cdot x_i+b_\text{opt})\}$$</div>
<p>
（2）感知机算法从 <span class="math">\(\hat{w}_0\)</span> 开始，如果实例被误分类，则更新权重。令 <span class="math">\(\hat{w}_{k-1}\)</span> 是第 <span class="math">\(k\)</span> 个误分类实例之前的扩充权重向量，即：
</p>
<div class="math">$$\hat{w}_{k-1}=(w_{k-1}^\text{T},b_{k-1})^\text{T}$$</div>
<p>
则第 <span class="math">\(k\)</span> 个被误分类实例的条件是
</p>
<div class="math">$$y_i(\hat{w}_{k-1}\cdot\hat{x}_i)=y_i(w_{k-1}\cdot x_i+b_{k-1})\leq0$$</div>
<p>
若 <span class="math">\((x_i,y_i)\)</span> 是被 <span class="math">\(\hat{w}_{k-1}\)</span> 误分类的数据，则 <span class="math">\(w\)</span> 和 <span class="math">\(b\)</span> 的更新为
</p>
<div class="math">$$\begin{eqnarray}
w_k &amp;\longleftarrow&amp; w_{k-1}+\eta y_ix_i \\
b_k &amp;\longleftarrow&amp; b_{k-1}+\eta y_i
\end{eqnarray}$$</div>
<p>
即
</p>
<div class="math">$$\hat{w}_k\longleftarrow \hat{w}_{k-1}+\eta y_i\hat{x}_i$$</div>
<p>下面推导两个不等式：</p>
<p>i. <span class="math">\(\hat{w}_k\cdot\hat{w}_\text{opt}\geq k\eta\gamma\)</span>
</p>
<div class="math">$$\hat{w}_k\cdot\hat{w}_\text{opt}=(\hat{w}_{k-1}+\eta y_i\hat{x}_i)\cdot\hat{w}_\text{opt}\geq\hat{w}_{k-1}\cdot\hat{w}_\text{opt}+\eta\gamma\geq\cdots\geq k\eta\gamma$$</div>
<p>
ii. <span class="math">\(||\hat{w}_k||^2\leq k\eta^2R^2\)</span>
</p>
<div class="math">$$
\begin{eqnarray}
||\hat{w}_k||^2 &amp;=&amp; ||\hat{w}_{k-1}+\eta y_i\hat{x}_i||^2 \\
&amp;=&amp; ||\hat{w}_{k-1}||^2+2\eta y_i\hat{w}_{k-1}\cdot\hat{x}_i+\eta^2||\hat{x}_i||^2 \\
&amp;\leq&amp; ||\hat{w}_{k-1}||^2+\eta^2||\hat{x}_i||^2 \\
&amp;\leq&amp; ||\hat{w}_{k-1}||^2+\eta^2R^2\leq\cdots\leq k\eta^2R^2
\end{eqnarray}$$</div>
<p>由上述两个不等式
</p>
<div class="math">$$k\eta\gamma\leq\hat{w}_k\cdot\hat{w}_\text{opt}\leq||\hat{w}_k||\ ||\hat{w}_\text{opt}||\leq\sqrt{k}\eta R$$</div>
<p>
于是
</p>
<div class="math">$$k\leq\left(\frac{R}{\gamma}\right)^2$$</div>
<p>定理表明，误分类次数是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。</p>
<p>感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。</p>
<p>当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。</p>
<h3 id="233">2.3.3 感知机学习算法的对偶形式</h3>
<p>对偶形式的基本想法是，将 <span class="math">\(w\)</span> 和 <span class="math">\(b\)</span> 表示为实例 <span class="math">\(x_i\)</span> 和标记 <span class="math">\(y_i\)</span> 的线性组合的形式，通过求解其系数而求得 <span class="math">\(w\)</span> 和 <span class="math">\(b\)</span>。不失一般性，在算法 2.1 中可以假设初始值 <span class="math">\(w_0\)</span> 和 <span class="math">\(b_0\)</span> 均为 0，对误分类点 <span class="math">\((x_i,y_i)\)</span> 通过
</p>
<div class="math">$$\begin{eqnarray}
w &amp;\longleftarrow&amp; w+\eta y_ix_i \\
b &amp;\longleftarrow&amp; b+\eta y_i
\end{eqnarray}$$</div>
<p>
逐步修改 <span class="math">\(w\)</span>，<span class="math">\(b\)</span>，设修改 <span class="math">\(n\)</span>次（随机梯度下降，一个点可能被选择多次），则 <span class="math">\(w\)</span> 和 <span class="math">\(b\)</span> 关于 <span class="math">\((x_i,y_i)\)</span>  的增量分别是 <span class="math">\(\alpha_iy_ix_i\)</span> 和 <span class="math">\(\alpha_iy_i\)</span> 这里
</p>
<div class="math">$$\alpha_i=n_i\eta$$</div>
<p>
这样，从学习过程不难看出，最后学习到的 <span class="math">\(w\)</span> 和 <span class="math">\(b\)</span> 可以分别表示为
</p>
<div class="math">$$\begin{eqnarray}
w &amp;=&amp; \sum_{i=1}^N\alpha_iy_ix_i \\
b &amp;=&amp; \sum_{i=1}^N\alpha_iy_i
\end{eqnarray}$$</div>
<p>
这里，<span class="math">\(\alpha_i\geq0\)</span>，<span class="math">\(i=1,2,\cdots,N\)</span>，当 <span class="math">\(\eta=1\)</span> 时，表示第 <span class="math">\(i\)</span> 个实例点由于误分类而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。换句话说，这样的实例对学习结果影响很大。</p>
<p><strong>算法 2.2 （感知机学习算法的对偶形式）<br/>
输入：训练数据集
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
其中
<div class="math">$$x_i\in{\cal X}=\mathbb{R}^n,y_i\in{\cal Y}=\{+1,-1\},i=1,2,\cdots,N$$</div>
学习率
<div class="math">$$\eta\ (0&lt;\eta\leq 1)$$</div>
输出：<span class="math">\(\alpha\)</span>，<span class="math">\(b\)</span>；
感知机模型
<div class="math">$$f(x)=\text{sign}(\sum_{j=1}^N\alpha_jy_jx_j\cdot x+b)$$</div>
其中 <span class="math">\(\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^\text{T}\)</span><br/>
(1) <span class="math">\(\alpha\longleftarrow0\)</span>， <span class="math">\(b\longleftarrow0\)</span><br/>
(2) 在训练集中选取数据 <span class="math">\((x_i,y_i)\)</span><br/>
(3) 如果 <span class="math">\(y_i\left(\sum_{j=1}^N\alpha_jy_jx_j\cdot x_i+b\right)\leq 0\)</span>
<div class="math">$$\begin{eqnarray}
\alpha_i &amp;\longleftarrow&amp; \alpha_i+\eta \\
b &amp;\longleftarrow&amp; b+\eta y_i
\end{eqnarray}$$</div>
(4) 转至 (2)，直至训练集中没有误分类点</strong></p>
<p>对偶形式中训练实例仅以内积的形式出现。为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的 Gram 矩阵（Gram matrix）
</p>
<div class="math">$$G=[x_i\cdot x_j]_{N\times N}$$</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <aside>
                    <hr />
                    <nav class="related">
                        <h1>
                            <font color="#771515"><em>RELATED</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/20/sl-12/">统计学习方法 第十二章 统计学习方法总结</a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-11/">统计学习方法 第十一章 条件随机场</a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-10/">统计学习方法 第十章 隐马尔科夫模型</a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/17/sl-9/">统计学习方法 第九章 EM 算法及其推广</a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-8/">统计学习方法 第八章 提升方法</a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/sl-1/">
                                    统计学习方法 第一章 统计学习方法概论
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/28/指示器随机变量/">
                                    指示器随机变量
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/27/find_maximum_subarray/">
                                    最大子数组问题
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/26/sort/">
                                    排序算法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/10/23/矩阵求导/">
                                    矩阵微积分
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="newer">
                        <h1>
                            <font color="#771515"><em>NEWER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/04/sl-3/">
                                    统计学习方法 第三章 k 近邻法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/05/bayes/">
                                    贝叶斯定理
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/07/sl-4/">
                                    统计学习方法 第四章 朴素贝叶斯法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/07/sl-5/">
                                    统计学习方法 第五章 决策树
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/09/sl-6/">
                                    统计学习方法 第六章 逻辑回归与最大熵模型
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <!-- Gitalk 评论 start  -->

                    <!-- Link Gitalk 的支持文件  -->
                    <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                    <div id="gitalk-container"></div>
                    <script type="text/javascript">
                        var dateTime = Date.now();
                        var timestamp = Math.floor(dateTime / 1000);
                        var gitalk = new Gitalk({

                            // gitalk的主要参数
                            clientID: '93f43349e9fd3154bfad',
                            clientSecret: 'd6d09d1d7261f6b62f46b39e5fcace85b81c3cd7',
                            repo: 'xutree.github.io',
                            owner: 'xutree',
                            admin: ['xutree'],
                            id: String(timestamp)

                        });
                        gitalk.render('gitalk-container');
                    </script> -->
                    <!-- Gitalk end -->
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-04T21:04:02+08:00">2018-11-04 21:04:02</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-07 15:34:59</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>20</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>15</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>