<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shu" />
    <meta name="copyright" content="Shu" />

<meta name="keywords" content="统计学习, 机器学习, 读书笔记, " />
    <title>统计学习方法 第八章 提升方法  · You Know Nothing
</title>
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/slim-081711.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/bootstrap-combined.min.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/style.css" media="screen">
    <link rel="stylesheet" type="text/css" href="https://xutree.github.io/theme/css/solarizedlight.css" media="screen">
</head>

<body>
    <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://xutree.github.io/"><span class=site-name>You Know Nothing</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://xutree.github.io/index.html">主页</a></li>
                            <li ><a href="https://xutree.github.io/categories.html">分类</a></li>
                            <li ><a href="https://xutree.github.io/tags.html">标签</a></li>
                            <li ><a href="https://xutree.github.io/archives.html">归档</a></li>
                            <li>
                                <form class="navbar-search" action="https://xutree.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="关键字搜索" name="q" id="tipue_search_input"></form>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
    <div class="row-fluid">
        <header class="page_header span10 offset2">
            <h1><a href="https://xutree.github.io/pages/2018/11/12/sl-8/"> 统计学习方法 第八章 提升方法  </a></h1>
        </header>
    </div>

    <div class="row-fluid">
        <!--  -->
        <div class="span2" style="float:left;font-size:1em;">
            <nav>
                <!-- <h4>目录</h4> -->
                <div class="toc">
<ul>
<li><a href="#81-adaboost">8.1 提升方法 AdaBoost 算法</a><ul>
<li><a href="#811">8.1.1 提升方法的基本思想</a></li>
<li><a href="#812-adaboost">8.1.2 AdaBoost 算法</a></li>
</ul>
</li>
<li><a href="#82-adaboost">8.2 AdaBoost  算法的训练误差分析</a></li>
<li><a href="#83-adaboost">8.3 AdaBoost 算法的解释</a><ul>
<li><a href="#831">8.3.1 前向分步算法</a></li>
<li><a href="#832-adaboost">8.3.2 前向分布算法与 AdaBoost</a></li>
</ul>
</li>
<li><a href="#84">8.4 提升树</a><ul>
<li><a href="#841">8.4.1 提升树模型</a></li>
<li><a href="#842">8.4.2 提升树算法</a></li>
<li><a href="#843">8.4.3 梯度提升</a></li>
</ul>
</li>
</ul>
</div>
            </nav>
        </div>
        <div class="span8 article-content">

                
<p>提升（boosting）方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。</p>
<h2 id="81-adaboost">8.1 提升方法 AdaBoost 算法</h2>
<h3 id="811">8.1.1 提升方法的基本思想</h3>
<p>提升方法是一种可以用来减小监督式学习中偏差的机器学习元算法。面对的问题是迈可·肯斯（Michael Kearns）提出的：一组“弱学习者”的集合能否生成一个“强学习者”？弱学习者一般是指一个分类器，它的结果只比随机分类好一点点；强学习者指分类器的结果非常接近真值。</p>
<p>大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。这样对于提升方法来说，有两个问题需要回答：</p>
<ul>
<li>在每一轮如何改变训练数据的权值或概率分布</li>
<li>如何将弱分类器组合为一个强分类器</li>
</ul>
<p>关于第一个问题，AdaBoost 的做法是，提高那些被前一轮分类器错误分类样本的权值，而降低那些被正确分类样本的权值。至于第二个问题，即弱分类器的组合，AdaBoost 采取加权多数表决的方法，具体的加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用。</p>
<h3 id="812-adaboost">8.1.2 AdaBoost 算法</h3>
<p><strong>
算法 8.1（AdaBoost）<br/>
输入：训练数据集
<div class="math">$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$</div>
其中 <span class="math">\(x_i\in{\cal X}\subseteq\mathbb{R}^n\)</span>，<span class="math">\(y_i\inΥ=\{−1,+1\}\)</span>；弱学习算法<br/>
输出：最终分类器 <span class="math">\(G(x)\)</span><br/>
(1) 初始化训练数据的权值分布
<div class="math">$$D_1= (w_{11},\cdots,w_{1i},w_{1N}),\ w_{1i}=\frac{1}{N},\ i=1,2,\cdots,N$$</div>
(2) 对 <span class="math">\(m=1,2,\cdots,M\)</span><br/>
(2.a) 使用具有权值分布 <span class="math">\(D_m\)</span> 的训练数据集学习，得到基本分类器
<div class="math">$$G_m(x):{\cal X} \rightarrow \{-1,+1\}$$</div>
(2.b) 计算 <span class="math">\(G_m(x)\)</span> 在训练数据集上的分类误差率
<div class="math">$$e_m = \sum_{i=1}^NP(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}\mathbb{I}(G_m(x_i)\neq y_i)$$</div>
(2.c) 计算 <span class="math">\(G_m(x)\)</span> 的系数
<div class="math">$$\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}$$</div>
这里的对数是自然对数<br/>
(2.d) 更新训练数据集的权值分布
<div class="math">$$D_{m+1}=(w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N}) \\
w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)),\ i=1,2,\cdots,N$$</div>
这里，<span class="math">\(Z_m\)</span> 是规范化因子
<div class="math">$$Z_m=\sum_{i=1}^{N}w_{mi}\exp\left(-\alpha_my_iG_m(x_i)\right)$$</div>
它使 <span class="math">\(D_{m+1}\)</span> 成为一个概率分布<br/>
(3) 构建基本分类器的线性组合
<div class="math">$$f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)$$</div>
得到最终分类器
<div class="math">$$G(x)=\text{sign}(f(x))=\text{sign}\left(\sum_{m=1}^M\alpha_mG_m(x)\right)$$</div>
</strong></p>
<p><span class="math">\(\alpha_m\)</span> 表示 <span class="math">\(G_m(x)\)</span> 在最终分类器中的重要性。当 <span class="math">\(e_m\leq\frac{1}{2}\)</span> 时，<span class="math">\(\alpha_m\geq0\)</span>，并且 <span class="math">\(\alpha_m\)</span> 随着 <span class="math">\(e_m\)</span> 的减小而增大。</p>
<p>更新数据的权值方案也可以写为
</p>
<div class="math">$$w_{m+1,i}=\begin{cases}
\frac{w_{mi}}{Z_m}\text{e}^{-\alpha_m}, &amp; G_m(x_i)=y_i \\
\frac{w_{mi}}{Z_m}\text{e}^{\alpha_m}, &amp; G_m(x_i)\neq y_i
\end{cases}$$</div>
<p>
显然，正确分类样本的权值在缩小，错误分类样本的权值在增大，误分类样本的权值被放大
</p>
<div class="math">$$\text{e}^{2\alpha_m}=\frac{1-e_m}{e_m}$$</div>
<h2 id="82-adaboost">8.2 AdaBoost  算法的训练误差分析</h2>
<p><strong>
定理 8.1（AdaBoost 的训练误差界）AdaBoost 算法最终分类器的训练误差界为
<div class="math">$$\frac{1}{N}\sum_{i=1}^N\mathbb{I}\left(G(x_i)\neq y_i\right)\leq\frac{1}{N}\sum_{i=1}^N\exp(-y_if(x_i))=\prod_{m=1}^MZ_m$$</div>
其中
<div class="math">$$G(x)=\text{sign}(f(x))=\text{sign}\left(\sum_{m=1}^M\alpha_mG_m(x)\right) \\
f(x)=\sum_{m=1}^{M}\alpha_mG_m(x) \\
Z_m=\sum_{i=1}^{N}w_{mi}\exp\left(-\alpha_my_iG_m(x_i)\right)$$</div>
</strong></p>
<p><strong>证明</strong>：当 <span class="math">\(G(x_i)\neq y_i\)</span> 时，<span class="math">\(y_if(x_i)&lt;0\)</span>，因而 <span class="math">\(\exp(-y_if(x_i))\geq1\)</span>，前半部分成立。</p>
<p>对于后半部分，注意到
</p>
<div class="math">$$w_{mi}\exp(-\alpha_my_iG_m(x_i))=Z_mw_{m+1,i}$$</div>
<p>
则
</p>
<div class="math">$$\begin{eqnarray}
\frac{1}{N} &amp;\sum_i&amp; \exp(-y_if(x_i)) \\
&amp;=&amp; \frac{1}{N}\sum_i\exp\left(-y_i\sum_{m=1}^{M}\alpha_mG_m(x_i)\right) \\
&amp;=&amp; \frac{1}{N}\sum_i\prod_{m=1}^M\exp\left(-y_i\alpha_mG_m(x_i)\right) \\
&amp;=&amp; \sum_iw_{1i}\prod_{m=1}^M\exp\left(-y_i\alpha_mG_m(x_i)\right) \\
&amp;=&amp; Z_1\sum_iw_{2i}\prod_{m=2}^M\exp\left(-y_i\alpha_mG_m(x_i)\right) \\
&amp;=&amp; Z_1Z_2\sum_iw_{3i}\prod_{m=3}^M\exp\left(-y_i\alpha_mG_m(x_i)\right) \\
&amp;=&amp; \cdots \\
&amp;=&amp; \prod_{m=1}^MZ_m
\end{eqnarray}$$</div>
<p>这一定理说明，可以在每一轮选取适当的 <span class="math">\(G_m\)</span> 使得 <span class="math">\(Z_m\)</span> 最小，从而使训练误差下降最快。</p>
<p><strong>
定理 8.2（二类分类问题 AdaBoost 的训练误差界）
<div class="math">$$\prod_{m=1}^MZ_m=\prod_{m=1}^M\left[2\sqrt{e_m(1-e_m)}\right]=\prod_{m=1}^M\sqrt{(1-4\gamma_m^2)}\leq\exp\left(-2\sum_{m=1}^M\gamma_m^2\right)$$</div>
这里，<span class="math">\(\gamma_m=\frac{1}{2}-e_m\)</span>
</strong></p>
<p><strong>证明</strong>：对二类分类问题
</p>
<div class="math">$$\begin{eqnarray}
Z_m &amp;=&amp; \sum_{i=1}^Nw_{mi}\exp(-\alpha_my_iG_m(x_i)) \\
&amp;=&amp; \sum_{y_i=G_m(x_i)}w_{mi}\text{e}^{-\alpha_m}+\sum_{y_i\neq G_m(x_i)}w_{mi}\text{e}^{\alpha_m} \\
&amp;=&amp; (1-e_m)\text{e}^{-\alpha_m}+e_m\text{e}^{\alpha_m} \\
&amp;=&amp; 2\sqrt{e_m(1-e_m)}=\sqrt{1-4\gamma_m^2}
\end{eqnarray}$$</div>
<p>
后半部分证明可以由 <span class="math">\(\text{e}^x\)</span> 和 <span class="math">\(\sqrt{1-x}\)</span> 在点 <span class="math">\(x=0\)</span> 的泰勒展开式推出不等式
</p>
<div class="math">$$\sqrt{1-4\gamma_m^2}\leq\exp(-2\gamma_m^2)$$</div>
<p>
进而得到。</p>
<p><strong>
推论 8.1 如果存在 <span class="math">\(\gamma&gt;0\)</span>，对所有的 <span class="math">\(m\)</span> 有 <span class="math">\(\gamma_m\geq\gamma\)</span>，则
<div class="math">$$\frac{1}{N}\sum_{i=1}^N\mathbb{I}\left(G(x_i)\neq y_i\right)\leq\exp(-2M\gamma^2)$$</div>
</strong>
这表明在此条件下，AdaBoost 训练误差关于训练次数 <span class="math">\(M\)</span> 以指数速率下降。</p>
<h2 id="83-adaboost">8.3 AdaBoost 算法的解释</h2>
<p>AdaBoost 算法还有另外一个解释，即可以认为 AdaBoost 算法是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法时的二类分类学习方法。</p>
<h3 id="831">8.3.1 前向分步算法</h3>
<p>考虑加法模型（additive model）
</p>
<div class="math">$$f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)$$</div>
<p>
其中，<span class="math">\(b\left(x;\gamma_{m}\right)\)</span> 为基函数，<span class="math">\(\beta_{m}\)</span> 为基函数系数，<span class="math">\(\gamma_{m}\)</span> 为基函数参数。</p>
<p>在给定训练数据及损失函数 <span class="math">\(L\left(y,f\left(x\right)\right)\)</span> 的条件下，学习加法模型 <span class="math">\(f\left(x\right)\)</span> 成为经验风险极小化问题
</p>
<div class="math">$$\begin{align*} \\ &amp; \min_{\beta_{m},\gamma_{m}} \sum_{i=1}^{N} L \left( y_{i}, \sum_{m=1}^{M} \beta_{m} b\left(x_{i};\gamma_{m}\right) \right) \end{align*}$$</div>
<p>通常这是一个复杂的优化问题。前向分步算法（forward stagewise algorithm）求解这一问题的想法是：因为学习是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体的，每步只需优化如下损失函数
</p>
<div class="math">$$\begin{align*} \\ &amp; \min_{\beta,\gamma} \sum_{i=1}^{N} L \left( y_{i}, \beta b\left(x_{i};\gamma\right) \right) \end{align*}$$</div>
<p><strong>
算法 8.2（前向分步算法）<br/>
输入：训练数据集
<div class="math">$$T=\left\{(x_{1},y_{1}),(x_{2},y_{2}),\cdots,(x_{N},y_{N})\right\}$$</div>
损失函数 <span class="math">\(L\left(y,f\left(x\right)\right)\)</span>；基函数集 <span class="math">\(\left\{b\left(x;\gamma\right)\right\}\)</span><br/>
输出：加法模型 <span class="math">\(f\left(x\right)\)</span><br/>
(1) 初始化 <span class="math">\(f_{0}\left(x\right)=0\)</span><br/>
(2) 对 <span class="math">\(m=1,2,\cdots,M\)</span><br/>
(2.a) 极小化损失函数
<div class="math">$$\begin{align*} \\ &amp; \left(\beta_{m},\gamma_{m}\right) = \arg \min_{\beta,\gamma} \sum_{i=1}^{N} L \left( y_{i},f_{m-1} \left(x_{i}\right) + \beta b\left(x_{i};\gamma \right)\right) \end{align*}$$</div>
得到参数 <span class="math">\(\beta_{m}\)</span>，<span class="math">\(\gamma_{m}\)</span><br/>
(2.b) 更新
<div class="math">$$\begin{align*} \\&amp; f_{m} \left(x\right) = f_{m-1} \left(x\right) + \beta_{m} b\left(x;\gamma_{m}\right) \end{align*}$$</div>
(3) 得到加法模型
<div class="math">$$\begin{align*} \\ &amp; f \left( x \right) = f_{M} \left( x \right) = \sum_{m=1}^{M} \beta_{m} b \left( x; \gamma_{m} \right) \end{align*}$$</div>
</strong></p>
<h3 id="832-adaboost">8.3.2 前向分布算法与 AdaBoost</h3>
<p><strong>
定理 8.3 AdaBoost 算法是前向分布加法算法的特例。这时模型是由基本分类器组成的加法模型，损失函数是指数函数。
</strong></p>
<p><strong>证明</strong>：加法模型等价于 AdaBoost 的最终分类器
</p>
<div class="math">$$f(x)=\sum_{m=1}^{M}\alpha_{m}G_{m}(x)$$</div>
<p>
由基本分类器 <span class="math">\(G_{m}(x)\)</span> 及其系数 <span class="math">\(\alpha_{m}\)</span> 组成，<span class="math">\(m=1,2,\cdots,M\)</span>。前向分布算法逐一学习基本函数，这一过程与 AdaBoost 算法逐一学习基本分类器的过程一致。下面证明前向分布算法的损失函数是指数损失函数（exponential loss function）
</p>
<div class="math">$$L(y,f(x))=exp[-yf(x)]$$</div>
<p>
时，其学习的具体操作等价于 AdaBoost 算法学习的具体操作。</p>
<p>假设经过 <span class="math">\(m-1\)</span> 轮迭代前向分布算法已经得到 <span class="math">\(f_{m-1}(x)\)</span>：
</p>
<div class="math">$$\begin{eqnarray}
f_{m-1}(x) &amp;=&amp; f_{m-2}(x)+\alpha_{m-1}G_{m-1}(x) \\
&amp;=&amp; \alpha_{1}G_{1}(x)+\cdots +\alpha_{m-1}G_{m-1}(x)
\end{eqnarray}$$</div>
<p>
在第 <span class="math">\(m\)</span> 轮得到 <span class="math">\(\alpha_{m}\)</span>，<span class="math">\(G_{m}(x)\)</span> 和 <span class="math">\(f_{m}(x)\)</span>
</p>
<div class="math">$$f_{m}(x)=f_{m-1}(x)+\alpha_{m}G_{m}(x)$$</div>
<p>
目标是使前向算法得到的 <span class="math">\(\alpha_{m}\)</span> 和 <span class="math">\(G_{m}\)</span> 使 <span class="math">\(f_{m}(x)\)</span> 在训练数据集 <span class="math">\(T\)</span> 上的指数损失最小，即：
</p>
<div class="math">$$\left(\alpha_{m},G_{m}(x)\right)=\arg\underset{\alpha,G}{\min}\sum_{i=1}^{N}\exp\left[-y_{i}(f_{m-1}(x_{i})+\alpha G(x_{i}))\right]$$</div>
<p>
可以表示成：
</p>
<div class="math">$$(\alpha_{m},G_{m}(x))=\arg\underset{\alpha,G}{\min}\sum_{i=1}^{N}\overline{w}_{mi}\exp[-y_{i}\alpha G(x_{i})]$$</div>
<p>
其中
</p>
<div class="math">$$\overline{w}_{mi}=\exp[-y_{i}f_{m-1}(x_{i})]$$</div>
<p> <span class="math">\(\overline{w}_{mi}\)</span> 不依赖于 <span class="math">\(\alpha\)</span>，也不依赖于 <span class="math">\(G\)</span>，但依赖于 <span class="math">\(f_{m-1}(x)\)</span>，随着每一轮迭代而发生改变。</p>
<p>首先求 <span class="math">\(G_{m}^{\star}(x)\)</span>，进一步展开：
</p>
<div class="math">$$\begin{eqnarray}
\sum_{i=1}^{N}\overline{w}_{mi}\exp[-y_{i}\alpha G(x_{i})] &amp;=&amp; \sum_{i=1}^{N}\overline{w}_{mi}\text{e}^{-\alpha}\mathbb{I}(y_{i}=G(x_{i}))+\sum_{i=1}^{N}\overline{w}_{mi}\text{e}^{\alpha}\mathbb{I}(y_{i}\neq G(x_{i})) \\
&amp;=&amp; \text{e}^{-\alpha}\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}=G(x_{i}))+\text{e}^{\alpha}\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i})) \\
&amp;+&amp; \text{e}^{-\alpha}\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))-e^{-\alpha}\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i})) \\
&amp;=&amp; \text{e}^{-\alpha}\sum_{i=1}^{N}\overline{w}_{mi}+(\text{e}^{\alpha}-\text{e}^{-\alpha})\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))
\end{eqnarray}$$</div>
<p>
所以最小化 <span class="math">\(G(x)\)</span> 由下式得到：
</p>
<div class="math">$$G_{m}^{\star}(x)=\arg\underset{G}{\min}\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))$$</div>
<p>之后我们求解 <span class="math">\(\alpha_{m}^{\star}\)</span>：
</p>
<div class="math">$$\sum_{i=1}^{N}\overline{w}_{mi}\exp[-y_{i}\alpha G(x_{i})]=\sum_{y_{i}=G_{m}(x_{i})}\overline{w}_{mi}\text{e}^{-\alpha}+\sum_{y_{i}\neq G_{m}(x_{i})}\overline{w}_{mi}\text{e}^{\alpha}\\
=\text{e}^{-\alpha}\sum_{i=1}^{N}\overline{w}_{mi}+(\text{e}^{\alpha}-\text{e}^{-\alpha})\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))$$</div>
<p>对 <span class="math">\(\alpha\)</span> 求导：
</p>
<div class="math">$$\frac{\partial }{\partial \alpha}\left(\text{e}^{-\alpha}\sum_{i=1}^{N}\overline{w}_{mi}+(\text{e}^{\alpha}-\text{e}^{-\alpha})\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))\right)\\
=-\text{e}^{-\alpha}\sum_{i=1}^{N}\overline{w}_{mi}+(\text{e}^{\alpha}+\text{e}^{-\alpha})\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))=0$$</div>
<p>
即得：
</p>
<div class="math">$$\frac{\text{e}^{\alpha}+\text{e}^{-\alpha}}{\text{e}^{-\alpha}}=\frac {\sum_{i=1}^{N}\overline{w}_{mi}}{\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))} \\
\alpha_{m}^{\star}=\frac{1}{2}\log\frac{1-e_{m}}{e_{m}}$$</div>
<p>
其中 <span class="math">\(e_{m}\)</span> 是分类错误率：
</p>
<div class="math">$$e_{m}=\frac {\sum_{i=1}^{N}\overline{w}_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))}{\sum_{i=1}^{N}\overline{w}_{mi}}=\sum_{i=1}^{N}w_{mi}\mathbb{I}(y_{i}\neq G(x_{i}))$$</div>
<p>这里的 <span class="math">\(\alpha_{m}^{\star}\)</span> 与 AdaBoost 算法的 <span class="math">\(\alpha_{m}\)</span> 完全一致。</p>
<p>再看一下每一轮的权值更新，由：
</p>
<div class="math">$$f_{m}(x)=f_{m-1}(x)+\alpha_{m}G_{m}(x)$$</div>
<p>
以及
</p>
<div class="math">$$\overline{w}_{mi}=\exp[-y_{i}f_{m-1}(x_{i})]$$</div>
<p>
可得：
</p>
<div class="math">$$\overline{w}_{m+1,i}=\overline{w}_{m,i}\exp[-y_{i}\alpha_{m}G_{m}(x)]$$</div>
<p>
这与 AdaBoost 算法的样本权值的更新，只相差规范会因子，因此等价。</p>
<h2 id="84">8.4 提升树</h2>
<p>提升树是以分类或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。</p>
<h3 id="841">8.4.1 提升树模型</h3>
<p>提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。只有一个条件的基本分类器可以看做是由一个根结点直接连接两个叶结点简单决策树，即所谓的决策树桩（decision stump）。提升树模型可以表示为决策树的加法模型：
</p>
<div class="math">$$f_{M}(x)=\sum_{m=1}^{M}T(x;\Theta_{m})$$</div>
<p>
其中，<span class="math">\(T(x;\Theta_{m})\)</span> 表示决策树；<span class="math">\(\Theta_{m}\)</span> 为决策树的参数；<span class="math">\(M\)</span> 为树的个数。提升树中树之间没有权重，或者说它们的权重都是一样的，树之间是独立的，训练样本之间也没有权重的概念，这是提升树和随机森林、AdaBoost 之间的区别。</p>
<h3 id="842">8.4.2 提升树算法</h3>
<p>提升树算法采用前向分步算法。首先确定初始提升树 <span class="math">\(f_{0}(x)=0\)</span>，第 <span class="math">\(m\)</span> 步的模型是：
</p>
<div class="math">$$f_{m}(x)=f_{m-1}(x)+T(x;\Theta_{m})$$</div>
<p>其中，<span class="math">\(f_{m-1}(x)\)</span> 为当前模型，通过经验风险最小化确定下一决策树的参数 <span class="math">\(\Theta_{m}\)</span>：
</p>
<div class="math">$$\hat {\Theta}_{m}=\arg\underset{\Theta_{m}}{\min}\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+T(x_{i};\Theta_{m})$$</div>
<p>不同问题的提升树有不同的学习算法，其主要区别在于使用的损失函数不同。包含用平凡误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。对于二分类问题，提升树只需将 Adaboost 算法的基本分类器限制为二分类即可，可以说这时的提升树算法是 Adaboost 算法的特殊情况。下面主要讨论下回归问题。</p>
<p>已知训练数据集
</p>
<div class="math">$$\begin{align*} \\&amp; T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\} \end{align*} \\ $$</div>
<p>
其中，<span class="math">\(x_{i} \in \mathcal{X} \subseteq \mathbb{R}^{n}\)</span>，<span class="math">\(y_{i} \in \mathcal{Y} \subseteq \mathbb{R}\)</span>，<span class="math">\(i = 1, 2,\cdots, N\)</span>。</p>
<p>将输入空间 <span class="math">\(\mathcal{X}\)</span> 划分为 <span class="math">\(J\)</span> 个互不相交的区域 <span class="math">\(R_{1},R_{2},\cdots,R_{J}\)</span>，且在每个区域上确定输出的常量 <span class="math">\(c_{j}\)</span>，则回归树
</p>
<div class="math">$$\begin{align*} \\&amp; T \left(x; \varTheta\right) = \sum_{j=1}^{J} c_{j} \mathbb{I} \left(x \in R_{j}\right) \end{align*} \\$$</div>
<p>
其中，参数
</p>
<div class="math">$$\varTheta = \left\{ \left(R_{1}, c_{1}\right),\left(R_{2}, c_{2}\right),\cdots,\left(R_{J}, c_{J}\right) \right\}$$</div>
<p>
表示树的区域划分和各区域上的常数。<span class="math">\(J\)</span> 是回归树的复杂度即叶结点个数。</p>
<p>回归提升树使用前向分布算法
</p>
<div class="math">$$\begin{align*} \\&amp; f_{0}=0 \\ &amp; f_{m}\left(x\right) = f_{m-1}\left(x\right) + T \left(x; \varTheta_{m}\right),\ m=1,2,\cdots,M\\ &amp; f_{M} = \sum_{m=1}^{M} T \left(x; \varTheta_{m}\right) \end{align*} \\$$</div>
<p>在前向分布算法的第 <span class="math">\(m\)</span> 步给定当前模型 <span class="math">\(f_{m-1}\left(x\right)\)</span>，模型参数
</p>
<div class="math">$$\begin{align*} \\&amp; \hat \varTheta_{m} = \arg \min_{\varTheta_{m}} \sum_{i=1}^{N} L \left( y_{i}, f_{m-1}\left(x_{i}\right) + T \left( x_{i}; \varTheta_{m} \right) \right) \end{align*} \\ $$</div>
<p>
得到第 <span class="math">\(m\)</span> 棵树的参数 <span class="math">\(\hat \varTheta_{m}\)</span></p>
<p>当采用平方误差损失函数
</p>
<div class="math">$$\begin{align*} \\&amp; L \left( y, f_{m-1}\left(x\right)+T\left(x;\varTheta_{m}\right)\right) \\ &amp; = \left[y-f_{m-1}\left(x\right)-T\left(x;\varTheta_{m}\right)\right]^{2} \\ &amp; = \left[r-T\left(x;\varTheta_{m}\right)\right]^{2}\end{align*} \\ $$</div>
<p>
其中，<span class="math">\(r=y-f_{m-1}\left(x\right)\)</span> 是当前模型拟合数据的残差。</p>
<p><strong>
算法 8.3（回归问题的提升树方法）<br/>
输入：训练数据集
<div class="math">$$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$$</div>
<span class="math">\(x_{i} \in \mathcal{X} \subseteq R^{n}\)</span>，<span class="math">\(y_{i} \in \mathcal{Y} \subseteq R\)</span>，<span class="math">\(i = 1, 2, \cdots, N\)</span><br/>
输出：回归提升树 <span class="math">\(f_{M}\left(x\right)\)</span><br/>
(1) 初始化 <span class="math">\(f_{0}\left(x\right)=0\)</span><br/>
(2) 对 <span class="math">\(m=1,2,\cdots,M\)</span><br/>
(2.) 计算残差
<div class="math">$$\begin{align*} \\ &amp; r_{mi}=y_{i}-f_{m-1}\left(x_{i}\right),\quad i=1,2,\cdots,N \end{align*}$$</div>
(2.b) 拟合残差 <span class="math">\(r_{mi}\)</span> 学习一个回归树，得到 <span class="math">\(T\left(x;\varTheta_{m}\right)\)</span><br/>
(2.c) 更新 <div class="math">$$f_{m}=f_{m-1}\left(x\right)+T\left(x;\varTheta_{m}\right)$$</div>
(3) 得到回归提升树
<div class="math">$$\begin{align*} \\ &amp; f_{M} \left( x \right) = \sum_{m=1}^{M} T \left(x;\varTheta_{m}\right) \end{align*}$$</div>
</strong></p>
<h3 id="843">8.4.3 梯度提升</h3>
<p>提升树用加法模型与前向分布算法实现学习的优化过程。当损失函数是平方损失函数时，每一步优化是很简单的，但是对一般损失函数而言，往往每一步优化并不那么容易。因此Freidman提过了梯度提升（gradient boosting）。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值：
</p>
<div class="math">$$-\left [ \frac{\partial L(y_{i},f(x_{i}))}{\partial f(x_{i})} \right]_{f(x)=f_{m-1}(x)}$$</div>
<p>
作为回归问题提升树算法中的残差近似值，拟合一个回归树。</p>
<p><strong>
算法 8.4（梯度提升算法）<br/>
输入：训练数据集
<div class="math">$$T = \left\{ \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right), \cdots, \left( x_{N}, y_{N} \right) \right\}$$</div>
<span class="math">\(x_{i} \in \mathcal{X} \subseteq \mathbb{R}^{n}\)</span>，<span class="math">\(y_{i} \in \mathcal{Y} \subseteq \mathbb{R}\)</span>，<span class="math">\(i = 1, 2, \cdots, N\)</span>，最大迭代次数 <span class="math">\(M\)</span> ，损失函数 <span class="math">\(L\left(y,f\left(x\right)\right)\)</span><br/>
输出：回归树 <span class="math">\(\hat f\left(x\right)\)</span><br/>
(1) 初始化
<div class="math">$$\begin{align*} \\ &amp; f_{0}\left(x\right) = \arg \min_{c} \sum_{i=1}^{N} L \left(y_{i},c\right) \end{align*}$$</div>
(2) 对 <span class="math">\(m=1,2,\cdots,M\)</span><br/>
(2.a) 对 <span class="math">\(i=1,2,\cdots,N\)</span> 计算
<div class="math">$$\begin{align*} \\ &amp; r_{mi}=- \left[ \dfrac {\partial L \left(y_{i},f\left(x_{i}\right) \right)}{\partial f \left(x_{i} \right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)} \end{align*}$$</div>
(2.b) 对 <span class="math">\(r_{mi}\)</span> 拟合回归树，得到第 <span class="math">\(m\)</span> 棵树的叶结点区域 <span class="math">\(R_{mj}\)</span>，<span class="math">\(j=1,2,\cdots,J\)</span> <br/>
(2.c) 对 <span class="math">\(j=1,2,\cdots,J\)</span>，计算<br/>
<div class="math">$$\begin{align*} \\ &amp; c_{mj}=\arg \min_{c} \sum_{x_{i} \in R_{mj}} L \left( y_{i},f_{m-1} \left(x_{i}\right)+c \right) \end{align*}$$</div>
(2.d) 更新 <span class="math">\(f_{m}\left(x\right)= f_{m-1}\left(x\right) + \sum_{j=1}^{J} c_{mj} \mathbb{I} \left(x \in R_{mj} \right)\)</span><br/>
(3) 得到回归树
<div class="math">$$\begin{align*} \\ &amp; \hat f \left( x \right) = f_{M} \left( x \right) = \sum_{m=1}^{M} \sum_{j=1}^{J} c_{mj} \mathbb{I} \left( x \in R_{mj} \right) \end{align*}$$</div>
</strong></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                <aside>
                    <hr />
                    <nav class="related">
                        <h1>
                            <font color="#771515"><em>RELATED</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/20/sl-12/">统计学习方法 第十二章 统计学习方法总结</a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-11/">统计学习方法 第十一章 条件随机场</a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-10/">统计学习方法 第十章 隐马尔科夫模型</a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/17/sl-9/">统计学习方法 第九章 EM 算法及其推广</a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-7_4/">统计学习方法 第七章 支持向量机（4）——序列最小最优化算法</a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="older">
                        <h1>
                            <font color="#771515"><em>OLDER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-7_4/">
                                    统计学习方法 第七章 支持向量机（4）——序列最小最优化算法
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/12/sl-7_3/">
                                    统计学习方法 第七章 支持向量机（3）——非线性支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/11/sl-7_2/">
                                    统计学习方法 第七章 支持向量机（2）——线性支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/11/sl-7_1/">
                                    统计学习方法 第七章 支持向量机（1）——线性可分支持向量机
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/10/directional_derivative-gradient/">
                                    方向导数和梯度
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <nav class="newer">
                        <h1>
                            <font color="#771515"><em>NEWER</em></font>
                        </h1>
                        <ul>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/16/likelihood-function/">
                                    似然函数
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/17/sl-9/">
                                    统计学习方法 第九章 EM 算法及其推广
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-10/">
                                    统计学习方法 第十章 隐马尔科夫模型
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/19/sl-11/">
                                    统计学习方法 第十一章 条件随机场
                                </a>
                            </li>
                            <li>
                                <a href="https://xutree.github.io/pages/2018/11/20/sl-12/">
                                    统计学习方法 第十二章 统计学习方法总结
                                </a>
                            </li>
                        </ul>
                    </nav>
                    <!-- Gitalk 评论 start  -->

                    <!-- Link Gitalk 的支持文件  -->
                    <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
                    <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script>
                    <div id="gitalk-container"></div>
                    <script type="text/javascript">
                        var dateTime = Date.now();
                        var timestamp = Math.floor(dateTime / 1000);
                        var gitalk = new Gitalk({

                            // gitalk的主要参数
                            clientID: '93f43349e9fd3154bfad',
                            clientSecret: 'd6d09d1d7261f6b62f46b39e5fcace85b81c3cd7',
                            repo: 'xutree.github.io',
                            owner: 'xutree',
                            admin: ['xutree'],
                            id: String(timestamp)

                        });
                        gitalk.render('gitalk-container');
                    </script> -->
                    <!-- Gitalk end -->
                </aside>
            </div>
            <section>
                <div class="span2" style="float:right;font-size:0.9em;">
                    <h4>发布日期</h4>
                    <time pubdate="pubdate" datetime="2018-11-12T21:42:33+08:00">2018-11-12 21:42:33</time>
                    <h4>最后更新</h4>
                    <div class="last_updated">2018-11-13 10:34:40</div>
                    <h4>分类</h4>
                    <a class="category-link" href="/categories.html#读书笔记-ref">读书笔记</a>
                    <h4>标签</h4>
                    <ul class="list-of-tags tags-in-article">
                        <li><a href="/tags.html#机器学习-ref">机器学习
                                <span>20</span>
</a></li>
                        <li><a href="/tags.html#统计学习-ref">统计学习
                                <span>15</span>
</a></li>
                    </ul>

                </div>
            </section>
        </div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>    <script src="https://code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
    <script>
        function validateForm(query) {
            return (query.length > 0);
        }
    </script>
</body>

</html>